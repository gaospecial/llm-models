[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "大模型平台和应用实例",
    "section": "",
    "text": "Preface\nThis is a Quarto book, created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "本书介绍开源模型的部署和应用。",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "setup-env.html",
    "href": "setup-env.html",
    "title": "3  设置运行环境",
    "section": "",
    "text": "3.1 使用 Conda\nConda 是一个强大的包管理和环境管理工具，广泛用于 Python 项目。以下是使用 Conda 的基本步骤：\n首先，从 Anaconda 官网下载并安装 Conda。您可以选择安装完整的 Anaconda 发行版，或者更轻量级的 Miniconda。\n创建一个新的 Conda 环境：\n这将创建一个名为 myenv 的 Python 3.10 环境。\n在创建的环境中安装所需的包：\n这将安装 NumPy 包。\n激活环境：\n这将激活您创建的环境。\n如果您已经有一个想要记录的 Conda 环境，可以使用以下命令导出它：\n这将创建一个包含当前环境所有包及其版本的 environment.yml 文件。\n创建好 environment.yml 文件后，您可以使用以下命令创建新环境：\n这将创建一个与 environment.yml 文件中记录的包及其版本完全相同的新环境。",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>设置运行环境</span>"
    ]
  },
  {
    "objectID": "setup-env.html#使用-conda",
    "href": "setup-env.html#使用-conda",
    "title": "3  设置运行环境",
    "section": "",
    "text": "安装 Conda\n\n\n\n创建环境\n\n\nconda create -n myenv python=3.10\n\n\n安装包\n\n\nconda install numpy\n\n\n激活环境\n\n\nconda activate myenv\n\n\n导出环境\n\n\nconda env export &gt; environment.yml\n\n\n使用文件创建环境\n\n\nconda env create -f environment.yml",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>设置运行环境</span>"
    ]
  },
  {
    "objectID": "setup-env.html#使用-pip",
    "href": "setup-env.html#使用-pip",
    "title": "3  设置运行环境",
    "section": "3.2 使用 pip",
    "text": "3.2 使用 pip\npip 是 Python 的包管理工具，广泛用于 Python 项目。以下是使用 pip 的基本步骤：\n\n安装 pip\n\n首先，确保您已经安装了 pip。pip 通常随 Python 一起安装。\n\n安装包\n\n在创建的环境中安装所需的包：\npip install numpy\n这将安装 NumPy 包。\n\n导出环境\n\n如果您已经有一个想要记录的 pip 环境，可以使用以下命令导出它：\npip freeze &gt; requirements.txt\n这将创建一个包含当前环境中所有已安装包及其版本的 requirements.txt 文件。\n\n使用文件创建环境\n\n创建好 requirements.txt 文件后，您可以使用以下命令创建新环境：\npip install -r requirements.txt\n这将安装 requirements.txt 文件中列出的所有包。",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>设置运行环境</span>"
    ]
  },
  {
    "objectID": "setup-env.html#注意",
    "href": "setup-env.html#注意",
    "title": "3  设置运行环境",
    "section": "3.3 注意",
    "text": "3.3 注意\n当你同时使用 conda 和 pip 安装 Python 环境后出现冲突时，可以尝试以下方法解决：\n\n优先使用 Conda：尽量使用 conda 来管理包，因为它更好地处理依赖关系。只有当 conda 中找不到包时，再使用 pip。\n创建隔离环境：通过 conda create -n myenv 创建一个新的环境，避免与全局环境冲突。\n使用 conda install pip：在 conda 环境中安装 pip，确保 pip 安装的包与环境兼容。\n重装冲突包：如果问题持续，尝试卸载冲突包，并使用单一包管理工具重新安装。",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>设置运行环境</span>"
    ]
  },
  {
    "objectID": "huggingface.html",
    "href": "huggingface.html",
    "title": "4  Hugging Face",
    "section": "",
    "text": "4.1 安装",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hugging Face</span>"
    ]
  },
  {
    "objectID": "huggingface.html#安装",
    "href": "huggingface.html#安装",
    "title": "4  Hugging Face",
    "section": "",
    "text": "安装机器学习基础库（pytorch 或者 TensorFlow）。 首先创建一个 Conda 环境，然后安装 Pytorch。conda install pytorch::pytorch torchvision torchaudio -c pytorch\n安装 transformers： pip install transformers datasets evaluate accelerate。",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hugging Face</span>"
    ]
  },
  {
    "objectID": "huggingface.html#配置-gpu-加速",
    "href": "huggingface.html#配置-gpu-加速",
    "title": "4  Hugging Face",
    "section": "4.2 配置 GPU 加速",
    "text": "4.2 配置 GPU 加速\nPytorch 支持 CUDA（NVIDIA）和 MPS（Mac）平台的 GPU 加速，所以这里检测一下硬件环境。\n\nimport torch\n\n# select the device for computation\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelse:\n    device = torch.device(\"cpu\")\n\nprint(f\"using device: {device}\")\n\nusing device: mps",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hugging Face</span>"
    ]
  },
  {
    "objectID": "huggingface.html#pipeline",
    "href": "huggingface.html#pipeline",
    "title": "4  Hugging Face",
    "section": "4.3 Pipeline",
    "text": "4.3 Pipeline\npipeline 是 transformers 库中的一个高层接口，旨在简化模型的使用。它封装了模型的加载、输入处理、预测和输出处理的细节，使得用户可以以更简单的方式执行常见任务。\n在 Hugging Face Transformers 库中，pipelines 可以被分为不同的类别，以适应音频、计算机视觉、自然语言处理（NLP）和多模态任务。以下是这些类别中常见的 pipelines：\n\n音频（Audio）:\n\n语音识别（Speech Recognition）: 将音频转换为文本。\n语音合成（Text-to-Speech）: 将文本转换为语音。\n语音分类（Speech Classification）: 对音频进行分类，如情绪识别。\n\n计算机视觉（Computer Vision）:\n\n图像分类（Image Classification）: 识别图像中的主要对象或场景。\n对象检测（Object Detection）: 识别并定位图像中的对象。\n图像分割（Image Segmentation）: 将图像分割成多个部分或对象。\n图像生成（Image Generation）: 根据文本描述生成图像。\n\n自然语言处理（NLP）:\n\n文本分类（Text Classification）: 对文本进行分类，如情感分析。\n命名实体识别（Named Entity Recognition, NER）: 识别文本中的实体。\n问答（Question Answering）: 从文本中找到问题的答案。\n文本生成（Text Generation）: 生成新的文本内容。\n摘要（Summarization）: 生成文本的摘要。\n翻译（Translation）: 将文本从一种语言翻译成另一种语言。\n\n多模态（Multimodal）:\n\n视觉问答（Visual Question Answering, VQA）: 结合图像和文本问题，提供答案。\n图像字幕生成（Image Captioning）: 为图像生成描述性文本。\n视频问答（Video Question Answering）: 根据视频内容回答问题。\n多模态情感分析（Multimodal Sentiment Analysis）: 结合文本、音频和视觉信息进行情感分析。\n\n\n这些 pipelines 利用了预训练的模型，可以处理各种任务，从单一模态的音频或图像处理到结合多种模态信息的复杂任务。用户可以根据自己的需求选择合适的模型和pipeline来实现特定的任务。\n\nimport inspect\nfrom transformers import pipelines\n\n# 获取 transformers.pipeline 模块中的所有成员\npipeline_members = inspect.getmembers(pipelines)\n\n# 过滤出类，并且名称以 'Pipeline' 结尾\npipeline_classes = [name for name, obj in pipeline_members if inspect.isclass(obj) and name.endswith('Pipeline')]\n\n# 打印符合条件的类名称\nprint(\"Classes ending with 'Pipeline':\")\nfor class_name in pipeline_classes:\n    print(class_name)\n\nClasses ending with 'Pipeline':\nAudioClassificationPipeline\nAutomaticSpeechRecognitionPipeline\nDepthEstimationPipeline\nDocumentQuestionAnsweringPipeline\nFeatureExtractionPipeline\nFillMaskPipeline\nImageClassificationPipeline\nImageFeatureExtractionPipeline\nImageSegmentationPipeline\nImageToImagePipeline\nImageToTextPipeline\nMaskGenerationPipeline\nNerPipeline\nObjectDetectionPipeline\nPipeline\nQuestionAnsweringPipeline\nSummarizationPipeline\nTableQuestionAnsweringPipeline\nText2TextGenerationPipeline\nTextClassificationPipeline\nTextGenerationPipeline\nTextToAudioPipeline\nTokenClassificationPipeline\nTranslationPipeline\nVideoClassificationPipeline\nVisualQuestionAnsweringPipeline\nZeroShotAudioClassificationPipeline\nZeroShotClassificationPipeline\nZeroShotImageClassificationPipeline\nZeroShotObjectDetectionPipeline\n\n\n以下是一些常见的 pipeline 类型和它们的应用场景：\n\n4.3.1 文本分类 (text-classification)\n用于对输入文本进行分类，例如情感分析。\n\nfrom transformers import pipeline\nfrom pprint import pprint  # pretty print\n\nclassifier = pipeline('text-classification')\nresult = classifier(\"I love using transformers!\")\npprint(result)\n\n[{'label': 'POSITIVE', 'score': 0.9994327425956726}]\n\n\n\n\n4.3.2 命名实体识别 (ner)\n用于识别文本中的命名实体（如人名、地点、组织等）。\n\nfrom transformers import pipeline\n\nner = pipeline('ner', device = device)\nresult = ner(\"Hugging Face is based in New York City.\")\npprint(result)\n\n[{'end': 2,\n  'entity': 'I-ORG',\n  'index': 1,\n  'score': 0.9736425,\n  'start': 0,\n  'word': 'Hu'},\n {'end': 7,\n  'entity': 'I-ORG',\n  'index': 2,\n  'score': 0.7939442,\n  'start': 2,\n  'word': '##gging'},\n {'end': 12,\n  'entity': 'I-ORG',\n  'index': 3,\n  'score': 0.9046833,\n  'start': 8,\n  'word': 'Face'},\n {'end': 28,\n  'entity': 'I-LOC',\n  'index': 7,\n  'score': 0.9990658,\n  'start': 25,\n  'word': 'New'},\n {'end': 33,\n  'entity': 'I-LOC',\n  'index': 8,\n  'score': 0.99908113,\n  'start': 29,\n  'word': 'York'},\n {'end': 38,\n  'entity': 'I-LOC',\n  'index': 9,\n  'score': 0.99939454,\n  'start': 34,\n  'word': 'City'}]\n\n\n\n\n4.3.3 问答 (question-answering)\n用于从给定上下文中回答问题。\n\nfrom transformers import pipeline\n\nquestion_answerer = pipeline('question-answering', device = device)\nresult = question_answerer(question=\"What is the capital of France?\", context=\"The capital of France is Paris.\")\npprint(result)\n\n{'answer': 'Paris', 'end': 30, 'score': 0.9863283038139343, 'start': 25}\n\n\n\n\n4.3.4 文本生成 (text-generation)\n用于生成文本，例如生成续写或对话。\n\nfrom transformers import pipeline\n\ngenerator = pipeline('text-generation', device = device)\nresult = generator(\"Once upon a time\", max_length=50)\npprint(result)\n\n[{'generated_text': 'Once upon a time, when I was young, I became obsessed '\n                    'with seeing people I was in the know. I had never seen, '\n                    'seen, even heard of, or read anything about how to get to '\n                    'know the people I knew. It was like'}]\n\n\n\n\n4.3.5 翻译 (translation)\n用于将文本从一种语言翻译成另一种语言。\n\nfrom transformers import pipeline\n\ntranslator = pipeline('translation_en_to_fr', device = device)\nresult = translator(\"Hello, how are you?\")\npprint(result)\n\n[{'translation_text': 'Bonjour, comment êtes-vous?'}]\n\n\n\n\n4.3.6 文本摘要 (summarization)\n用于对长文本进行摘要，提取主要内容。\n\nfrom transformers import pipeline\n\nsummarizer = pipeline('summarization', device = device)\nresult = summarizer(\"Hugging Face is creating a tool that democratizes AI. The library will support various tasks and models.\")\npprint(result)\n\n[{'summary_text': ' Hugging Face is creating a tool that democratizes AI . The '\n                  'library will support various tasks and models . Hugging '\n                  'face is creating an AI tool that can be used to help people '\n                  'learn more about AI . It will be available in the U.S. for '\n                  'free .'}]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hugging Face</span>"
    ]
  },
  {
    "objectID": "huggingface.html#运行机制",
    "href": "huggingface.html#运行机制",
    "title": "4  Hugging Face",
    "section": "4.4 运行机制",
    "text": "4.4 运行机制\n使用 Hugging Face 的 pipeline 运行任务时，任务默认是在本地执行的。\n当你使用 pipeline 函数时，它会加载一个预训练的模型（可以是 Hugging Face Hub 上的模型，也可以是你本地的模型），然后在你的本地机器上执行推理任务。这意味着所有计算都是在你的本地计算机上进行的，而不是在 Hugging Face 的服务器上进行的。\n不过，pipeline 也可以访问在线的模型存储库。如果你指定了一个在线模型（例如 Hugging Face Hub 上的某个模型），那么 pipeline 会先从在线存储库下载模型到本地，然后在本地运行推理任务。因此，即使你访问的是在线模型，执行过程仍然是在本地完成的。\n本地运行推理，可以很方便的执行批处理任务。\n\nfrom transformers import pipeline\n\nclassifier = pipeline(\"sentiment-analysis\", device=device)\n\nresults = classifier([\"We are very happy to show you the 🤗 Transformers library.\", \"We hope you don't hate it.\"])\nfor result in results:\n    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")\n\nlabel: POSITIVE, with score: 0.9998\nlabel: NEGATIVE, with score: 0.5309\n\n\n\n4.4.1 配置模型和参数\npipeline 会自动下载和使用默认的预训练模型。但是有些任务可能没有指定模型，这时候就需要配置模型参数。此外，如果需要使用特定的模型，也可以在 pipeline 构造函数中指定模型名称。\npipeline 会根据任务类型自动处理输入和输出。例如，文本分类任务的输出通常是每个类别的概率，而翻译任务的输出是翻译后的文本。如果提供的任务名称不正确，pipeline 可能会抛出错误。如果模型不适合特定任务，也可能会得到不准确的结果或遇到运行时错误。\n下面的示例中，我们针对 ZeroShotClassificationPipeline 任务指定使用了 Facebook 的 bart-large-mnli 模型。\n\noracle = pipeline(\"zero-shot-classification\", \n                  model=\"facebook/bart-large-mnli\",\n                  device=device)\noracle(\n    \"I have a problem with my iphone that needs to be resolved asap!!\",\n    candidate_labels=[\"urgent\", \"not urgent\", \"phone\", \"tablet\", \"computer\"],\n)\n\n{'sequence': 'I have a problem with my iphone that needs to be resolved asap!!',\n 'labels': ['urgent', 'phone', 'computer', 'not urgent', 'tablet'],\n 'scores': [0.5036347508430481,\n  0.47880011796951294,\n  0.012600583024322987,\n  0.0026557755190879107,\n  0.002308781025931239]}\n\n\n\noracle(\n    \"I have a problem with my iphone that needs to be resolved asap!!\",\n    candidate_labels=[\"english\", \"german\"],\n)\n\n{'sequence': 'I have a problem with my iphone that needs to be resolved asap!!',\n 'labels': ['english', 'german'],\n 'scores': [0.8135165572166443, 0.18648339807987213]}\n\n\n整个流程以及本地模型的安装和运行情况：\n\n模型下载：\n\n当你指定 model=\"facebook/bart-large-mnli\" 时，Hugging Face 的 transformers 库会从 Hugging Face Hub 下载这个预训练模型（facebook/bart-large-mnli）。\n模型下载后，会被存储在你的本地文件系统中，默认位置通常是 ~/.cache/huggingface/transformers/ 目录下。如果你有自定义的缓存目录设置，模型将下载到指定位置。\n\n模型加载：\n\n下载完成后，pipeline 会将该模型加载到内存中。这包括模型的权重、配置文件以及与之相关的词汇表（tokenizer）。\n\n任务执行：\n\n当你使用 oracle 这个 pipeline 对象来进行零样本分类任务时，所有的计算和推理（inference）都是在你的本地机器上执行的。这包括文本的预处理、模型的前向传播计算、以及后处理和输出结果。\n\n\n\n\n4.4.2 本地模型的存储和管理\n\n存储位置：模型的权重文件、配置文件和词汇表会存储在 ~/.cache/huggingface/hub/ 下的一个以模型名称命名的目录中。例如：~/.cache/huggingface/hub/models--facebook--bart-large-mnli/。\n缓存机制：如果你再次使用同一个模型（如 facebook/bart-large-mnli），pipeline 会直接从本地缓存中加载模型，而不会再次从 Hugging Face Hub 下载，除非你手动清除缓存或指定下载新的模型版本。\n\n\n\n4.4.3 配置一个翻译器\n上面我们调用一个翻译器，将英文翻译为法文。\n\nen_fr_translator = pipeline(\"translation_en_to_fr\", device=device)\nen_fr_translator(\"How old are you?\")\n\n[{'translation_text': ' quel âge êtes-vous?'}]\n\n\n不过，调用 pipeline(\"tranlation_en_to_zh\") 却会出错。这是因为：虽然 “translation_en_to_zh” 是一个有效的任务标识符，但它并不是直接指定模型的名称。这时，需要显式指定模型名称来避免这种情况：\n\n\n\n\n\n\nNote\n\n\n\n这里还需要安装一个缺失的模块：SentencePiece。\nSentencePiece 是一个用于文本分词和词汇生成的工具，它在自然语言处理（NLP）任务中非常有用，尤其是在训练和使用基于子词单元的模型时。SentencePiece 由 Google 开发，作为一种无语言依赖的方法，它可以处理几乎任何语言的文本数据。\nSentencePiece 的主要功能\n\n子词单元（Subword Units）生成:\n\nSentencePiece 不依赖于语言的特定词汇表，而是通过数据驱动的方法生成子词单元。它通过分析训练数据中的常见字符序列，生成适合该数据集的子词单元，这些子词单元可以是完整的词、词的一部分（如词缀、词根）、甚至是单个字符。\n这在处理低资源语言或多语言任务时特别有用，因为它可以生成跨语言的统一词汇表，减少OOV（Out of Vocabulary，词汇表外的词）问题。\n\nBPE（Byte-Pair Encoding）和 Unigram 模型:\n\nSentencePiece 支持多种子词分割方法，包括 BPE（Byte-Pair Encoding）和 Unigram 模型。BPE 是一种常用的子词分割算法，通过频繁地合并字符对来生成子词单元。Unigram 模型则是一种基于概率的模型，它根据子词单元的概率来分割文本。\n\n语言无关性:\n\n与传统的分词器不同，SentencePiece 不需要依赖于空格或其他特定的标记来分割词语。这使得它在处理没有明确单词边界的语言（如中文、日文、泰语等）时非常有效。\n\n处理未归一化的文本:\n\nSentencePiece 可以直接处理未经归一化的原始文本（如带有标点符号的文本），这在实际应用中非常有用，避免了对数据进行预处理的需求。\n\n\n使用 SentencePiece 的场景\n\n机器翻译: 在训练机器翻译模型时，使用 SentencePiece 可以将输入和输出文本分割成子词单元，减少词汇表大小，并提高模型的泛化能力。\n预训练语言模型: 诸如 BERT、GPT、T5 等模型在预训练时，通常使用 SentencePiece 生成子词单元词汇表，这些词汇表有助于处理多语言数据和稀有词汇。\n文本生成: 在生成任务中，子词单元可以更好地表示稀有或长尾词汇，减少生成过程中出现的OOV问题。\n\nSentencePiece 是一个强大的分词工具，它通过生成数据驱动的子词单元词汇表，在 NLP 任务中广泛使用，特别是在处理多语言文本和训练深度学习模型时。\n\n\n\ntranslator = pipeline(\"translation_en_to_zh\",\n                      model=\"Helsinki-NLP/opus-mt-en-zh\", device=device)\ntranslator(\"This is a introduction to Huggingface.\")\n\n[{'translation_text': '这是哈ggingface的介绍。'}]\n\n\n结果很一般。这是因为翻译任务不仅需要模型支持，还需要有一个中文的分词器。而默认的分词器不适合进行中文分词的任务。下面，我们优化一下分词器的设置。\n\nfrom transformers import AutoModelWithLMHead,AutoTokenizer,pipeline\nmode_name = 'liam168/trans-opus-mt-en-zh'\nmodel = AutoModelWithLMHead.from_pretrained(mode_name)\ntokenizer = AutoTokenizer.from_pretrained(mode_name)\ntranslation = pipeline(\"translation_en_to_zh\", \n                      model=model, \n                      tokenizer=tokenizer, device=device)\ntranslation('This is a introduction to Huggingface.')\n\n[{'translation_text': '这是\"抱起脸\"的引言'}]\n\n\n让我们逐行解释代码的含义：\nfrom transformers import AutoModelWithLMHead, AutoTokenizer, pipeline\n\nAutoModelWithLMHead: 这是一个自动加载语言模型（Language Model）的类，通常用于加载带有语言建模头的模型。LMHead 代表语言模型的输出层。\nAutoTokenizer: 这是一个自动加载适当的分词器（tokenizer）的类。分词器用于将输入文本转换为模型可以处理的令牌（tokens）序列。\npipeline: 这是 Hugging Face 的高层 API，它提供了各种 NLP 任务的预定义管道（pipeline），如文本分类、翻译、文本生成等。\n\nmode_name = 'liam168/trans-opus-mt-en-zh'\n\nmode_name: 这是一个字符串变量，存储了模型的名称或路径。这里的 liam168/trans-opus-mt-en-zh 是在 Hugging Face 模型库中的一个模型名称，表示一个预训练的从英语到中文翻译的模型。\n\nmodel = AutoModelWithLMHead.from_pretrained(mode_name)\n\nAutoModelWithLMHead.from_pretrained(mode_name): 这行代码加载了 liam168/trans-opus-mt-en-zh 模型的预训练权重和配置。from_pretrained 方法从 Hugging Face 的模型库下载（如果尚未下载）并加载该模型到内存中。\n\ntokenizer = AutoTokenizer.from_pretrained(mode_name)\n\nAutoTokenizer.from_pretrained(mode_name): 这行代码加载了与模型配套的分词器。分词器将输入的英文句子转换为模型所需的令牌（tokens），并且会执行必要的文本预处理。\n\ntranslation = pipeline(\"translation_en_to_zh\", model=model, tokenizer=tokenizer)\n\npipeline(\"translation_en_to_zh\", model=model, tokenizer=tokenizer): 这里使用了 pipeline 函数来创建一个翻译管道，指定了任务类型为 \"translation_en_to_zh\"（从英语到中文的翻译），并传入了之前加载的模型和分词器。这个管道封装了翻译任务的所有步骤，使得翻译文本变得简单且易于使用。\n\ntranslation('This is a introduction to Huggingface.')\n\ntranslation('This is a introduction to Huggingface.'): 这行代码调用了翻译管道，将输入的英文句子 \"This is a introduction to Huggingface.\" 翻译为中文。管道会自动执行以下步骤：\n\n使用 tokenizer 将输入的英文句子分词为令牌。\n将令牌输入到 model 中进行翻译。\n生成的中文令牌序列被解码成自然语言文本。\n\n\n最终输出会是 \"This is a introduction to Huggingface.\" 的中文翻译版本，例如 \"这是对 Huggingface 的介绍。\"（具体翻译结果可能有所不同，取决于模型的性能）。\n这段代码通过加载 Hugging Face 提供的预训练模型和分词器，实现了从英语到中文的自动翻译任务，并且通过使用高层的 pipeline API，简化了翻译任务的执行。",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hugging Face</span>"
    ]
  },
  {
    "objectID": "huggingface-models.html",
    "href": "huggingface-models.html",
    "title": "5  Hugging Face Model",
    "section": "",
    "text": "5.1 两种调用模型的方式\n查找 GPU 设备。\nimport torch\nfrom pprint import pprint\n\n# select the device for computation\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelse:\n    device = torch.device(\"cpu\")\n\nprint(f\"using device: {device}\")\n\nusing device: mps\n图片数据。\nimport io\nimport requests\nfrom PIL import Image\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hugging Face Model</span>"
    ]
  },
  {
    "objectID": "huggingface-models.html#两种调用模型的方式",
    "href": "huggingface-models.html#两种调用模型的方式",
    "title": "5  Hugging Face Model",
    "section": "",
    "text": "5.1.1 使用 Pipeline\n\n# Use a pipeline as a high-level helper\nfrom transformers import pipeline\n\nobject_detector = pipeline(\"object-detection\", model=\"facebook/detr-resnet-50\", device=device)\n\ndetection_results = object_detector(image)\npprint(detection_results)\n\n[{'box': {'xmax': 175, 'xmin': 40, 'ymax': 117, 'ymin': 70},\n  'label': 'remote',\n  'score': 0.9982202649116516},\n {'box': {'xmax': 368, 'xmin': 333, 'ymax': 187, 'ymin': 72},\n  'label': 'remote',\n  'score': 0.9960021376609802},\n {'box': {'xmax': 639, 'xmin': 0, 'ymax': 473, 'ymin': 1},\n  'label': 'couch',\n  'score': 0.9954743981361389},\n {'box': {'xmax': 314, 'xmin': 13, 'ymax': 470, 'ymin': 52},\n  'label': 'cat',\n  'score': 0.99880051612854},\n {'box': {'xmax': 640, 'xmin': 345, 'ymax': 368, 'ymin': 23},\n  'label': 'cat',\n  'score': 0.9986783862113953}]\n\n\n\n\n5.1.2 直接使用模型\n\n# Load model directly\nfrom transformers import AutoImageProcessor, AutoModelForObjectDetection\n\nimage_processor = AutoImageProcessor.from_pretrained(\"facebook/detr-resnet-50\", device=device)\nmodel = AutoModelForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hugging Face Model</span>"
    ]
  },
  {
    "objectID": "huggingface-models.html#对象检测",
    "href": "huggingface-models.html#对象检测",
    "title": "5  Hugging Face Model",
    "section": "5.2 对象检测",
    "text": "5.2 对象检测\nDEtection TRansformer（DETR）模型，通过端到端训练在 COCO 2017 对象检测数据集上进行训练（包含 118K 张标注图像）。\nDETR 模型是一种具有卷积骨干的编码器-解码器变换器。在解码器输出之上添加了两个头部，以执行对象检测：一个线性层用于类别标签，一个多层感知器（MLP）用于边界框。该模型使用所谓的对象查询来检测图像中的对象。每个对象查询都在寻找图像中的特定对象。对于 COCO，设置的对象查询数量为 100。\n模型使用“二部匹配损失”进行训练：将预测的 N=100 个对象查询中的每个类别的预测框与 ground truth 注释进行比较，填充到相同的长度 N（如果一张图片只包含 4 个对象，那么 96 个注释将只是“无对象”作为类别，“无框”作为框）。匈牙利匹配算法用于在每个 N 查询和每个 N 注释之间创建最优的一对一映射。接下来，使用标准交叉熵（对于类别）以及 L1 和通用 IoU 损失的线性组合（对于框）来优化模型参数。\n下面是使用第一种调用方式调用 DETR 模型时结果的处理示例。\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.patches as patches\n\ndef random_color():\n    \"\"\"Generate a random color.\"\"\"\n    return np.random.rand(3,)\n\n# Create a figure and axis for plotting\nfig, ax = plt.subplots(1, 1, figsize=(12, 8))\n\n# Display the original image\nax.imshow(image)\n\n# Overlay bounding boxes and labels with random colors\nfor result in detection_results:\n    score = result['score']\n    label = result['label']\n    box = result['box']\n    \n    # Generate a random color\n    color = random_color()\n    \n    # Draw bounding box\n    rect = patches.Rectangle(\n        (box['xmin'], box['ymin']),\n        box['xmax'] - box['xmin'],\n        box['ymax'] - box['ymin'],\n        linewidth=2,\n        edgecolor=color,\n        facecolor='none'\n    )\n    ax.add_patch(rect)\n    \n    # Draw label and score with the same color as the rectangle\n    label_text = f\"{label}: {score:.2f}\"\n    ax.text(\n        box['xmin'],\n        box['ymin'] - 10,\n        label_text,\n        color=color,\n        fontsize=12,\n        bbox=dict(facecolor='white', alpha=0.5, \n                  edgecolor=color, boxstyle='round,pad=0.5')\n    )\n\n\n# Hide axis\nplt.axis('off')\n\n# Show the plot with bounding boxes and labels\nplt.show()\n\n\n\n\n\n\n\n\n下面是对第二种调用方式结果处理的方法。\n\n# prepare image for the model\ninputs = image_processor(images=image, return_tensors=\"pt\")\n\n# forward pass\noutputs = model(**inputs)\n\n# convert outputs (bounding boxes and class logits) to COCO API\n# let's only keep detections with score &gt; 0.9\ntarget_sizes = torch.tensor([image.size[::-1]])\nresults = image_processor.post_process_object_detection(\n    outputs, \n    target_sizes=target_sizes, \n    threshold=0.9)[0]\n\nfor score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n    box = [round(i, 2) for i in box.tolist()]\n    print(\n            f\"Detected {model.config.id2label[label.item()]} with confidence \"\n            f\"{round(score.item(), 3)} at location {box}\"\n    )\n\nDetected remote with confidence 0.998 at location [40.16, 70.81, 175.55, 117.98]\nDetected remote with confidence 0.996 at location [333.24, 72.55, 368.33, 187.66]\nDetected couch with confidence 0.995 at location [-0.02, 1.15, 639.73, 473.76]\nDetected cat with confidence 0.999 at location [13.24, 52.05, 314.02, 470.93]\nDetected cat with confidence 0.999 at location [345.4, 23.85, 640.37, 368.72]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hugging Face Model</span>"
    ]
  },
  {
    "objectID": "huggingface-models.html#yolov8",
    "href": "huggingface-models.html#yolov8",
    "title": "5  Hugging Face Model",
    "section": "5.3 YOLOv8",
    "text": "5.3 YOLOv8\nYOLOv8 需要使用 pip 或者 conda 安装。安装后提供 cli 和 Python 等两种运行方式。详情参见：https://docs.ultralytics.com/quickstart/。",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hugging Face Model</span>"
    ]
  },
  {
    "objectID": "huggingface-models.html#vit-图像分类",
    "href": "huggingface-models.html#vit-图像分类",
    "title": "5  Hugging Face Model",
    "section": "5.4 ViT 图像分类",
    "text": "5.4 ViT 图像分类\nThe Vision Transformer（ViT）是一种以监督方式在大量图像集合（即 ImageNet - 21k）上进行预训练的 Transformer 编码器模型（类似于 BERT），图像分辨率为 224x224 像素。接下来，该模型在 ImageNet（也称为 ILSVRC2012）上进行微调，这是一个包含 100 万张图像和 1000 个类别的数据集，图像分辨率同样为 224x224。\n图像作为一系列固定大小的补丁（分辨率为 16x16）呈现给模型，这些补丁是线性嵌入的。还在序列开头添加一个[CLS]标记，用于分类任务。在将序列输入到 Transformer 编码器的层之前，还添加了绝对位置嵌入。\n通过对模型进行预训练，它学习到图像的内部表示，然后可用于提取对下游任务有用的特征：例如，如果您有一个带标签图像的数据集，则可以通过在预训练的编码器顶部放置一个线性层来训练标准分类器。通常会在 [CLS] 标记的顶部放置一个线性层，因为此标记的最后一个隐藏状态可以视为整个图像的表示。\n\nimage_classifier = pipeline(\"image-classification\", \n                            model=\"google/vit-base-patch16-224\", \n                            device=device)\n\nclass_results = image_classifier(image)\npprint(class_results)\n\n[{'label': 'Egyptian cat', 'score': 0.9374417066574097},\n {'label': 'tabby, tabby cat', 'score': 0.038442544639110565},\n {'label': 'tiger cat', 'score': 0.014411404728889465},\n {'label': 'lynx, catamount', 'score': 0.003274324582889676},\n {'label': 'Siamese cat, Siamese', 'score': 0.000679593242239207}]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hugging Face Model</span>"
    ]
  },
  {
    "objectID": "huggingface-models.html#vit-特征提取",
    "href": "huggingface-models.html#vit-特征提取",
    "title": "5  Hugging Face Model",
    "section": "5.5 ViT 特征提取",
    "text": "5.5 ViT 特征提取\n视觉转换器（ViT）模型在 ImageNet-21k 上预训练，包含 1.4 亿张图片和 21843 个类别。其分辨率为 224 x 224。\n\nfrom transformers import ViTImageProcessor, ViTModel\nfrom PIL import Image\nimport requests\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\nmodel = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\ninputs = processor(images=image, return_tensors=\"pt\")\n\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state\n\nBaseModelOutputWithPooling 是 Hugging Face 的 transformers 库中的一个类，用于模型输出的表示。这个类通常在模型返回的输出中包含了池化层的结果，这对于一些任务，比如文本分类或嵌入生成，特别有用。\n\n5.5.1 主要功能\nBaseModelOutputWithPooling 类是从 BaseModelOutput 派生而来的，它包含以下几个重要的组件：\n\nlast_hidden_state：模型在所有隐藏层的输出，这些输出通常用于获取序列的特征表示。\npooler_output：经过池化层（通常是池化后的第一个 token）的输出，用于获得序列的整体表示。对于 BERT 等模型，这通常是 [CLS] token 的输出经过池化操作的结果。\nhidden_states（可选）：模型在每个隐藏层的输出（如果 output_hidden_states=True 时会返回）。\n\n\n\n5.5.2 用途\n\npooler_output：这个输出是用来获取序列的整体表示的，例如用于分类任务。对于很多预训练模型来说，这个输出是对 [CLS] token 的表示经过池化后的结果。\nlast_hidden_state：如果你需要对每个 token 的表示进行进一步的处理或分析（例如，进行序列标注任务），这个输出将是有用的。\n\n\n\n5.5.3 示例\n以下是如何在使用 Hugging Face 模型时，利用 BaseModelOutputWithPooling 获取模型输出的一个例子：\n\n# Extract the output\nlast_hidden_state = outputs.last_hidden_state  # Shape: [batch_size, sequence_length, hidden_size]\npooler_output = outputs.pooler_output  # Shape: [batch_size, hidden_size]\n\nprint(\"Last hidden state:\", last_hidden_state.shape)\nprint(\"Pooler output:\", pooler_output.shape)\n\nLast hidden state: torch.Size([1, 197, 768])\nPooler output: torch.Size([1, 768])\n\n\n\n\n5.5.4 说明：\n\nlast_hidden_state：通常是三维张量，形状为 [batch_size, sequence_length, hidden_size]。\npooler_output：通常是二维张量，形状为 [batch_size, hidden_size]，用于表示整个序列的特征。\n\nBaseModelOutputWithPooling 是一个结构化的返回对象，帮助你从模型中提取有用的特征表示，特别是当需要处理序列数据时。",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hugging Face Model</span>"
    ]
  },
  {
    "objectID": "chatanywhere.html",
    "href": "chatanywhere.html",
    "title": "6  ChatAnyWhere 服务",
    "section": "",
    "text": "6.1 模型列表\nChatAnyWhere 提供的模型名字可能会与 OpenAI 不同。例如 *-ca 模型会有更优惠的价格。为了配置调用参数时写对模型名称，需要查询提供的模型列表。\nlibrary(httr)\nlibrary(glue)\n\n# 读取 CHATANYWHERE_API_KEY 环境变量\nOPENAI_API_KEY = Sys.getenv(\"CHATANYWHERE_API_KEY\")\n\nheaders = c(\n   'Authorization' = glue('Bearer {OPENAI_API_KEY}'),\n   'User-Agent' = 'Apifox/1.0.0 (https://apifox.com)',\n   'Content-Type' = 'application/json'\n)\n\n# 使用 GET 方法获取\nres &lt;- VERB(\"GET\", \n            url = \"https://api.chatanywhere.tech/v1/models\", \n            add_headers(headers))\n将 JSON 输出为表格。\nlibrary(jsonlite)\nlibrary(tidyverse)\nlibrary(gt)\n\nmodels = jsonlite::fromJSON(content(res, as = \"text\", encoding = \"UTF-8\"))$data\n\nmodels |&gt; \n  mutate(created = as_datetime(created) |&gt; as_date()) |&gt; \n  gt(groupname_col = \"owned_by\")\n\n\n\n\n\n\n\nid\nobject\ncreated\n\n\n\n\nopenai-internal\n\n\ntext-embedding-ada-002\nmodel\n2022-12-16\n\n\ngpt-3.5-turbo-16k\nmodel\n2023-05-10\n\n\nwhisper-1\nmodel\n2023-02-27\n\n\ntts-1\nmodel\n2023-04-19\n\n\nsystem\n\n\ntext-embedding-3-small\nmodel\n2024-01-22\n\n\ntext-embedding-3-large\nmodel\n2024-01-22\n\n\ngpt-3.5-turbo-0125\nmodel\n2024-01-23\n\n\ngpt-3.5-turbo-1106\nmodel\n2023-11-02\n\n\ngpt-3.5-turbo-instruct\nmodel\n2023-08-24\n\n\ngpt-3.5-turbo-instruct-0914\nmodel\n2023-09-07\n\n\ngpt-4o\nmodel\n2024-05-10\n\n\ngpt-4o-2024-05-13\nmodel\n2024-05-10\n\n\nchatgpt-4o-latest\nmodel\n2024-05-10\n\n\ngpt-4o-2024-08-06\nmodel\n2024-05-10\n\n\ngpt-4o-mini\nmodel\n2024-05-10\n\n\ngpt-4o-mini-2024-07-18\nmodel\n2024-05-10\n\n\ngpt-4-turbo\nmodel\n2024-04-05\n\n\ngpt-4-turbo-2024-04-09\nmodel\n2024-04-08\n\n\ngpt-4-turbo-preview\nmodel\n2024-01-23\n\n\ngpt-4-1106-preview\nmodel\n2023-11-02\n\n\ngpt-4-0125-preview\nmodel\n2024-01-23\n\n\ngpt-4-vision-preview\nmodel\n2023-11-02\n\n\ngpt-4-1106-vision-preview\nmodel\n2024-03-26\n\n\ntts-1-1106\nmodel\n2023-11-03\n\n\ntts-1-hd\nmodel\n2023-11-03\n\n\ntts-1-hd-1106\nmodel\n2023-11-03\n\n\ndall-e-2\nmodel\n2023-11-01\n\n\ndall-e-3\nmodel\n2023-10-31\n\n\nca\n\n\ngpt-3.5-turbo-ca\nmodel\n2024-01-23\n\n\ngpt-4o-ca\nmodel\n2024-01-23\n\n\ngpt-4-turbo-ca\nmodel\n2024-01-23\n\n\ngpt-4-turbo-preview-ca\nmodel\n2024-01-23\n\n\no1-mini-ca\nmodel\n2024-01-23\n\n\no1-preview-ca\nmodel\n2024-01-23\n\n\ngpt-4-ca\nmodel\n2023-03-12\n\n\nclaude-3-5-sonnet-20240620\nmodel\n2023-03-12\n\n\nopenai\n\n\ngpt-3.5-turbo\nmodel\n2023-02-28\n\n\ngpt-3.5-turbo-0613\nmodel\n2023-06-12\n\n\ngpt-3.5-turbo-0301\nmodel\n2023-03-01\n\n\ngpt-3.5-turbo-16k-0613\nmodel\n2023-05-30\n\n\ngpt-4\nmodel\n2023-03-12\n\n\ngpt-4-0613\nmodel\n2023-06-12\n下面依次介绍这些模型的用法。",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ChatAnyWhere 服务</span>"
    ]
  },
  {
    "objectID": "chatanywhere.html#对话模型",
    "href": "chatanywhere.html#对话模型",
    "title": "6  ChatAnyWhere 服务",
    "section": "6.2 对话模型",
    "text": "6.2 对话模型\n以 gpt-* 开头的都是文本对话模型。调用 OpenAI 的模型时，通常需要配置以下一些关键参数来控制模型的行为和生成结果的方式。\n\nPrompt（提示）\n\n模型的输入文本，通常称为“提示”。\n可以是简单的文本，或者带有一些问题或任务描述，告诉模型生成哪类内容。\n\nMax Tokens（最大令牌数）\n\n指定生成的文本中最多包含多少个令牌（tokens）。一个令牌大约对应一个英文单词或标点符号。\n该参数可以控制生成的响应长度，但包括输入和输出在内的令牌总数不能超过模型的上下文长度限制。\n\nTemperature（温度）\n\n控制生成文本的随机性。范围是 0 到 2：\n\ntemperature=0 时，输出更加确定和保守，偏向生成常见的或“最可能”的答案。\n较高的 temperature 值（如 0.7）会让生成的内容更加随机和多样化。\n\n\nn（生成次数）\n\n控制生成多少个不同的响应。\nn=1 只生成一个响应；n=2 会生成多个响应，适合比较或选择最合适的内容。\n\n\n调用 OpenAI 模型时，通常需要设置 模型名称、提示、最大令牌数、温度、top-p、生成次数 等参数，视任务需求还可以调整 出现惩罚、频率惩罚、停止序列 等其他配置，以控制生成的内容质量和行为。\n下面这个例子，展示了 ChatGPT 数不清楚“temperature”这个单词里面有几个字母“e”。\n\nbody = '{\n   \"model\": \"gpt-4o-mini\",\n   \"messages\": [\n      {\n         \"role\": \"system\",\n         \"content\": \"You are a helpful assistant.\"\n      },\n      {\n         \"role\": \"user\",\n         \"content\": \"Temperature这个单词中含有几个字母e？\"\n      }\n   ],\n   \"temperature\": 2\n}';\n\nres &lt;- VERB(\"POST\", \n            url = \"https://api.chatanywhere.tech/v1/chat/completions\", \n            body = body, \n            add_headers(headers))\n\ncontent(res, 'text', encoding = \"UTF-8\") |&gt; \n  fromJSON() |&gt; \n  str()\n\nList of 7\n $ id                : chr \"chatcmpl-ABwBJsmRdhuBlKfEHdMtFwwASkSpK\"\n $ object            : chr \"chat.completion\"\n $ created           : int 1727408621\n $ model             : chr \"gpt-4o-mini-2024-07-18\"\n $ choices           :'data.frame': 1 obs. of  4 variables:\n  ..$ index        : int 0\n  ..$ message      :'data.frame':   1 obs. of  2 variables:\n  .. ..$ role   : chr \"assistant\"\n  .. ..$ content: chr \"\\\"Temperature\\\"这个单词中有两个字母e。\"\n  ..$ logprobs     : logi NA\n  ..$ finish_reason: chr \"stop\"\n $ usage             :List of 4\n  ..$ prompt_tokens            : int 29\n  ..$ completion_tokens        : int 13\n  ..$ total_tokens             : int 42\n  ..$ completion_tokens_details:List of 1\n  .. ..$ reasoning_tokens: int 0\n $ system_fingerprint: chr \"fp_1bb46167f9\"",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ChatAnyWhere 服务</span>"
    ]
  },
  {
    "objectID": "chatanywhere.html#使用-openai-api",
    "href": "chatanywhere.html#使用-openai-api",
    "title": "6  ChatAnyWhere 服务",
    "section": "6.3 使用 OpenAI API",
    "text": "6.3 使用 OpenAI API\n\nfrom openai import OpenAI\nimport os\n\n# 创建 client\nclient = OpenAI(\n    api_key=os.getenv(\"CHATANYWHERE_API_KEY\"), # 如果您没有配置环境变量，请在此处用您的API Key进行替换\n    base_url=\"https://api.chatanywhere.tech\",  # 填写 openAI 服务的 base_url\n)\n\n# 生成对话\ncompletion = client.chat.completions.create(\n    model=\"gpt-4o-ca\",\n    messages=[\n        {'role': 'system', 'content': 'You are a helpful assistant.'},\n        {'role': 'user', 'content': '你是谁'}],\n    temperature=0.8,\n    top_p=0.8\n    )\n\nprint(completion.choices[0].message.content)\n\n我是一个由人工智能驱动的助手，旨在帮助你回答问题、提供信息和完成任务。有任何问题或需要帮助的地方，请随时告诉我！\n\n\n\n6.3.1 解析文件\nChatAnyWhere 的服务器是不是不支持下面这个命令？\n\nfrom pathlib import Path\n\nassistant = client.beta.assistants.create(\n  name=\"Document Reader Assistant\",\n  instructions=\"You are an document reader\",\n  model=\"gpt-4o\",\n  tools=[{\"type\": \"file_search\"}],\n)\n\nfile_object = client.files.create(file=open(Path(\"example/Kraken2.pdf\"),\"r\"), \n                                  purpose=\"assistants\")\n\nprint(f\"文件上传成功，文件ID: {file_object.id}\")\n\n\n\n6.3.2 使用 Kimi 服务器解析\n\nfrom pathlib import Path\nfrom openai import OpenAI\nimport os\n\nclient = OpenAI(\n   api_key = os.getenv(\"MOONSHOT_API_KEY\"),  # 替换为你的API密钥\n   base_url = \"https://api.moonshot.cn/v1\",\n)\n\npdf_file_path = Path(\"example/Kraken2.pdf\")\n\n# 上传文件\nfile_object = client.files.create(file = pdf_file_path, \n                                  purpose = \"file-extract\")\n\nprint(f\"文件上传成功，文件ID: {file_object.id}\")\n\n文件上传成功，文件ID: crshkmalve9pdev71plg\n\n# 获取文件内容\nfile_content = client.files.content(file_id=file_object.id).text\n\n# 打印前 200 字\nprint(file_content[:200])\n\n{\"content\":\"利⽤宏基因组的reads进⾏物种注释的常⽤⼯具有很多，它们通过⽐对reads到参考数据库来识别微⽣物的分类信\\n息。以下是⼏种常⽤的⼯具：\\n1. Kraken2\\n⼯作原理：基于k-mer⽐对技术，将reads⽐对到参考数据库中的特定k-mer集合，以实现快速的分类注\\n释。\\n优点：速度⾮常快，能处理⼤规模数据。\\n数据库：Kraken2允许使⽤标准数据库（如NCBI \n\n\n\nfrom IPython.display import Markdown\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"你是Kimi，由Moonshot AI提供的人工智能助手。\"\n    },\n    {\n        \"role\": \"system\",\n        \"content\": file_content  # 将文件内容作为系统提示\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"总结一下这份文件中的内容\"\n    }\n]\n\n# 发送对话请求\nresponse = client.chat.completions.create(\n    model=\"moonshot-v1-32k\",\n    messages=messages,\n    temperature=0.3,\n)\n\n# 打印回答\nMarkdown(response.choices[0].message.content)\n\n这份文件是关于如何使用宏基因组的reads进行物种注释的指南，主要介绍了几种常用的工具，包括Kraken2、MetaPhlAn3和Centrifuge。以下是对这些工具的总结：\n\nKraken2：\n\n工作原理：基于k-mer比对技术，将reads比对到参考数据库中的特定k-mer集合。\n优点：速度快，能处理大规模数据。\n数据库：可以使用标准数据库或自定义数据库。\n输出：物种分类信息和物种相对丰度。\nStandard-8数据库：为资源有限的环境优化，数据库大小限制为8GB，针对常见物种优化，快速分析。\n\nMetaPhlAn3：\n\n工作原理：使用已知微生物的标记基因进行比对。\n优点：适合分析物种相对丰度，计算效率高。\n数据库：使用MetaPhlAn自带的标记基因数据库。\n输出：物种分类和丰度表。\n\nCentrifuge：\n\n工作原理：基于压缩索引技术，将reads与压缩参考数据库中的基因组序列进行比对。\n优点：处理大数据库速度快，适合处理低质量reads。\n数据库：使用Centrifuge提供的参考数据库或自定义数据库。\n输出：物种分类和丰度估计。\n\n\n文件还提供了使用Kraken2进行宏基因组测序数据物种注释分析的步骤，包括安装Kraken2、下载数据库、运行Kraken2进行分类注释以及分析结果。此外，还提供了如何使用R语言的ggplot2库对分类结果进行可视化的示例代码。\n最后，文件提供了一个Kraken2输出报告的示例，显示了样本中不同分类单元的相对丰度，并展示了如何使用R语言绘制分类结果的柱状图。",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ChatAnyWhere 服务</span>"
    ]
  },
  {
    "objectID": "chatanywhere.html#词嵌入模型",
    "href": "chatanywhere.html#词嵌入模型",
    "title": "6  ChatAnyWhere 服务",
    "section": "6.4 词嵌入模型",
    "text": "6.4 词嵌入模型\n所有三个词嵌入模型都是基于 Transformer 架构，这使得它们在处理自然语言时具有良好的性能。下表比较了 text-embedding-ada-002、text-embedding-3-small 和 text-embedding-3-large 这三个词嵌入模型的特点：\n\n\n\n\n\n\n\n\n\n\n\n\n模型名称\n参数量\n嵌入维度\n性能\n适用场景\n优点\n缺点\n\n\n\n\ntext-embedding-ada-002\n中等\n1536\n高效，性能优秀\n通用文本嵌入，适用于广泛的 NLP 任务\n高精度嵌入，适合各种语义匹配任务\n相比小型模型，计算资源需求较高\n\n\ntext-embedding-3-small\n小\n512\n较快，资源效率高\n资源受限的应用场景，低计算成本的嵌入生成\n计算效率高，适合实时或资源有限的场景\n嵌入维度较低，可能影响语义表达能力\n\n\ntext-embedding-3-large\n大\n2048\n更高性能，精度极高\n高端应用场景，如高精度语义搜索和推荐系统\n嵌入维度更高，能够捕捉复杂语义关系\n资源消耗大，适合计算资源充足的场景\n\n\n\n\nbody = '{\n   \"model\": \"text-embedding-ada-002\",\n   \"input\": \"The food was delicious and the waiter...\"\n}';\n\nres = VERB(\"POST\", \n           url = \"https://api.chatanywhere.tech/v1/embeddings\", \n           body = body, \n           add_headers(headers))\n\ncontent = content(res, 'text', encoding = \"UTF-8\")\n\nembedding = fromJSON(content)\nstr(embedding)\n\nList of 4\n $ object: chr \"list\"\n $ data  :'data.frame': 1 obs. of  3 variables:\n  ..$ object   : chr \"embedding\"\n  ..$ embedding:List of 1\n  .. ..$ : num [1:1536] 0.00238 -0.00932 0.01569 -0.00774 -0.00472 ...\n  ..$ index    : int 0\n $ model : chr \"text-embedding-ada-002\"\n $ usage :List of 3\n  ..$ prompt_tokens    : int 8\n  ..$ completion_tokens: int 0\n  ..$ total_tokens     : int 8",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ChatAnyWhere 服务</span>"
    ]
  },
  {
    "objectID": "chatanywhere.html#文生图模型",
    "href": "chatanywhere.html#文生图模型",
    "title": "6  ChatAnyWhere 服务",
    "section": "6.5 文生图模型",
    "text": "6.5 文生图模型\ndall-e-2 和 dall-e-3 是文生图模型。\nDALL-E 2 支持以下三种图像尺寸：\n\n256x256\n512x512\n1024x1024\n\nDALL-E 3 支持以下图像尺寸：\n\n1024x1024: 正方形图像，适合大多数使用场景，是默认推荐的尺寸。\n1792x1024: 宽屏图像，适合需要更宽视野的场景或横向布局的设计。\n1024x1792: 纵向图像，适合需要更高视野的场景或纵向布局的设计。\n\n你可以根据具体需求选择合适的图像尺寸进行生成。\n\nurl = \"https://api.chatanywhere.tech/v1/images/generations\"\n\nbody = '{\n   \"prompt\": \"A colorful sunset over the snow mountains\",\n   \"n\": 1,\n   \"model\":  \"dall-e-2\",\n   \"size\": \"256x256\"\n}';\n\nresponse = VERB(\"POST\", url, body = body, add_headers(headers))\n\ncontent = content(response, \"text\", encoding = \"UTF-8\")\nprint(content)\n\n[1] \"{\\\"created\\\":1727601271,\\\"data\\\":[{\\\"url\\\":\\\"https://oaidalleapiprodscus.blob.core.windows.net/private/org-iEnzkgXz3MLYFDoiaMsyPKYw/user-4vHAQk9cYlLmeDvsuy2iKvDZ/img-rtLChxv4kEKdWs8fWdKyHaLc.png?st=2024-09-29T08%3A14%3A31Z&se=2024-09-29T10%3A14%3A31Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=d505667d-d6c1-4a0a-bac7-5c84a87759f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2024-09-29T01%3A03%3A42Z&ske=2024-09-30T01%3A03%3A42Z&sks=b&skv=2024-08-04&sig=A2h8h/24EXYfTQXp4jrt2794eopKRKIQUypio0ccnNU%3D\\\"}]}\"\n\n\n获取图片。\n# 获取生成的图像 URL\nimage_url = fromJSON(content)[[\"data\"]][[\"url\"]]\n\n# 下载图片\nresponse &lt;- GET(image_url)\n\n# 检查请求是否成功\nif (status_code(response) == 200) {\n  # 将图片保存到磁盘\n  writeBin(content(response, \"raw\"), \"output/sunset.png\")\n  cat(\"图片已成功保存到 `output/sunset.png`。\")\n} else {\n  cat(\"下载图片失败，状态码：\", status_code(response), \"\\n\")\n}\n图片已成功保存到 output/sunset.png。",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ChatAnyWhere 服务</span>"
    ]
  },
  {
    "objectID": "chatanywhere.html#识图功能不支持",
    "href": "chatanywhere.html#识图功能不支持",
    "title": "6  ChatAnyWhere 服务",
    "section": "6.6 识图功能（不支持）",
    "text": "6.6 识图功能（不支持）\n使用多模态模型，可以识别图片中的信息。\n\n# 设置请求体\nbody = list(\n  model = \"gpt-4o-ca\",\n  file = upload_file(\"output/sunset.png\"),  # 文件路径\n  prompt = \"这是什么?\",  # 提示\n  encode = \"multipart\"\n  )\n\nres = VERB(\"POST\", \n            url = \"https://api.chatanywhere.tech/v1/chat/completions\", \n            body = body, \n            add_headers(headers))\n\ncat(content(res, 'text', encoding = \"UTF-8\"))\n\n{\"error\":{\"message\":\"JSON parse error: Unexpected character ('-' (code 45)) in numeric value: expected digit (0-9) to follow minus sign, for valid numeric value; nested exception is com.fasterxml.jackson.core.JsonParseException: Unexpected character ('-' (code 45)) in numeric value: expected digit (0-9) to follow minus sign, for valid numeric value\\n at [Source: (org.springframework.util.StreamUtils$NonClosingInputStream); line: 1, column: 3]\",\"type\":\"chatanywhere_error\",\"param\":null,\"code\":\"400 BAD_REQUEST\"}}",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ChatAnyWhere 服务</span>"
    ]
  },
  {
    "objectID": "chatanywhere.html#文字转语音模型",
    "href": "chatanywhere.html#文字转语音模型",
    "title": "6  ChatAnyWhere 服务",
    "section": "6.7 文字转语音模型",
    "text": "6.7 文字转语音模型\n将一段文字转变为语音，支持中英文混合。\n\nbody = '{\n   \"model\": \"tts-1\",\n   \"input\": \"今天天气不错。It is a nice day today.\",\n   \"voice\": \"alloy\"\n}';\n\nres &lt;- VERB(\"POST\", \n            url = \"https://api.chatanywhere.tech/v1/audio/speech\", \n            body = body, \n            add_headers(headers))\n\n# 检查请求是否成功\nif (status_code(res) == 200) {\n  # 将响应保存为音频文件（假设返回的是二进制音频数据）\n  audio_file &lt;- \"output/audio-goodday.mp3\"  # 你可以更改文件名和扩展名\n  writeBin(content(res, \"raw\"), audio_file)\n  message(\"Audio saved successfully as: \", audio_file)\n} else {\n  message(\"Request failed with status: \", status_code(res))\n}",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ChatAnyWhere 服务</span>"
    ]
  },
  {
    "objectID": "chatanywhere.html#语音识别模型",
    "href": "chatanywhere.html#语音识别模型",
    "title": "6  ChatAnyWhere 服务",
    "section": "6.8 语音识别模型",
    "text": "6.8 语音识别模型\nwhisper-1 是 OpenAI 开发的一个强大的语音识别模型。它主要用于将语音转换为文本（也称为语音转文字，Speech-to-Text，简称 STT）。该模型能够处理多种语言的语音输入，并能够识别不同的口音和语音风格，非常适用于各种音频转录任务。\n\nheaders_multipart = c(\n   'Authorization' = glue('Bearer {OPENAI_API_KEY}'),\n   'User-Agent' = 'Apifox/1.0.0 (https://apifox.com)',\n   'Content-Type' = 'multipart/form-data'\n)\n\nbody = list(\n   'file' = upload_file('output/audio-goodday.mp3'),\n   'model' = 'whisper-1',\n   'prompt' = 'eiusmod nulla',\n   'response_format' = 'json',\n   'temperature' = '0',\n   'language' = ''\n)\n\nres = VERB(\"POST\", \n            url = \"https://api.chatanywhere.tech/v1/audio/transcriptions\", \n            body = body, \n            add_headers(headers_multipart),\n            encode = 'multipart')\n\ncat(content(res, 'text', encoding = \"UTF-8\"))\n\n{\n  \"text\": \"今年天氣不錯 It is a nice day today\"\n}",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ChatAnyWhere 服务</span>"
    ]
  },
  {
    "objectID": "chatanywhere.html#claude-模型无效",
    "href": "chatanywhere.html#claude-模型无效",
    "title": "6  ChatAnyWhere 服务",
    "section": "6.9 Claude 模型（无效）",
    "text": "6.9 Claude 模型（无效）\nChatAnywhere 提供了一个 claude-3-5-sonnet-20240620 模型。\n\nbody = '{\n   \"model\": \"claude-3-5-sonnet-20240620\",\n   \"messages\": [\n      {\n         \"role\": \"system\",\n         \"content\": \"You are a helpful assistant.\"\n      },\n      {\n         \"role\": \"user\",\n         \"content\": \"Temperature这个单词中含有几个字母e？\"\n      }\n   ],\n   \"temperature\": 2\n}';\n\nres &lt;- VERB(\"POST\", \n            url = \"https://api.chatanywhere.tech/v1/chat/completions\", \n            body = body, \n            add_headers(headers))\n\ncontent(res, 'text', encoding = \"UTF-8\") |&gt; \n  fromJSON() |&gt; \n  str()\n\nList of 1\n $ error:List of 4\n  ..$ message: chr \"模型无返回结果，可能是内容违规、输入过长、输入格式有误或负载较高，请检查后再试。No response, please try again.\"\n  ..$ type   : chr \"chatanywhere_error\"\n  ..$ param  : NULL\n  ..$ code   : chr \"503 SERVICE_UNAVAILABLE\"",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ChatAnyWhere 服务</span>"
    ]
  },
  {
    "objectID": "chatanywhere.html#语音翻译模型无效",
    "href": "chatanywhere.html#语音翻译模型无效",
    "title": "6  ChatAnyWhere 服务",
    "section": "6.10 语音翻译模型（无效）",
    "text": "6.10 语音翻译模型（无效）\n将音频翻译成英文。\n\nbody = list(\n   'file' = upload_file('output/audio-goodday.mp3'),\n   'model' = 'whisper-1',\n   'prompt' = '',\n   'response_format' = 'json',\n   'temperature' = '0'\n)\n\nres &lt;- VERB(\"POST\", \n            url = \"https://api.chatanywhere.tech/v1/audio/translations\", \n            body = body, \n            add_headers(headers_multipart), \n            encode = 'multipart')\n\ncat(content(res, 'text', encoding = \"UTF-8\"))\n\n{\n  \"text\": \"今天天气不错。It is a nice day today.\"\n}",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ChatAnyWhere 服务</span>"
    ]
  },
  {
    "objectID": "chatanywhere.html#用量查询无效",
    "href": "chatanywhere.html#用量查询无效",
    "title": "6  ChatAnyWhere 服务",
    "section": "6.11 用量查询（无效）",
    "text": "6.11 用量查询（无效）\n\nbody = '{\n   \"model\": \"gpt-4o-mini\",\n   \"hours\": 24\n}';\n\nres &lt;- VERB(\"POST\", \n            url = \"https://api.chatanywhere.tech/v1/query/usage_details\", \n            body = body, \n            add_headers(headers))\n\ncat(content(res, 'text', encoding = \"UTF-8\"))\n\n{\"error\":{\"message\":\"wrong api key\",\"type\":\"chatanywhere_error\",\"param\":null,\"code\":\"401 UNAUTHORIZED\"}}",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ChatAnyWhere 服务</span>"
    ]
  },
  {
    "objectID": "chatanywhere.html#自动补全无效",
    "href": "chatanywhere.html#自动补全无效",
    "title": "6  ChatAnyWhere 服务",
    "section": "6.12 自动补全（无效）",
    "text": "6.12 自动补全（无效）\n文档里说：“给定一个提示，该模型将返回一个或多个预测的完成，并且还可以返回每个位置的替代标记的概率”。但是，实际上这个接口不可用。\n\nbody = '{\n   \"model\": \"gpt-4o-mini\",\n   \"prompt\": \"Say this is a test\",\n   \"max_tokens\": 7,\n   \"temperature\": 0,\n   \"top_p\": 1,\n   \"n\": 1,\n   \"stream\": false,\n   \"logprobs\": null,\n   \"stop\": \"\\\\n\"\n}';\n\nres &lt;- VERB(\"POST\", url = \"https://api.chatanywhere.tech/v1/completions\", \n            body = body, \n            add_headers(headers))\n\n# 优雅地输出 JSON\ncontent(res, 'text', encoding = 'UTF-8') |&gt; \n  jsonlite::prettify()\n\n{\n    \"error\": {\n        \"message\": \"This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?\",\n        \"type\": \"invalid_request_error\",\n        \"param\": \"model\",\n        \"code\": null\n    }\n}",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ChatAnyWhere 服务</span>"
    ]
  },
  {
    "objectID": "moonshot-kimi.html",
    "href": "moonshot-kimi.html",
    "title": "7  Moonshot Kimi",
    "section": "",
    "text": "7.1 支持的模型\nKimi 的模型具有 3 种不同规格的上下文长度。\nfrom openai import OpenAI\nimport os\nfrom IPython.display import Markdown\nfrom pprint import pprint\n\n \nMOONSHOT_API_KEY = os.getenv(\"MOONSHOT_API_KEY\")\n\nclient = OpenAI(\n    api_key = MOONSHOT_API_KEY,\n    base_url = \"https://api.moonshot.cn/v1\",\n)\n \nmodel_list = client.models.list()\nmodel_data = model_list.data\n \nfor i, model in enumerate(model_data):\n    print(f\"model[{i}]:\", model.id)\n\nmodel[0]: moonshot-v1-8k\nmodel[1]: moonshot-v1-128k\nmodel[2]: moonshot-v1-32k\nmodel[3]: moonshot-v1-auto\n作为 Moonshot AI 开发的人工智能助手，Kimi 具备一些独特的技能和特点，这些可能与文心一言、通义千问等其他AI助手有所不同。",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Moonshot Kimi</span>"
    ]
  },
  {
    "objectID": "moonshot-kimi.html#支持的模型",
    "href": "moonshot-kimi.html#支持的模型",
    "title": "7  Moonshot Kimi",
    "section": "",
    "text": "长文本处理：我能够处理最多 20 万字的输入和输出（moonshot-v1-128k），这使得 Kimi 能够处理长篇文章、报告和代码等。\n文件阅读和解析：用户可以上传多种格式的文件，如TXT、PDF、Word文档、PPT幻灯片和Excel电子表格，Kimi 可以阅读这些文件的内容并据此回答问题。\n搜索能力：Kimi 可以结合搜索结果来提供更全面和准确的回答。",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Moonshot Kimi</span>"
    ]
  },
  {
    "objectID": "moonshot-kimi.html#基于文档的对话",
    "href": "moonshot-kimi.html#基于文档的对话",
    "title": "7  Moonshot Kimi",
    "section": "7.2 基于文档的对话",
    "text": "7.2 基于文档的对话\nKimi 提供的文档上传功能采用 OpenAI 相同的接口标准。\n以下是使用 Python 代码和 OpenAI SDK 上传 PDF 文档并实现基于文档对话的步骤。\n\n7.2.1 安装 OpenAI SDK\n如果你还没有安装 OpenAI SDK，可以通过以下命令安装：\npip install --upgrade 'openai&gt;=1.0'\n\n\n7.2.2 上传文件\n编写代码上传文件：使用以下 Python 代码示例上传 PDF 文件。请确保将$MOONSHOT_API_KEY替换为你的实际API密钥。\n\nfrom pathlib import Path\nfrom openai import OpenAI\nimport os\n\nclient = OpenAI(\n   api_key = MOONSHOT_API_KEY,  # 替换为你的API密钥\n   base_url = \"https://api.moonshot.cn/v1\",\n)\n\n\n# 指定要上传的PDF文件路径\npdf_file_path = Path(\"example/Kraken2.pdf\")\n\n# 上传文件\nfile_object = client.files.create(file = pdf_file_path, \n                                  purpose = \"file-extract\")\n\nprint(f\"文件上传成功，文件ID: {file_object.id}\")\n\n文件上传成功，文件ID: crkm4io04rj08lpsiulg\n\n\n\n\n7.2.3 获取文件内容\n上传文件后，你可以使用文件ID来获取文件内容。\n\n# 获取文件内容\nfile_content = client.files.content(file_id=file_object.id).text\n\n\n# 打印前 200 字\npprint(file_content[:200], width = 72)\n\n('{\"content\":\"利⽤宏基因组的reads进⾏物种注释的常⽤⼯具有很多，它们通过⽐对reads到参考数据库来识别微⽣物的分类信\\\\n息。以下是⼏种常⽤的⼯具：\\\\n1. '\n 'Kraken2\\\\n⼯作原理：基于k-mer⽐对技术，将reads⽐对到参考数据库中的特定k-mer集合，以实现快速的分类注\\\\n释。\\\\n优点：速度⾮常快，能处理⼤规模数据。\\\\n数据库：Kraken2允许使⽤标准数据库（如NCBI ')\n\n\n\n\n7.2.4 使用文件内容对话\n使用文件内容进行对话是一种常见的场景，尤其是在需要结合特定文档内容来回答用户问题时。\n一旦你获取了文件内容，就可以将其作为输入传递给Kimi进行进一步的处理或分析。具体做法是将获取的文件内容作为系统提示（system message）加入到对话中，然后发送用户的问题。\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"你是Kimi，由Moonshot AI提供的人工智能助手。\"\n    },\n    {\n        \"role\": \"system\",\n        \"content\": file_content  # 将文件内容作为系统提示\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"总结一下这份文件中的内容\"\n    }\n]\n\n# 发送对话请求\nresponse = client.chat.completions.create(\n    model=\"moonshot-v1-32k\",\n    messages=messages,\n    temperature=0.3,\n)\n\n# 打印回答\nMarkdown(response.choices[0].message.content)\n\n这份文件是关于如何使用宏基因组学工具Kraken2进行物种注释的指南。以下是文件内容的总结：\n\n宏基因组物种注释工具：\n\nKraken2：基于k-mer比对技术，快速处理大规模数据，支持标准数据库和自定义数据库。\nMetaPhlAn (MetaPhlAn3)：使用已知微生物的标记基因进行比对，适合分析物种相对丰度。\nCentrifuge：基于压缩索引技术，处理较大数据库，适合低质量reads。\n\nKraken2的Standard-8数据库：\n\n数据库大小限制：限制为8 GB，适合资源有限的环境。\n针对常见物种优化：保留了常见和临床相关的微生物物种。\n快速分析：数据库较小，提高了分析速度和降低了资源需求。\n\nKraken2的使用步骤：\n\n安装Kraken2：通过BioConda安装。\n下载数据库：可以选择下载标准数据库或构建自定义数据库。\n运行Kraken2：使用命令行工具进行reads的分类注释。\n分析结果：输出包括物种分类信息和丰度。\n\nKraken2输出文件：\n\nsample_report.txt：包含物种分类的比例和统计信息。\nsample_output.txt：每条read的分类结果。\n\n可视化结果：\n\n使用R或Python等工具对分类结果进行可视化，如R中的ggplot2库。\n\n示例代码：\n\n提供了一个R脚本示例，用于读取Kraken2的报告文件并绘制前10个物种相对丰度的柱状图。\n\n\n文件还强调了Kraken2 Standard-8数据库的特点，包括其适用场景，如台式电脑本地分析、云计算环境中有存储限制的分析，以及需要快速获得初步结果的情况。\n\n\n\n\n7.2.5 发起追问\n如果你需要继续追问 PDF 文档中的内容，你可以在对话历史中添加用户的问题（user message），然后调用 Chat Completion 接口来获取模型的回答。\n\n# 发起追问\nfollow_up_question = \"这份文档中提到的关键技术是什么？\"\nmessages.append({\"role\": \"user\", \"content\": follow_up_question})\nfollow_up_response = client.chat.completions.create(\n    model=\"moonshot-v1-32k\",\n    messages=messages,\n    temperature=0.3,\n)\n\n# 打印追问的回答\nMarkdown(follow_up_response.choices[0].message.content)\n\n这份文档中提到的关键技术是宏基因组学中的物种注释工具，主要包括以下三种：\n\nKraken2：一种基于k-mer比对技术的物种注释工具，能够快速处理大规模数据，通过比对reads到参考数据库中的特定k-mer集合来实现分类注释。\nMetaPhlAn (MetaPhlAn3)：使用已知微生物的标记基因进行比对，专注于每个物种的标记基因进行分类，适合分析物种相对丰度，计算效率高。\nCentrifuge：基于压缩索引技术，将reads与压缩参考数据库中的基因组序列进行比对，适合处理低质量的reads，处理较大数据库并且比对速度快。\n\n文档还特别介绍了Kraken2的Standard-8数据库，这是一个优化版本，旨在资源有限的环境中提供有效的分类能力，其特点包括：\n\n数据库大小限制为8 GB，适合计算资源有限的场景。\n针对常见物种优化，保留了常见和临床相关性较高的微生物物种和分类单元。\n分析速度快，内存和存储需求低。\n适用于一般性宏基因组分析，特别是在需要快速获得分析结果或资源有限的情况下。\n\n文档还提供了使用Kraken2进行宏基因组测序数据物种注释分析的步骤，包括安装Kraken2、下载数据库、运行Kraken2进行分类注释，以及分析结果。最后，文档提供了一个使用R语言进行结果可视化的示例。\n\n\n\n\n7.2.6 多轮对话\n上面的单轮对话的例子中语言模型将用户信息列表作为输入，并将模型生成的信息作为输出返回。 有时我们也可以将模型输出的结果继续作为输入的一部分以实现多轮对话。\nKimi API 与 Kimi 智能助手不同，API 本身不具有记忆功能，它是无状态的，这意味着，当你多次请求 API 时，Kimi 大模型并不知道你前一次请求的内容，也不会记忆任何请求的上下文信息。例如，你在前一次请求中告诉 Kimi 大模型你今年 27 岁，在下一次请求中，Kimi 大模型并不会记住你 27 岁这件事。\n因此，在多轮对话中我们需要手动维护每次请求的上下文，即 Context，把上一次请求过的内容手动加入到下一次请求中，让 Kimi 大模型能正确看到此前我们都聊了什么。\n下面是一组简单的实现多轮对话的例子：\n\n# 假设需要追问\ndef chat(query, messages):\n    messages.append({\n        \"role\": \"user\", \n        \"content\": query\n    })\n    completion = client.chat.completions.create(\n        model=\"moonshot-v1-8k\",\n        messages=messages,\n        temperature=0.3,\n    )\n    result = completion.choices[0].message.content\n    messages.append({\n        \"role\": \"assistant\",\n        \"content\": result\n    })\n    return result\n\n使用自定义函数 chat() 继续问新的问题。\nprint(chat(\"Kraken2 有哪些数据库\", messages))\n文档中提到的关键技术是宏基因组测序数据的物种注释分析，主要通过比对reads到参考数据库来识别微生物的分类信息。文档中提到的常用工具包括Kraken2、MetaPhlAn（MetaPhlAn3）和Centrifuge。\nKraken2的数据库包括： 1. 标准数据库（Standard Database）：这是Kraken2的默认数据库，通常包含细菌、病毒、真菌等多种微生物的参考序列，数据库大小通常在几十GB到上百GB不等。 2. Standard-8数据库：这是标准数据库的一个优化版本，旨在在资源有限的环境中提供有效的分类能力。它的数据库大小被限制在8GB以内，保留了常见和临床相关性较高的微生物物种和分类单元，适合快速分析和资源有限的情况。\n文档中还提到了如何安装Kraken2、下载数据库、运行Kraken2进行物种注释，以及如何分析Kraken2的输出结果。此外，还提供了一个使用R语言进行数据可视化的简单示例。\n\n\n\n\n\n\nWarning\n\n\n\n值得注意的是，随着对话的进行，模型每次需要传入的 token 都会线性增加，必要时，需要一些策略进行优化，例如只保留最近几轮对话。",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Moonshot Kimi</span>"
    ]
  },
  {
    "objectID": "moonshot-kimi.html#上传多个文件",
    "href": "moonshot-kimi.html#上传多个文件",
    "title": "7  Moonshot Kimi",
    "section": "7.3 上传多个文件",
    "text": "7.3 上传多个文件\n如果你想一次性上传多个文件，并根据这些文件与 Kimi 对话，你可以参考如下示例：\nfrom typing import *\n \nimport os\nimport json\nfrom pathlib import Path\n \nfrom openai import OpenAI\n \nclient = OpenAI(\n    base_url=\"https://api.moonshot.cn/v1\",\n    # 我们会从环境变量中获取 MOONSHOT_API_KEY 的值作为 API Key，\n    api_key=os.environ[\"MOONSHOT_API_KEY\"],\n)\n \n \ndef upload_files(files: List[str]) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    upload_files 会将传入的文件（路径）全部通过文件上传接口 '/v1/files' 上传，\n    并获取上传后的文件内容生成文件 messages。每个文件会是一个独立的 message，\n    这些 message 的 role 均为 system，Kimi 大模型会正确识别这些 system messages\n    中的文件内容。\n \n    :param files: 一个包含要上传文件的路径的列表，路径可以是绝对路径也可以是\n        相对路径，请使用字符串的形式传递文件路径。\n    :return: 一个包含了文件内容的 messages 列表，请将这些 messages 加入到 \n        Context 中，即请求 `/v1/chat/completions` 接口时的 messages 参数中。\n    \"\"\"\n    messages = []\n \n    # 对每个文件路径，我们都会上传文件并抽取文件内容，最后生成一个 role \n    # 为 system 的 message，并加入到最终返回的 messages 列表中。\n    for file in files:\n        file_object = client.files.create(file=Path(file), \n                                          purpose=\"file-extract\")\n        file_content = client.files.content(file_id=file_object.id).text\n        messages.append({\n            \"role\": \"system\",\n            \"content\": file_content,\n        })\n \n    return messages\n \n \ndef main():\n    # 使用glob方法与通配符**/*来获取目录下的所有文件和子目录中的文件\n    file_messages = upload_files(files=Path(\"example\").glob(\"**/*\"))\n \n    messages = [\n        # 我们使用 * 语法，来解构 file_messages 消息，使其成为 messages 列表的前 N 条 messages。\n        *file_messages,\n        {\n            \"role\": \"system\",\n            \"content\": \"你是 Kimi，由 Moonshot AI 提供的人工智能助手。\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"总结一下这些文件的内容。\",\n        },\n    ]\n \n    completion = client.chat.completions.create(\n        model=\"moonshot-v1-128k\",\n        messages=messages,\n    )\n \n    print(completion.choices[0].message.content)\n \n \nif __name__ == '__main__':\n    main()\n这些文件包含了多个主题的信息，我将分别为您总结每个文件的主要内容：\n\nKraken2.pdf：\n\n介绍了几种常用的宏基因组数据分析工具，包括Kraken2、MetaPhlAn3和Centrifuge，它们通过比对reads到参考数据库来识别微生物的分类信息。\nKraken2使用k-mer比对技术，速度快，适合大规模数据处理，支持自定义数据库。\nMetaPhlAn3使用标记基因进行物种分类，适合分析物种相对丰度，计算效率高。\nCentrifuge基于压缩索引技术，适合处理低质量reads，支持自定义数据库。\n提供了Kraken2的安装、数据库下载、运行分类注释和结果分析的详细步骤。\n介绍了如何使用R或Python进行结果的可视化。\n\nsyncom-group-library.md：\n\n描述了一个名为“土壤生物膜与环境健康书屋”的图书借阅服务，旨在服务于不同阶段的学生和研究人员。\n提供了书屋的开放时间、借阅期限、批注和标记的鼓励以及图书外借的登记要求。\n\nlaboratory-instruments.docx：\n\n列出了华中农业大学资源与环境学院的实验室设备，包括培养设备、分子生物学设备、基础微生物实验设备、计算设施和其他设备。\n详细说明了每种设备的用途、位置和使用注意事项。\n\ngaoch-cv.md：\n\n高春辉的个人简介，他是华中农业大学资源与环境学院的副研究员，硕士生导师。\n他的研究领域包括分子生物学、生物化学、生物信息学和功能基因组学，目前专注于合成微生物群落的研究。\n他主持过多个科研项目，并在多个学术期刊上发表了研究论文。\n\nmachine-learning-course.md：\n\n提供了多个机器学习课程的培训目录，涵盖深度学习在基因组学、单细胞分析、单细胞空间转录组分析、蛋白组学、微生物组学和代谢组学中的应用。\n每个课程包括理论部分和实操内容，涉及算法介绍、数据处理、模型构建和评估等。\n\n\n这些文件涵盖了从宏基因组数据分析工具的使用、图书借阅规则、实验室设备清单、个人简介到机器学习在不同生物信息学领域的应用等多个方面的内容。\n文件接口与 Kimi 智能助手中上传文件功能所使用的相同，支持相同的文件格式，它们包括 .pdf .txt .csv .doc .docx .xls .xlsx .ppt``.pptx .md .jpeg .png .bmp .gif .svg .svgz .webp .ico .xbm .dib .pjp .tif .pjpeg .avif .dot .apng .epub .tiff``.jfif .html .json .mobi .log .go .h .c .cpp .cxx .cc .cs .java .js .css .jsp .php .py .py3 .asp .yaml .yml``.ini .conf .ts .tsx 等格式。\n如果您的文件数量多、体积大、内容长，并且您不想在每次请求都原样携带大体积的文件内容，或是想寻求更加高效且低成本的文件对话方式，请参考使用了 Context Caching 技术的文件上传示例。\n\n上下文缓存 Context Caching （上下文缓存）是一种高效的数据管理技术，它允许系统预先存储那些可能会被频繁请求的大量数据或信息。这样，当您再次请求相同信息时，系统可以直接从缓存中快速提供，而无需重新计算或从原始数据源中检索，从而节省时间和资源。使用 Context Caching 时，首先需要通过 API 创建缓存，指定要存储的数据类型和内容，然后设置一个合适的过期时间以保持数据的有效性。一旦缓存创建完成，任何对该数据的请求都会首先检查缓存，如果缓存有效，则直接使用缓存（此时已缓存的内容将不再收取 Tokens 费用），否则需要重新生成并更新缓存。这种方法特别适用于需要处理大量重复请求的应用程序，可以显著提高响应速度和系统性能。",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Moonshot Kimi</span>"
    ]
  },
  {
    "objectID": "bigmodel.html",
    "href": "bigmodel.html",
    "title": "8  智谱开放平台",
    "section": "",
    "text": "8.1 模型清单\n智谱AI 开放平台提供了包括通用大模型、图像大模型、超拟人大模型、向量大模型等多种模型。",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>智谱开放平台</span>"
    ]
  },
  {
    "objectID": "bigmodel.html#模型清单",
    "href": "bigmodel.html#模型清单",
    "title": "8  智谱开放平台",
    "section": "",
    "text": "8.1.1 语言模型\n\n\n\n\n\n\n\n\n\n模型\n描述\n上下文\n最大输出\n\n\n\n\nGLM-4-Plus\n高智能旗舰: 性能全面提升，长文本和复杂任务能力显著增强\n128K\n4K\n\n\nGLM-4-0520\n高智能模型：适用于处理高度复杂和多样化的任务\n128K\n4K\n\n\nGLM-4-Long\n超长输入：专为处理超长文本和记忆型任务设计\n1M\n4K\n\n\nGLM-4-AirX\n极速推理：具有超快的推理速度和强大的推理效果\n8K\n4K\n\n\nGLM-4-Air\n高性价比：推理能力和价格之间最平衡的模型\n128K\n4K\n\n\nGLM-4-Flash\n免费调用：智谱AI首个免费API，零成本调用大模型。\n128K\n4K\n\n\nGLM-4V\n图像理解：具备图像理解能力和推理能力\n2K\n1k\n\n\nGLM-4-AllTools\nAgent模型：自主规划和执行复杂任务\n128K\n4K\n\n\nGLM-4\n旧版旗舰：发布于2024年1月16日，目前已被GLM-4-0520取代\n128K\n4K\n\n\n\n\n\n8.1.2 多模态模型\n\n\n\n\n\n\n\n\n\n模型\n描述\n最大输入\n输出分辨率\n\n\n\n\nGLM-4V-Plus\n视频和图像理解：具备视频内容和多图片的理解能力\n8K\n-\n\n\nGLM-4V\n图像理解：具备图像理解能力和推理能力\n2K\n-\n\n\nCogVideoX\n视频生成：输入文本或图片即可轻松制作视频\n0.5K\n1440x960\n\n\nCogView-3.5\n图片生成：根据用户文字描述生成高质量图像，支持多图片尺寸\n1k\n1024x1024\n768x1344\n864x1152 等\n\n\nCogView-3\n图片生成：根据用户文字描述快速、精准生成图像\n1k\n1024x1024\n\n\n\n\n\n8.1.3 向量模型\n\n\n\n\n\n\n\n\n\n模型\n描述\n最大输入\n向量维度\n\n\n\n\nEmbedding-3\n最新模型：支持自定义向量维度\n8K\n2048\n\n\nEmbedding-2\n旧版模型：目前已被Embedding-3取代\n8K\n1024\n\n\n\n\n\n8.1.4 其他模型\n\n\n\n\n\n\n\n\n\n模型\n描述\n上下文\n最大输出\n\n\n\n\nChatGLM-3\n拟人模型：适用于情感陪伴和虚拟角色。\n4K\n2K\n\n\nEmohaa\n心理模型：具备专业咨询能力，帮助用户理解情感并应对情绪问题。\n8K\n4k\n\n\nCodeGeeX-4\n代码模型：适用于代码自动补全任务\n128K\n4k\n\n\n\n\n\n\n\n\n\n关键概念\n\n\n\nGLM\nGLM 全名 General Language Model ，是一款基于自回归填空的预训练语言模型。ChatGLM系列模型，支持相对复杂的自然语言指令，并且能够解决困难的推理类问题。该模型配备了易于使用的 API 接口，允许开发者轻松将其融入各类应用，广泛应用于智能客服、虚拟主播、聊天机器人等诸多领域。\nEmbedding\nEmbedding 是一种将数据（如文本）转化为向量形式的表示方法，这种表示方式确保了在某些特定方面相似的数据在向量空间中彼此接近，而与之不相关的数据则相距较远。通过将文本字符串转换为向量，使得数据能够有效用于搜索、聚类、推荐系统、异常检测和分类等应用场景。\nToken\nToken 是模型用来表示自然语言文本的基本单位，可以直观的理解为“字”或“词”；通常 1 个中文词语、1 个英文单词、1 个数字或 1 个符号计为 1 个token。\n一般情况下 ChatGLM 系列模型中 token 和字数的换算比例约为 1:1.8 ，但因为不同模型的分词不同，所以换算比例也存在差异，每一次实际处理 token 数量以模型返回为准，您可以从返回结果的 usage 中查看。",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>智谱开放平台</span>"
    ]
  },
  {
    "objectID": "bigmodel.html#语言模型-1",
    "href": "bigmodel.html#语言模型-1",
    "title": "8  智谱开放平台",
    "section": "8.2 语言模型",
    "text": "8.2 语言模型\nGLM-4 提供了多款模型，适用于各种应用场景。\n模型编码：glm-4-plus、glm-4-0520、glm-4 、glm-4-air、glm-4-airx、glm-4-long、 glm-4-flash。\n\n8.2.1 使用 OpenAI SDK\n使用 OpenAI SDK 创建 Client，示例如下：\n\nfrom openai import OpenAI\nimport os\n\n# format output\nfrom IPython.display import Markdown\nfrom pprint import pprint\n\n\nclient = OpenAI(\n    api_key= os.getenv(\"ZHIPUAI_API_KEY\"),\n    base_url=\"https://open.bigmodel.cn/api/paas/v4/\"\n) \n\n以下代码是 GLM-4 的对话调用示例，请注意：\n\ntemperature：参数的区间为 (0,1)\ndo_sample = False (temperature = 0)：在 OpenAI 调用中并不适用\n\n# 提交对话请求\ncompletion = client.chat.completions.create(\n    model=\"GLM-4-Flash\",\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"你是一个聪明且富有创造力的小说作家\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": (\n                \"请你作为童话故事大王，写一篇短篇童话故事，\"\n                \"故事的主题是要永远保持一颗善良的心，要能够\"\n                \"激发儿童的学习兴趣和想象力，同时也能够帮助\"\n                \"儿童更好地理解和接受故事中所蕴含的道理和价值观。\"\n            )\n        }\n    ],\n    top_p=0.7,\n    temperature=0.9\n)\n\nprint(completion.choices[0].message.content)\n标题：《星星的魔法书》\n从前，在一个遥远的星星王国里，住着一群拥有星星光芒的孩子们。他们每天在夜空中闪烁，为人们带来温暖和希望。在这个王国里，有一颗特别明亮的星星，名叫小星。小星不仅光芒璀璨，还拥有一本神奇的魔法书。\n这本魔法书里记载着无数的智慧和故事，它能够把书中的文字变成真实的世界。小星的魔法书是星星王国的宝藏，而小星是这本书的守护者。\n有一天，小星在翻阅魔法书时，发现了一个新故事。故事讲述了一个名叫小兔的森林小动物，它因为善良和勇敢，得到了星星的祝福，拥有了神奇的力量。\n小星被这个故事深深吸引，它决定将这个故事变成现实，让森林里的孩子们都能感受到善良的力量。于是，小星将魔法书中的文字召唤到现实世界，创造了一个充满奇幻色彩的森林。\n森林里，小兔和它的朋友们住在一座美丽的小木屋里。他们每天在森林里玩耍，帮助其他小动物解决问题。小兔总是用心倾听，用善良的行动温暖着每一个朋友的心。\n有一天，森林里的小河被一块巨大的石头堵住了，水无法流淌，小动物们都非常苦恼。小兔想起了小星的魔法书，它决定去寻找帮助。\n小兔来到了小星的森林，它请求小星帮助解决河流的问题。小星微笑着告诉小兔，真正的帮助来自于内心的善良和勇气。于是，小星给了小兔一本小书，书中记载着如何用善良的力量去改变世界。\n小兔带着小书回到了森林，它用自己的善良感染了每一个动物。大家齐心协力，用树枝和叶子搭建了一个小桥，让河水重新流淌起来。\n森林里的动物们为小兔的善良欢呼，他们开始明白，每个人都有能力用善良的心去改变世界。小兔的故事在森林里传开了，每个孩子都受到了启发。\n星星王国的孩子们也被这个故事感动，他们纷纷效仿小兔，用自己的善良去帮助他人。小星的魔法书成为了他们学习和成长的指南。\n从此，星星王国的孩子们在夜空中闪烁着更加明亮的光芒，他们用善良的心照亮了整个世界。而小星的魔法书，也成为了永恒的传说，激励着每一个孩子永远保持一颗善良的心。\n故事结束，但善良的力量永远流传。愿每一个孩子都能在成长的道路上，拥有这样一颗光芒四射的善良之心。\n\n\n8.2.2 使用智谱AI SDK\n使用智谱AI SDK 可以更方便的创建 client 和使用模型提供的附加功能。\n\nfrom zhipuai import ZhipuAI\n\nclient = ZhipuAI(api_key=os.getenv(\"ZHIPUAI_API_KEY\")) # 请填写您自己的APIKey\n\ntools = [{\n    \"type\": \"web_search\",\n    \"web_search\": {\n        \"enable\": True #默认为关闭状态（False） 禁用：False，启用：True。\n    }\n}]\n\nmessages = [{\n    \"role\": \"user\",\n    \"content\": \"中国 2024 年一季度的GDP是多少 \"\n}]\n\nresponse = client.chat.completions.create(\n    model=\"GLM-4-Flash\",\n    messages=messages,\n    tools=tools\n)\n\nprint(response.choices[0].message)\n\nCompletionMessage(content='中国2024年一季度的国内生产总值（GDP）为296299亿元人民币。这一数据是根据国家统计局的初步核算得出的，按不变价格计算，同比增长5.3%，比上年四季度环比增长1.6%。', role='assistant', tool_calls=None)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>智谱开放平台</span>"
    ]
  },
  {
    "objectID": "bigmodel.html#多模态模型-1",
    "href": "bigmodel.html#多模态模型-1",
    "title": "8  智谱开放平台",
    "section": "8.3 多模态模型",
    "text": "8.3 多模态模型\n\n8.3.1 文生视频\nCogVideoX 是由智谱AI开发的视频生成大模型，具备强大的视频生成能力、只需输入文本或图片就可以轻松完成视频制作。本指南将教您如何系统地构建提示词，从而生成专业级别的视频作品。\n提示词的精确度与细节水平直接影响视频内容的质量。采用结构化提示词可以极大提升视频内容的符合度和专业性。以下是构建提示词的关键组成部分：\n提示词 = (镜头语言 +景别角度+ 光影) + 主体 (主体描述) + 主体运动 +场景 (场景描述) + (氛围)\n\n镜头语言:通过镜头的各种应用以及镜头之间的衔接和切换来传达故事或信息，并创造出特定的视觉效果和情感氛围。如镜头平移，推近、拉远、升降拍摄、摇摄、跟随拍摄、手持拍摄、无人机航拍等;\n景别角度：控制相机与被摄对象之间距离和角度，实现不同的视觉效果和情感表达。如大全景、中景、近景 、鸟瞰视角 、跟随视角、鱼眼效果等;\n光影:光影是赋予摄影作品灵魂的关键元素，光影的运用可以使照片更具深度，更具情感，我们可以通过光影创造出富有层次感和情感表达力的作品。如自然光、丁达尔效应、柔和散射、硬光直射 、逆光剪影、三点布光等;\n主体:主体是视频中的主要表现对象。如儿童、狮子、向日葵，汽车、城堡等;\n主体描述:对主体外貌细节和肢体姿态等的描述，如人物的服饰、动物的毛色、植物的颜色、物体的状态和建筑的风格;\n主体运动:对主体运动状态的描述，包括静止和运动等，运动状态不宜过于复杂，符合6s视频内可以展现的画面即可，\n场景: 场景是主体所处的环境，包括前景、背景等;\n场景描述:对主体所处环境的细节描述。如都市环境、乡村风光、工业区等;\n氛围:对预期视频画面的氛围描述。如喧嚣繁忙、悬疑惊悚、宁静舒适等;\n\n\n\n8.3.2 图生视频\nCogVideoX 可以将用户提供的静态图像转化为 6 秒的动态视频。为达到最佳效果，推荐上传比例为3:2的图片，并且文件格式为 PNG 或 JPEG，文件大小不超过5MB。提示词建议使用“主体（背景）+ 运动描述”的表达方式。\n\n\n8.3.3 文生图片\nCogView-3-Plus使用Transformer架构训练扩散模型，优化了效果并验证了参数量提升的效益。我们还构建了高质量图像微调数据集，使模型生成更符合指令且美学评分更高的图像，效果接近MJ-V6和FLUX等一流模型。\n\n\n8.3.4 图片视频理解\nGLM-4V-Plus 集图像理解与视频理解能力于一体的多模态模型。",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>智谱开放平台</span>"
    ]
  },
  {
    "objectID": "bigmodel.html#增强检索",
    "href": "bigmodel.html#增强检索",
    "title": "8  智谱开放平台",
    "section": "8.4 增强检索",
    "text": "8.4 增强检索\n通过在大语言模型生成答案之前，先从知识库中检索相关知识，然后将相关知识作为背景信息输入给大模型，有效地提升内容的准确性和相关性。\n\n8.4.1 创建知识库\n用于管理文件，支持上传多个文件，并通过关联知识库 ID 后进行调用。\n\nknowledgeDB = client.knowledge.create(\n    embedding_id=3,\n    name=\"default\",\n    description=\"the default knowledge database\"\n)\nprint(knowledgeDB.id)\n\n\n\n1835987442205917184\n\n\n\n\n8.4.2 请求知识库文件列表\n\n# 请求知识库文件列表\nknowledge_files = client.knowledge.document.list(\n    purpose=\"retrieval\",   #支持retrieval\n    knowledge_id=knowledgeDB.id\n)\nprint(knowledge_files)\n\nDocumentPage(list=[DocumentData(id='1835992620149698560', custom_separator=['\\n'], sentence_size=202, length=123750, word_num=1469, name='Kraken2.pdf', url='https://cdn-enterprise-document.bigmodel.cn/knowledge/93037CDE114D4C72A23FC15189EF8FBD.pdf?e=1727637428&token=YsX0TFbWddOwmx008e_eINoC4Oe-PFD_fGIbivRa:WTwlabzg5hvea03zmdUBsqKl5WU=', embedding_stat=1, failInfo=None, knowledge_type=1, parse_image=0), DocumentData(id='1835989396608970752', custom_separator=['\\n'], sentence_size=202, length=123750, word_num=1469, name='Kraken2.pdf', url='https://cdn-enterprise-document.bigmodel.cn/knowledge/CBC4FE6CDC1D41B090CA8670F85CB092.pdf?e=1727637428&token=YsX0TFbWddOwmx008e_eINoC4Oe-PFD_fGIbivRa:6sCGlbkbuPA8uv0u9zws8Rd7618=', embedding_stat=1, failInfo=None, knowledge_type=1, parse_image=0)], object=None, total=2)\n\n\n\n\n8.4.3 上传文件\n支持将doc、docx、pdf、xlsx类型文件上传到知识库，支持自定义文件切片的大小和规则。文件大小不得超过 50 MB，总数为 100 个文件。\n#| output: asis\nresp = client.knowledge.document.create(\n    file=open(\"example/Kraken2.pdf\", \"rb\"),\n    purpose=\"retrieval\",\n    knowledge_id=knowledgeDB.id,\n    sentence_size=202,\n    custom_separator=[\"\\n\"]\n)\n\nprint(resp)\n\n\nDocumentData(id='1835992620149698560', custom_separator=['\\n'], sentence_size=202, length=123750, word_num=1469, name='Kraken2.pdf', url='https://cdn-enterprise-document.bigmodel.cn/knowledge/93037CDE114D4C72A23FC15189EF8FBD.pdf?e=1727637428&token=YsX0TFbWddOwmx008e_eINoC4Oe-PFD_fGIbivRa:WTwlabzg5hvea03zmdUBsqKl5WU=', embedding_stat=1, failInfo=None, knowledge_type=1, parse_image=0)\n\n\n\n\n8.4.4 文件内容提取\n文件内容提取仅适用于以 purpose=\"file-extract\" 为目的上传的文件。\n# 文件内容抽取\nfile_content = client.files.content(file_id=knowledge_files.list[0].id)\nprint(file_content)\n\n\n8.4.5 检索知识库\n\nknowledge_list = client.knowledge.query(\n    page=1,\n    size=10,\n)\npprint(knowledge_list.model_dump())\n\n{'list': [{'background': 'blue',\n           'bucket_id': None,\n           'customer_identifier': '',\n           'description': 'the default knowledge database',\n           'document_size': 2,\n           'embedding_id': 3,\n           'icon': 'question',\n           'id': '1835987442205917184',\n           'length': 247500,\n           'name': 'default',\n           'word_num': 2938}],\n 'object': None,\n 'total': 1}\n\n\n创建知识库后，您将获得一个知识库ID。调用模型服务时，传入知识库ID，使大模型能获取相关内容以响应用户查询。\n\n\n8.4.6 调用知识库\n创建知识库后，您将获得一个知识库ID。调用模型服务时，传入知识库ID，使大模型能获取相关内容以响应用户查询。\n\nquestion = \"Kraken2的安装方法\"\nknowledge = \"默认知识库\"\nresponse = client.chat.completions.create(\n    model=\"GLM-4-Flash\",  # 填写需要调用的模型名称\n    messages=[\n        {\"role\": \"user\", \"content\": question},\n    ],\n    tools=[\n            {\n                \"type\": \"retrieval\",\n                \"retrieval\": {\n                    \"knowledge_id\": knowledgeDB.id,\n                    \"prompt_template\": \"从文档“{{knowledge}}” 中找问题“{{question}}”的答案，找到答案就仅使用文档语句回答问题，找不到答案就用自身知识回答并且告诉用户该信息不是来自文档。\\n不要复述问题，直接开始回答。\"\n                }\n            }\n            ],\n    stream=False,\n)\n\nMarkdown(response.choices[0].message.content)\n\nKraken2是一款广泛使用的基因比对软件，适用于将序列与参考基因组进行比对。以下是安装Kraken2的步骤，以Linux操作系统为例：\n\n8.4.7 1. 安装依赖\nKraken2需要一些依赖库，首先需要安装这些依赖。\n# 安装依赖\nsudo apt-get update\nsudo apt-get install -y automake autoconf libtool libboost-all-dev zlib1g-dev libssl-dev libncurses5-dev libbz2-dev\n\n\n8.4.8 2. 下载Kraken2\n从Kraken2的GitHub页面下载最新版本的源代码。\n# 下载源代码\ngit clone https://github.com/DerrickWood/Kraken2.git\ncd Kraken2\n\n\n8.4.9 3. 编译安装\n在下载的源代码目录中，运行以下命令进行编译和安装。\n# 配置编译\n./configure --prefix=/usr/local/kraaken2\n\n# 编译\nmake\n\n# 安装\nsudo make install\n\n\n8.4.10 4. 检查安装\n确保Kraken2已经正确安装，可以通过以下命令检查。\n# 检查Kraken2版本\nkraken2 --version\n\n\n8.4.11 5. 使用Kraken2\n安装完成后，您可以使用Kraken2进行基因比对。以下是一个简单的示例：\n# 使用Kraken2进行比对\nkraken2 -t 4 -o output kraken_db/kraaken_db.kdb reads.fna\n这里，-t 4表示使用4个线程，-o output指定输出文件的目录，kraken_db/kraaken_db.kdb是Kraken2的参考数据库，reads.fna是您要比对的序列文件。\n\n\n8.4.12 注意事项\n\n根据您的系统环境，可能需要调整依赖安装命令。\n如果您需要将Kraken2安装到其他路径，可以在configure命令中指定--prefix。\n\n希望这些步骤能够帮助您成功安装和使用Kraken2！\n\n\n\n\n\n8.4.13 删除文件\nresult = client.files.delete(\n    file_id=\"文件id\"      #支持retrieval、batch、fine-tune、file-extract文件\n)\n\n\n8.4.14 删除知识库\n使用下面的语句删除知识库。\nresult = client.knowledge.delete(\n    knowledge_id=\"xxxxxxxxxxxx\"\n)\n\n\n8.4.15 用量查询\nresult = client.knowledge.used()\npprint(result.model_dump_json(), width = 72)\n‘{“used”:{“word_num”:2938,“length”:247500},“total”:{“word_num”:5000000,“length”:1073741824}}’",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>智谱开放平台</span>"
    ]
  },
  {
    "objectID": "bigmodel.html#基于文件的问答",
    "href": "bigmodel.html#基于文件的问答",
    "title": "8  智谱开放平台",
    "section": "8.5 基于文件的问答",
    "text": "8.5 基于文件的问答\nGLM-4-Long 最长支持200万字符上下文，结合文件问答功能。\n\nfrom pathlib import Path\nimport json\n\n# 上传并提取文本内容\nfile_01 = client.files.create(file=Path(\"example/Kraken2.pdf\"), purpose=\"file-extract\")\ncontent_01 = json.loads(client.files.content(file_01.id).content)[\"content\"]\n\n# 上传并提取文本内容\nfile_02 = client.files.create(file=Path(\"example/gaoch-cv.md\"), purpose=\"file-extract\")\ncontent_02 = json.loads(client.files.content(file_02.id).content)[\"content\"]\n\n# 生成请求消息，将不同文件的内容放入消息中\nmessage_content = (\n    \"请对以下论文进行分析，并且生成一份论文综述：\\n\\n\"\n    \"第一篇论文内容如下：\\n\"    \n    f\"{content_01}\\n\\n\"\n    \"第二篇论文内容如下：\\n\"\n    f\"{content_02}\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"glm-4-long\",  \n    messages=[\n        {\"role\": \"user\", \"content\": message_content}\n    ],\n)\n\nprint(response.choices[0].message)\n\n\n8.5.1 删除文件\n请求文件列表，然后利用 file_id 删除文件。\n# 请求文件列表\nresult = client.files.list(\n    purpose=\"file-extract\",    #支持batch、file-extract、fine-tune\n)\npprint(result)\nresult = client.files.delete(\n    file_id=\"文件id\"      #支持retrieval、batch、fine-tune、file-extract文件\n)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>智谱开放平台</span>"
    ]
  },
  {
    "objectID": "grounding-dino.html",
    "href": "grounding-dino.html",
    "title": "9  DDS Could",
    "section": "",
    "text": "9.1 Grounding DINO\nGrounding DINO 能够根据用户提供的类别名（文本），智能地识别图像中与用户提供的类别名相关的物体的位置和置信度，让各种场景的检测变得更加简单，包括了常见物体目标检测，长尾物体目标检测，密集场景检测，长短文本物体定位等。\nimport json\nimport time\nimport os\n\nimport requests\n\n# 1. 封装 Header，带上 Token\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Token\"       : os.getenv(\"DINO_API_KEY\")\n}",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>DDS Could</span>"
    ]
  },
  {
    "objectID": "grounding-dino.html#grounding-dino",
    "href": "grounding-dino.html#grounding-dino",
    "title": "9  DDS Could",
    "section": "",
    "text": "Note\n\n\n\nGrounding DINO 是一种先进的视觉语言模型，专门用于开放领域目标检测任务。这意味着它不仅可以检测和识别特定类别的目标，还能够根据自然语言提示，检测图像中任意对象。这种模型结合了目标检测（如 DINO）和自然语言处理（grounding）的能力，因此具备了强大的灵活性和广泛的适应性。\nGrounding DINO 的核心特性：\n\n开放领域目标检测：模型能够识别并检测图像中任意类别的对象，而无需事先定义特定的标签集。用户可以通过输入任意的文本提示，模型会依据提示在图像中找到相应的对象。例如，输入 “find a person with a red shirt” 这样的指令，模型可以在图像中定位并标出红色衬衫的人。\n结合视觉和文本特征：Grounding DINO 能够将文本的语义信息与视觉特征相结合，在理解自然语言的同时，还能够高效处理图像中的目标检测任务。它采用了 Transformer 架构（如 DINO）进行图像的目标检测，同时与语言模型紧密结合，实现对图像和文本的联合理解。\n应用场景广泛：Grounding DINO 可以应用在许多需要图像与语言交互的场景中，比如图像标注、自动驾驶、机器人导航、增强现实（AR）等领域。它的开放领域检测特性使得它非常适合在没有明确类别标签的情况下执行复杂的目标检测任务。\n\n相关工作：\n\nDINO（DEtection Transformer with Improved Optimization）：是一种基于 Transformer 的目标检测模型。Grounding DINO 继承了 DINO 模型在目标检测中的强大性能，同时扩展了其与自然语言提示交互的能力。\n视觉语言模型：Grounding DINO 属于视觉语言模型家族的一员，这些模型致力于处理视觉和文本的交互问题。例如，CLIP 等模型能够从文本描述中找到与之对应的图像，而 Grounding DINO 更侧重在图像中找到指定的对象。\n\nGrounding DINO 的工作流程：\n\n输入：一张图像和一个文本提示。\n处理：模型通过视觉编码器对图像进行处理，并通过语言编码器对文本进行编码，然后结合这两部分信息。\n输出：模型根据文本提示返回图像中检测到的目标位置（通常是边界框）。\n\nGrounding DINO 在开放场景目标检测中展现出了强大的表现，并在很多研究和应用中取得了显著成果。\n\n\n\n\n9.1.1 参数\n\n\n\n\n\n\n\n\n\n\n参数\n说明\n类型\n默认值\n是否必填\n\n\n\n\nimage\n目标图片， base64 或者 http(s) 链接图片 url\nString\n无\n是\n\n\nprompts\n提示类别，多个提示类别，数组设置多项\nArray\n无\n是\n\n\nprompts[].type\nprompt 类别，目前支持文本，固定值为 “text”\nString\n无\n是\n\n\nprompts[].text\nprompt 提示文本\nString\n无\n是\n\n\nmodel\ndino 模型，值可为：GroundingDino-1, GroundingDino-1.5-Edge, GroundingDino-1.5-Pro\nEnum\n无\n是\n\n\ntargets\n检测目标类型， 最多支持同时 “bbox”和 “mask”，bbox 为检测边界框 mask 为分割掩码，如[“bbox”]\nArray\n无\n否\n\n\nbbox_threshold\n边框置信度阈值, 范围是 0-1\nNumber\n0.25\n否\n\n\niou_threshold\n交并比阈值，IoU 阈值，范围是 0-1\nNumber\n0.8\n否\n\n\n\n\n\n9.1.2 提交任务\n设置 Body。\n\nbody = {\n    \"image\": \"https://playground/grounding_DINO-1.5/02.jpg\",\n    \"prompts\": [\n        {\"type\": \"text\", \"text\": \"window covering\"},\n        {\"type\": \"text\", \"text\": \"dinner table\"}\n    ],\n    \"model\": 'GroundingDino-1.5-Pro',\n    \"targets\": [\"bbox\"]\n}\n\n如果使用本地图片，需要进行 base64 编码转换。\n\nimport base64\n\ndef image_base64(image_path):\n    # 读取图片并将其转换为 base64\n    with open(image_path, \"rb\") as image_file:\n        # 读取图片的二进制数据\n        image_data = image_file.read()\n        \n        # 将图片数据转换为 base64 编码\n        encoded_image = base64.b64encode(image_data)\n        \n        # 将 base64 数据转换为字符串形式\n        encoded_image_str = encoded_image.decode('utf-8')\n        \n        return encoded_image_str\n\n# 打印或使用 base64 编码字符串\nimage_path = \"example/images/leaf-plate.jpg\"\nprint(image_base64(image_path)[:70])\n\n/9j/4Q/+RXhpZgAATU0AKgAAAAgABgESAAMAAAABAAEAAAEaAAUAAAABAAAAVgEbAAUAAA\n\n\n将图片的 base64 编码与 mime type 组成一个字符串，可以传递给 body 参数。\n\nimport mimetypes\n\ndef image2base64_with_mime(image_path):\n    # 获取文件的 MIME 类型\n    mime_type, _ = mimetypes.guess_type(image_path)\n    \n    # 读取图片并将其转换为 base64\n    encoded_image_str = image_base64(image_path)\n\n    # 如果未能识别出 MIME 类型，抛出异常\n    if mime_type is None:\n        raise ValueError(\"无法识别文件的 MIME 类型\")\n    \n    # 构建带有前缀的 base64 数据\n    data_uri = f\"data:{mime_type};base64,{encoded_image_str}\"\n    \n    return data_uri\n\nimage = image2base64_with_mime(image_path)\nprint(image[:70])\n\ndata:image/jpeg;base64,/9j/4Q/+RXhpZgAATU0AKgAAAAgABgESAAMAAAABAAEAAAE\n\n\n提示文本也是必需参数。\n\nprompts = [\n        {\"type\": \"text\", \"text\": \"leaf\"},\n        {\"type\": \"text\", \"text\": \"root\"},\n        {\"type\": \"text\", \"text\": \"plate\"},\n        {\"type\": \"text\", \"text\": \"mark\"},\n        {\"type\": \"text\", \"text\": \"seedling\"},\n        {\"type\": \"text\", \"text\": \"green leaf\"},\n        {\"type\": \"text\", \"text\": \"withered leaf\"},\n    ]\n\n# 完整的 body 信息\nbody = {\n    \"image\": image,\n    \"prompts\": prompts,\n    \"model\": 'GroundingDino-1.5-Pro',\n    \"targets\": [\"bbox\"]\n}\n\n提交并获取结果。\nmax_retries = 60  # max retry times\nretry_count = 0\n\n# send request\nresp = requests.post(\n    'https://api.deepdataspace.com/tasks/detection',\n    json=body,\n    headers=headers\n)\n\nif resp.status_code == 200:\n    json_resp = resp.json()\n    print(json_resp)\n    # {'code': 0, 'data': {'task_uuid': '092ccde4-a51a-45'}, 'msg': 'ok'}\n\n    # get task_uuid\n    task_uuid = json_resp[\"data\"][\"task_uuid\"]\n    print(f'task_uuid:{task_uuid}')\n\n    # poll get task result\n    while retry_count &lt; max_retries:\n        url = f'https://api.deepdataspace.com/task_statuses/{task_uuid}'\n        resp = requests.get(url, headers=headers)\n        if resp.status_code != 200:\n            break\n        json_resp = resp.json()\n        if json_resp[\"data\"][\"status\"] not in [\"waiting\", \"running\"]:\n            break\n        time.sleep(1)\n        retry_count += 1\n\n    if json_resp[\"data\"][\"status\"] == \"failed\":\n        print(f'failed resp: {json_resp}')\n    elif json_resp[\"data\"][\"status\"] == \"success\":\n        print(f'success resp: {json_resp}')\n    else:\n        print(f'get task resp: {resp.status_code} - {resp.text}')\nelse:\n    print(f'Error: {resp.status_code} - {resp.text}')\n\n\n{'code': 0,\n 'data': {'task_uuid': '4cc0ca70-9e00-4698-9386-2776cbd9bc0d'},\n 'msg': 'ok'}\n'task_uuid:4cc0ca70-9e00-4698-9386-2776cbd9bc0d'\n{'code': 0,\n 'data': {'error': None,\n          'result': {'mask_url': None,\n                     'objects': [{'bbox': [-0.19334310293197632,\n                                           1.469710350036621,\n                                           1773.765869140625,\n                                           1747.221435546875],\n                                  'category': 'plate',\n                                  'score': 0.6576007008552551},\n                                 {'bbox': [143.88165283203125,\n                                           73.70735931396484,\n                                           432.5969543457031,\n                                           248.65553283691406],\n                                  'category': 'mark',\n                                  'score': 0.3864704370498657},\n                                 {'bbox': [1368.73193359375,\n                                           1434.9140625,\n                                           1633.30908203125,\n                                           1605.239013671875],\n                                  'category': 'mark',\n                                  'score': 0.3484154939651489},\n                                 {'bbox': [1112.5635986328125,\n                                           180.6923370361328,\n                                           1361.9908447265625,\n                                           303.2415466308594],\n                                  'category': 'green leaf',\n                                  'score': 0.3173876404762268},\n                                 {'bbox': [1439.4031982421875,\n                                           387.93609619140625,\n                                           1567.3350830078125,\n                                           563.5098266601562],\n                                  'category': 'green leaf',\n                                  'score': 0.28551268577575684},\n                                 {'bbox': [1212.9088134765625,\n                                           558.487548828125,\n                                           1326.8262939453125,\n                                           646.2600708007812],\n                                  'category': 'green leaf',\n                                  'score': 0.2655079662799835},\n                                 {'bbox': [1361.4508056640625,\n                                           172.87673950195312,\n                                           1473.9033203125,\n                                           267.9229431152344],\n                                  'category': 'green leaf',\n                                  'score': 0.256084144115448}]},\n          'session_id': '50d8c56e8548497aaf2e91a310f128de',\n          'status': 'success',\n          'uuid': '9248e4dd-8050-4edf-8e95-f1c03a690648'},\n 'msg': 'ok'}\n\n\n\n\n9.1.3 处理结果\n将检测的结果标在图片上面。\n\n# 计算识别到的类型数目\ndef category_num(data):\n    # 初始化一个字典用于存储每个 category 的计数\n    category_count = {}\n    \n    # 获取 JSON 数据中的 objects 列表\n    objects = data['data']['result']['objects']\n    \n    # 遍历每个 object\n    for obj in objects:\n        category = obj.get('category')\n        \n        # 如果 category 已存在于字典中，则计数加1，否则初始化为1\n        if category in category_count:\n            category_count[category] += 1\n        else:\n            category_count[category] = 1\n    \n    return category_count\n\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.patches as patches\n\n\ndef plot_with_result(image_path, json_resp):\n    # generate random color\n    category_count = category_num(json_resp)\n    colors = {category: np.random.rand(3,) for category in category_count}\n    \n    # Create a figure and axis for plotting\n    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n    \n    # Display the original image\n    image = Image.open(image_path)\n    ax.imshow(image)\n    \n    # 获取 JSON 数据中的 objects 列表\n    detection_results = json_resp['data']['result']['objects']\n    \n    # Overlay bounding boxes and labels with random colors\n    for result in detection_results:\n        score = result.get('score') or 0\n        label = result.get('category')\n        bbox = result['bbox']\n        \n        # Generate a random color\n        color = colors[label]\n        \n        # 创建一个矩形框，参数为 (左上角 x, 左上角 y, 宽度, 高度)\n        rect = patches.Rectangle(\n            (bbox[0], bbox[1]),  # 左上角坐标\n            bbox[2] - bbox[0],   # 宽度\n            bbox[3] - bbox[1],   # 高度\n            linewidth=2,         # 边框宽度\n            edgecolor=color,     # 边框颜色\n            facecolor='none'     # 填充颜色\n        )\n        ax.add_patch(rect)\n        \n        # Draw label and score with the same color as the rectangle\n        label_text = f\"{label}: {score:.2f}\"\n        ax.text(\n            bbox[0],\n            bbox[1] - 10,\n            label_text,\n            color=color,\n            fontsize=12,\n            bbox=dict(facecolor='white', alpha=0.5, \n                      edgecolor=color, boxstyle='round,pad=0.5')\n        )\n    \n    \n    # Hide axis\n    # plt.axis('off')\n    \n    # Show the plot with bounding boxes and labels\n    plt.show()\n\nplot_with_result(image_path, json_resp)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>DDS Could</span>"
    ]
  },
  {
    "objectID": "grounding-dino.html#grounded-sam",
    "href": "grounding-dino.html#grounded-sam",
    "title": "9  DDS Could",
    "section": "9.2 Grounded SAM",
    "text": "9.2 Grounded SAM\nGrounded SAM 算法集成了 Grounding DINO 1.0 和 SAM 算法 2 种功能，利用文本提示有效地检测边界框和分割掩码。\n\n9.2.1 参数\n\n\n\n\n\n\n\n\n\n\n参数\n说明\n类型\n默认值\n是否必填\n\n\n\n\nimage\n目标图片， base64 或者 http(s) 链接图片 url\nString\n无\n是\n\n\nprompts\n提示类别，多个提示类别，数组设置多项\nArray\n无\n是\n\n\nprompts[].type\nprompt 类别，固定值为 “text”\nString\n无\n是\n\n\nprompts[].text\n提示文本\nString\n无\n是\n\n\nmodel_type\n模型，值可为：“swinb” ,‘swint’\nEnum\n无\n是\n\n\nbox_threshold\n边框置信度阈值, 范围是 0-1\nNumber\n0.3\n否\n\n\nnms_threshold\n交并比阈值，IoU 阈值，范围是 0-1\nNumber\n0.8\n否\n\n\n\n\n\n9.2.2 提交任务\n# 使用同样的 image 和 prompt\nbody = {\n    \"image\": image,\n    \"prompts\": prompts,\n    \"model_type\": \"swinb\"\n}\n\n# 获取结果\n# send request\nresp1 = requests.post(\n    url='https://api.deepdataspace.com/tasks/grounded_sam',\n    json=body,\n    headers=headers\n)\n运行图像分割任务。\n# get task_uuid\nretry_count = 0\nif resp.status_code == 200:\n    json_resp = resp.json()\n    # task_uuid = json_resp[\"data\"][\"task_uuid\"]\n    task_uuid = \"50b6ac9f-aa4f-44b8-897f-6f2f41accdf6\"\n    print(f'task_uuid:{task_uuid}')\n    \n    # poll get task result\n    while retry_count &lt; max_retries:\n        resp = requests.get(f'https://api.deepdataspace.com/task_statuses/{task_uuid}', headers=headers)\n        if resp.status_code != 200:\n            break\n        json_resp = resp.json()\n        if json_resp[\"data\"][\"status\"] not in [\"waiting\", \"running\"]:\n            break\n        time.sleep(1)\n        retry_count += 1\n\n    if json_resp[\"data\"][\"status\"] == \"failed\":\n        print(f'failed resp: {json_resp}')\n    elif json_resp[\"data\"][\"status\"] == \"success\":\n        print(f'success resp: {json_resp}')\n    else:\n        print(f'get task resp: {resp.status_code} - {resp.text}')\nelse:\n    print(f'Error: {resp.status_code} - {resp.text}')\n\n\n{}\n\n\n\n\n9.2.3 处理结果\n\n# plot_with_result(image_path, json_resp)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>DDS Could</span>"
    ]
  },
  {
    "objectID": "grounding-dino.html#interactive-visual-prompting",
    "href": "grounding-dino.html#interactive-visual-prompting",
    "title": "9  DDS Could",
    "section": "9.3 Interactive Visual Prompting",
    "text": "9.3 Interactive Visual Prompting\n交互式视觉提示（IVP）是一种基于 T-Rex 模型的交互式目标检测和计数系统。它通过视觉提示实现目标检测和计数，无需任何训练，真正实现了适用于多种场景的单一视觉模型。它在密集或重叠场景中计数物体方面尤其出色。\n\n9.3.1 提交任务\nbody = {\n    \"prompt_image\": \"https://playground/ivp/v2/left_17.jpeg\",\n    \"infer_image\": \"https://playground/ivp/v2/right_17.jpeg\",\n    \"prompts\": [\n        {\n            \"type\": \"rect\",\n            \"is_positive\": 1,\n            \"rect\": [\n                421.4149253731343,\n                381.7002098880597,\n                561.88656716417,\n                506.9031949626866\n            ],\n        }\n    ],\n    \"label_types\": [\"bbox\"]\n}\n\nmax_retries = 60  # 最大重试次数\nretry_count = 0\n\n# send request\nresp1 = requests.post(\n    'https://api.deepdataspace.com/tasks/ivp',\n    json=body,\n    headers=headers\n)\n\n\n9.3.2 处理结果\nif resp1.status_code == 200:\n    json_resp1 = resp1.json()\n    print(json_resp1)\n    # {'code': 0, 'data': {'task_uuid': '092ccde4-a51a-489b-b384-9c4ba8af7375'}, 'msg': 'ok'}\n\n    # get task_uuid\n    task_uuid = json_resp1[\"data\"][\"task_uuid\"]\n    print(f'task_uuid:{task_uuid}')\n\n    # poll get task result\n    while retry_count &lt; max_retries:\n        resp = requests.get(f'https://api.deepdataspace.com/task_statuses/{task_uuid}', headers=headers)\n        if resp.status_code != 200:\n            break\n        json_resp = resp.json()\n        if json_resp[\"data\"][\"status\"] not in [\"waiting\", \"running\"]:\n            break\n        time.sleep(1)\n        retry_count += 1\n\n    if json_resp[\"data\"][\"status\"] == \"failed\":\n        print(f'failed resp: {json_resp}')\n    elif json_resp[\"data\"][\"status\"] == \"success\":\n        print(f'success resp: {json_resp}')\n    else:\n        print(f'get task resp: {resp.status_code} - {resp.text}')\nelse:\n    print(f'Error: {resp1.status_code} - {resp1.text}')\n\n\n{'code': 0, 'msg': 'ok', 'data': {'task_uuid': 'ebff4e6f-35db-4c7f-9f46-72034bd462af'}}\n{'code': 0,\n 'data': {'error': None,\n          'result': {'objects': [{'bbox': [386, 266, 428, 315],\n                                  'score': 0.31}]},\n          'session_id': 'd8399018353c469cb3aa7f0b7c1e68d3',\n          'status': 'success',\n          'uuid': 'ebff4e6f-35db-4c7f-9f46-72034bd462af'},\n 'msg': 'ok'}\n\n\n\nprompt_image = \"example/images/left_17.jpeg\"\ninfer_image = \"example/images/right_17.jpeg\"\n\nprompt_data = {\"data\": {\"result\": {\"objects\": [{\"category\": \"prompt\",\n                                                \"score\": 1,\n                                                \"bbox\": [421.4149253731343,\n                                                          381.7002098880597,\n                                                          561.88656716417,\n                                                          506.9031949626866]},\n                                               ]}}}\n\nplot_with_result(prompt_image, prompt_data)\nplot_with_result(infer_image, resp_json)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>DDS Could</span>"
    ]
  },
  {
    "objectID": "grounding-dino.html#使用-sdk-调用",
    "href": "grounding-dino.html#使用-sdk-调用",
    "title": "9  DDS Could",
    "section": "9.4 使用 SDK 调用",
    "text": "9.4 使用 SDK 调用\n配置 client。\n\n# 1. Initialize the client with your API token.\nimport os\nfrom dds_cloudapi_sdk import Config\nfrom dds_cloudapi_sdk import Client\n\ntoken = os.getenv(\"DINO_API_KEY\")\nconfig = Config(token)\nclient = Client(config)\n\n运行任务的一般流程如下。\n\n# 2. Upload local image to the server and get the URL.\ninfer_image_url = \"https://dev.deepdataspace.com/static/04_a.ae28c1d6.jpg\"\n# infer_image_url = client.upload_file(\"path/to/infer/image.jpg\")  \n# you can also upload local file for processing\nprompt_image_url = infer_image_url  # use the same image for prompt\n\n# 3. Create a task with proper parameters.\nfrom dds_cloudapi_sdk.tasks import IVPTask\nfrom dds_cloudapi_sdk.tasks import RectPrompt\nfrom dds_cloudapi_sdk.tasks import LabelTypes\n\ntask = IVPTask(\n    prompt_image_url=prompt_image_url,\n    prompts=[RectPrompt(rect=[475.18, 550.20, 548.10, 599.92], is_positive=True)],\n    infer_image_url=infer_image_url,\n    infer_label_types=[LabelTypes.BBox, LabelTypes.Mask],  # infer both bbox and mask\n)\n\n# 4. Run the task and get the result.\nclient.run_task(task)\n\n# 5. Parse the result.\nfrom dds_cloudapi_sdk.tasks.ivp import TaskResult\n\nresult: TaskResult = task.result\n\nmask_url = result.mask_url  # the image url with all masks drawn on\nobjects = result.objects  # the list of detected objects\nfor idx, obj in enumerate(objects):\n    # get the detection score\n    print(obj.score)  # 0.42\n\n    # get the detection bbox\n    print(obj.bbox)  # [635.0, 458.0, 704.0, 508.0]\n\n    # get the detection mask, it's of RLE format\n    print(obj.mask.counts)  # ]o`f08fa14M3L2O2M2O1O1O1O1N2O1N2O1N2N3M2O3L3M3N2M2N3N1N2O...\n\n    # convert the RLE format to RGBA image\n    mask_image = task.rle2rgba(obj.mask)\n    print(mask_image.size)  # (1600, 1170)\n\n    # save the image to file\n    mask_image.save(f\"data/mask_{idx}.png\")\n\n\n9.4.1 检测任务\n\nfrom dds_cloudapi_sdk import Config\nfrom dds_cloudapi_sdk import Client\nfrom dds_cloudapi_sdk import DetectionTask\nfrom dds_cloudapi_sdk import TextPrompt\nfrom dds_cloudapi_sdk import DetectionModel\nfrom dds_cloudapi_sdk import DetectionTarget\nimport os\n\ntoken = os.getenv(\"DINO_API_KEY\")\nconfig = Config(token)\nclient = Client(config)\n\n\n# Step 3: run the task by DetectionTask class\n# image_url = \"https://algosplt.oss-cn-shenzhen.aliyuncs.com/test_files/tasks/detection/iron_man.jpg\"\n# if you are processing local image file, upload them to DDS server to get the image url\nimage_url = client.upload_file(\"example/images/leaf-plate.jpg\")\n\ntask = DetectionTask(\n    image_url=image_url,\n    prompts=[TextPrompt(text=\"plate\"), \n             TextPrompt(text=\"leaf\"), \n             TextPrompt(text='primary root'),\n             TextPrompt(text=\"mark\"),\n             TextPrompt(text=\"seedlings\")],\n    targets=[DetectionTarget.Mask, DetectionTarget.BBox],  # detect both bbox and mask\n    model=DetectionModel.GDino1_5_Pro,  # detect with GroundingDino-1.5-Pro model\n)\n\n运行任务。\n\nclient.run_task(task)\nresult = task.result\n\n\n# 使用缓存的结果\nimport os\nimport pickle\nimport requests\n\n# 保存结果\nfile_path = \"example/results/detection-result.pkl\"\n# 检查文件是否存在\nif os.path.isfile(file_path):\n    # 文件存在，读取文件\n    with open(file_path, 'rb') as file:\n        result = pickle.load(file)\nelse:\n    client.run_task(task)\n    result = task.result\n    # 文件不存在，创建一个新的result对象并保存\n    with open(file_path, 'wb') as file:\n        pickle.dump(result, file)\n\n# 下载图片并保存\nresponse = requests.get(result.mask_url)\n\n# 检查请求是否成功\nsave_path = \"example/results/detection-result-mask.png\"\nif not os.path.isfile(save_path) and response.status_code == 200:\n    # 将图片保存到本地文件\n    with open(save_path, 'wb') as file:\n        file.write(response.content)\n    print(f\"图片成功保存到 {save_path}\")\n\n这个含有 mask 的图像，标记了检测到的物体的范围。\n\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image, ImageDraw\nimport matplotlib.patches as patches\n\nimage = Image.open(\"example/images/leaf-plate.jpg\")\nobjects = result.objects  # the list of detected objects\n# 创建一个图像副本用于绘制\nfig, ax = plt.subplots(1, 1, figsize=(12, 8))\n\nax.imshow(image)\n\nfor idx, obj in enumerate(objects):\n    # 将RLE格式转换为RGBA图像\n    mask_image = task.rle2rgba(obj.mask)\n    \n    ax.imshow(mask_image, alpha=0.5)\n    \n    # 在图像上绘制掩码\n    bbox = obj.bbox\n    # 创建一个矩形框，参数为 (左上角 x, 左上角 y, 宽度, 高度)\n    rect = patches.Rectangle(\n        (bbox[0], bbox[1]),  # 左上角坐标\n        bbox[2] - bbox[0],   # 宽度\n        bbox[3] - bbox[1],   # 高度\n        linewidth=2,         # 边框宽度\n        edgecolor='red',     # 边框颜色\n        facecolor='none'     # 填充颜色\n    )\n    ax.add_patch(rect)   \n    \n    # 在图像上添加类别和分数\n    text = f\"{obj.category} {obj.score:.2f}\"\n    ax.text(bbox[0], \n              bbox[1] - 10, \n              text, \n              color='red', \n              fontsize=12)\n\n# 显示图像\n# plt.axis('off')  # 不显示坐标轴\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>DDS Could</span>"
    ]
  },
  {
    "objectID": "langchain-chatchat.html",
    "href": "langchain-chatchat.html",
    "title": "10  本地知识库",
    "section": "",
    "text": "10.1 安装部署",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>本地知识库</span>"
    ]
  },
  {
    "objectID": "langchain-chatchat.html#安装部署",
    "href": "langchain-chatchat.html#安装部署",
    "title": "10  本地知识库",
    "section": "",
    "text": "10.1.1 软硬件要求\n💡 软件方面，本项目已支持在 Python 3.8-3.11 环境中进行使用，并已在 Windows、macOS、Linux 操作系统中进行测试。\n💻 硬件方面，因 0.3.0 版本已修改为支持不同模型部署框架接入，因此可在 CPU、GPU、NPU、MPS 等不同硬件条件下使用。\n\n\n10.1.2 安装步骤\n\n10.1.2.1 安装 Langchain-Chatchat\n从 0.3.0 版本起，Langchain-Chatchat 提供以 Python 库形式的安装方式，具体安装请执行：\npip install langchain-chatchat -U\n\n\n10.1.2.2 模型推理框架并加载模型\n如果使用在线服务的话，这一步不需要做。\n从 0.3.0 版本起，Langchain-Chatchat 不再根据用户输入的本地模型路径直接进行模型加载，涉及到的模型种类包括 LLM、Embedding、Reranker 及后续会提供支持的多模态模型等，均改为支持市面常见的各大模型推理框架接入，如 Xinference、Ollama、LocalAI、FastChat、One API 等。\n因此，请确认在启动 Langchain-Chatchat 项目前，首先进行模型推理框架的运行，并加载所需使用的模型。\n\n\n10.1.2.3 安装知识库工具\n\n安装搜索模块\n安装 duckduckgo 搜索模块。\npip install -U duckduckgo-search\n安装文档处理模块\nnltk 是 Python 中的自然语言处理 (NLP) 库，全称为 Natural Language Toolkit。它提供了大量的工具和资源来帮助处理和分析文本数据，涵盖了分词、词性标注、句法分析等任务。\n\npunkt 是 NLTK 的一个分词器，它用于将文本分成句子和单词。punkt_tab 应该是用来处理或扩展 punkt 的一些预定义标注数据（可能是分句表或者分词数据表）。\n\n分词（Tokenization）是 NLP 的基本任务之一，它将连续的文本字符串切分成较小的部分，如单词或句子。\n\naveraged_perceptron_tagger 是一种用于词性标注的算法，它会根据上下文为句子中的每个单词分配一个词性标签（如名词、动词、形容词等）。此模型基于英语的词性标注，因此在文件名中有 eng。\n\n词性标注（POS tagging）是文本分析的重要任务，可以帮助 NLP 系统理解单词的语法作用。\n\n\nimport nltk\n\n# 下载分词器所需的预定义标注表\nnltk.download('punkt_tab')\n\n# 下载词性标注器的预训练模型，支持对英文文本进行词性标注\nnltk.download('averaged_perceptron_tagger_eng')\n\n\n\n\n10.1.3 初始化\n从 0.3.1 版本起，Langchain-Chatchat 使用本地 yaml 文件的方式进行配置，用户可以直接查看并修改其中的内容，服务器会自动更新无需重启。\n\n10.1.3.1 初始化工作目录\n设置 Chatchat 存储配置文件和数据文件的根目录（可选）\n# on linux or macos\nexport CHATCHAT_ROOT=/path/to/chatchat_data\n\n# on windows\nset CHATCHAT_ROOT=/path/to/chatchat_data\n\n\n10.1.3.2 执行初始化\nchatchat init\n该命令会执行以下操作：\n\n创建所有需要的数据目录\n复制 samples 知识库内容\n生成默认 yaml 配置文件\n\n\n\n\n10.1.4 配置模型和工具\n修改配置文件\n\n10.1.4.1 配置模型（model_settings.yaml）\n具体参考 model_settings.yaml 中的注释。主要修改以下内容：\n# 默认选用的 LLM 名称\nDEFAULT_LLM_MODEL: \"gpt-4o-mini\"\n\n# 默认选用的 Embedding 名称\nDEFAULT_EMBEDDING_MODEL: \"text-embedding-ada-002\"\n\n# # 平台名称\nplatform_name: chatanywhere\n\nMODEL_PLATFORMS:\n  - platform_name: chatanywhere\n    platform_type: openai\n    api_base_url: https://api.chatanywhere.tech/v1\n    api_key: sk-J3LQR******************\n    api_proxy: ''\n    api_concurrencies: 5\n    auto_detect_model: false\n    llm_models:\n      - gpt-4o-mini\n      - gpt-4o-ca\n      - gpt-4-ca\n    embed_models:\n      - text-embedding-3-small\n      - text-embedding-3-large\n      - text-embedding-ada-002\n    text2image_models: []\n    image2text_models: []\n    rerank_models: []\n    speech2text_models: []\n    text2speech_models: []\n\n\n10.1.4.2 配置知识库路径（basic_settings.yaml）（可选）\n默认知识库位于 CHATCHAT_ROOT/data/knowledge_base，如果你想把知识库放在不同的位置，或者想连接现有的知识库，可以在这里修改对应目录即可。\n# 知识库默认存储路径\n KB_ROOT_PATH: D:\\chatchat-test\\data\\knowledge_base\n\n # 数据库默认存储路径。如果使用sqlite，可以直接修改DB_ROOT_PATH；如果使用其它数据库，请直接修改SQLALCHEMY_DATABASE_URI。\n DB_ROOT_PATH: D:\\chatchat-test\\data\\knowledge_base\\info.db\n\n # 知识库信息数据库连接URI\n SQLALCHEMY_DATABASE_URI: sqlite:///D:\\chatchat-test\\data\\knowledge_base\\info.db\n\n\n10.1.4.3 配置知识库（kb_settings.yaml）（可选）\n默认使用 FAISS 知识库，如果想连接其它类型的知识库，可以修改 DEFAULT_VS_TYPE 和 kbs_config。\n\n\n10.1.4.4 配置搜索引擎（tool_settings.yml）\n# 搜索引擎工具配置项。推荐自己部署 searx 搜索引擎，国内使用最方便。\nsearch_internet:\n  use: true\n  search_engine_name: duckduckgo\n  search_engine_config:\n    bing:\n      bing_search_url: https://api.bing.microsoft.com/v7.0/search\n      bing_key: '6f4f35****'\n\n\n\n10.1.5 启动服务\n\n10.1.5.1 初始化知识库\n运行下面命令初始化。\nchatchat kb -r\n更多功能可以查看 chatchat kb --help\n\n\n10.1.5.2 启动项目\nchatchat start -a\n出现网页界面即为启动成功。",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>本地知识库</span>"
    ]
  },
  {
    "objectID": "langchain-chatchat.html#更新知识库",
    "href": "langchain-chatchat.html#更新知识库",
    "title": "10  本地知识库",
    "section": "10.2 更新知识库",
    "text": "10.2 更新知识库",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>本地知识库</span>"
    ]
  },
  {
    "objectID": "openai.html",
    "href": "openai.html",
    "title": "11  OpenAI",
    "section": "",
    "text": "11.1 OpenAI 简介\nOpenAI是一家致力于人工智能研究和应用的科技公司，成立于2015年，总部位于美国旧金山。其使命是确保人工智能（AI）造福全人类。",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>OpenAI</span>"
    ]
  },
  {
    "objectID": "openai.html#openai-简介",
    "href": "openai.html#openai-简介",
    "title": "11  OpenAI",
    "section": "",
    "text": "11.1.1 主要目标\nOpenAI的主要目标是开发和推广友好的AI，推动AI技术的发展，同时确保其安全性和公正性。公司致力于将先进的AI技术应用到各个领域，如自然语言处理、计算机视觉、游戏、医疗等，推动社会和科技的进步。\n\n\n11.1.2 发展历程\n\n2015年: OpenAI由Elon Musk、Sam Altman、Greg Brockman等人共同创立，最初是一个非营利性组织，旨在通过公开研究来推动AI的安全发展。\n2016年: 发布了第一个开源AI工具包，OpenAI Gym，用于开发和比较强化学习算法。\n2019年: 发布了GPT-2模型，这是一种用于自然语言生成的强大模型。由于担心滥用，OpenAI最初选择不发布完整模型。\n2020年: 发布了GPT-3，这是当时最先进的自然语言处理模型之一，具有1750亿个参数。GPT-3引起了广泛关注，并被应用于各种语言任务，如翻译、文本生成和对话系统。\n2023年: 推出了GPT-4，一个进一步改进和扩展的语言模型，并探索了多模态AI（如文本、图像、视频的联合处理）。\n\n\n\n11.1.3 商业化\n尽管最初成立为非营利组织，OpenAI在2019年创建了一个“有限盈利”（capped-profit）实体，OpenAI LP，以确保能够吸引大量的资金用于研究和开发，同时将利润限制在一定范围内。OpenAI的商业化产品包括与微软合作推出的基于GPT-3的API服务，这些服务被集成到各种应用和平台中。\n\n\n11.1.4 关键技术和产品\n\nGPT（Generative Pre-trained Transformer）系列: GPT是OpenAI开发的自然语言处理模型系列，能够生成与人类相似的文本。GPT-3和GPT-4是其中最著名的版本，被广泛应用于文本生成、对话系统、代码生成等领域。\nDALL-E: 一个用于生成图像的模型，基于文本描述生成图像。DALL-E 展示了在多模态生成任务中的强大能力。\nCodex: 一个专门针对代码生成的模型，能够理解和生成多种编程语言的代码。Codex被集成到GitHub Copilot中，帮助开发者编写代码。\nWhisper: 一个用于语音识别的模型，能够高效地将语音转换为文本，支持多种语言和方言。\n\n\n\n11.1.5 社会影响\nOpenAI的技术对社会产生了深远的影响，尤其是在自动化、教育、内容创作、客服、健康等领域。然而，也有关于AI技术滥用、隐私问题和工作岗位替代等方面的担忧。OpenAI致力于通过研究、安全协议和政策倡导来解决这些问题，确保AI的公平、安全和透明。\n\n\n11.1.6 未来展望\nOpenAI继续探索更为复杂和强大的AI技术，特别是通用人工智能（AGI）。他们的目标是开发能够执行广泛任务并具有类人智能的AI系统，同时保持对其安全性和伦理问题的高度关注。\n通过与学术界、行业和政府的合作，OpenAI致力于将AI转变为造福全人类的强大工具，同时减少潜在的风险。",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>OpenAI</span>"
    ]
  },
  {
    "objectID": "openai.html#openai-模型",
    "href": "openai.html#openai-模型",
    "title": "11  OpenAI",
    "section": "11.2 OpenAI 模型",
    "text": "11.2 OpenAI 模型\n截至 2024年8月，OpenAI 通过 API 接口提供了以下 GPT-4 系列模型：GPT-4、GPT-4o 和 GPT-4o Mini，它们在性能、规模和应用场景上有所不同：\n\nGPT-4：\n\n发布时间：最初发布于2023年。\n特点：这是OpenAI的标准大模型，具有强大的自然语言处理能力，支持复杂任务和长上下文处理。GPT-4在各种文本生成、理解和推理任务中表现优异，适用于需要高精度和复杂性的应用。\n\nGPT-4o：\n\n发布时间：2024年发布的优化版本。\n特点：GPT-4o是GPT-4的优化版，重点在于提升模型的性能和成本效益。该模型在处理用户反馈、实验结果的基础上进行了改进，更适合大规模应用，同时保持了GPT-4的高质量输出能力。\n\nGPT-4o Mini：\n\n发布时间：2024年7月发布。\n特点：这是GPT-4o的缩小版，专为那些需要在成本与性能之间找到平衡的场景设计。尽管模型规模较小，GPT-4o Mini在学术基准测试中超过了GPT-3.5 Turbo，尤其在多模态推理和长上下文处理方面表现出色。它适用于资源有限的场景，但依然能提供高质量的文本处理能力。\n\n\n总结来说，GPT-4 是全规模的高性能模型，GPT-4o 是在此基础上的性能优化版本，而 GPT-4o Mini 则是针对小规模应用场景设计的成本效益更高的版本。",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>OpenAI</span>"
    ]
  },
  {
    "objectID": "openai.html#使用-openai",
    "href": "openai.html#使用-openai",
    "title": "11  OpenAI",
    "section": "11.3 使用 OpenAI",
    "text": "11.3 使用 OpenAI\n\n通过网页访问。\n\nOpenAI 官方网站：https://chatgpt.com/\n国内私服、中转站\n\nhttps://pro.gpt-academic.top\n\n\n使用 API 接口。\n\n购买国内可用的 API KEY。\n\nChatAnywhere：https://github.com/chatanywhere/GPT_API_free\n\n安装配置本地软件。\n\nChatBox：https://github.com/Bin-Huang/chatbox\nZotero：https://zotero.org\nGPT_academic：https://github.com/binary-husky/gpt_academic",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>OpenAI</span>"
    ]
  },
  {
    "objectID": "coze.html",
    "href": "coze.html",
    "title": "12  Coze",
    "section": "",
    "text": "12.1 ChatGPT\n要用 Python 实现这个 curl 命令，可以使用 requests 库。以下是相应的 Python 代码：\nimport requests\nimport os\n\nurl = \"https://api.coze.com/v3/chat\"\nbot_id = \"7403315657221079041\"\ncoze_api_key = os.getenv(\"COZE_APE_KEY\")\nif coze_api_key is None:\n    coze_api_key = \"pat_YEj2OkoKUFDzmZnN9uvnlGLQnJ0bK8yqii5Zz3pQ54RI7nLcpb3qxBxvphScpZ81\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {coze_api_key}\",\n    \"Content-Type\": \"application/json\"\n}\n\ndata = {\n    \"bot_id\": bot_id,\n    \"user_id\": \"0\",\n    \"stream\": False,\n    \"auto_save_history\": True,\n    \"additional_messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"你是谁？\",\n            \"content_type\": \"text\"\n        }\n    ]\n}\n\nresponse = requests.post(url, headers=headers, json=data)\n\nprint(response.status_code)\nprint(response.json())  # Assuming the response is JSON; otherwise use response.text\n\n200\n{'data': {'id': '7403350281725181960', 'conversation_id': '7403350281725165576', 'bot_id': '7403315657221079041', 'created_at': 1723726839, 'last_error': {'code': 0, 'msg': ''}, 'status': 'in_progress'}, 'code': 0, 'msg': ''}\n代码说明：\n根据服务器返回的信息，status 字段的值为 \"in_progress\"，这意味着当前操作正在进行中，尚未完成。下一步的操作通常取决于你希望如何处理这个状态。",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Coze</span>"
    ]
  },
  {
    "objectID": "coze.html#chatgpt",
    "href": "coze.html#chatgpt",
    "title": "12  Coze",
    "section": "",
    "text": "url: 这是要发送请求的完整 URL，包括查询参数。\nheaders: 请求头包含了授权令牌和内容类型，确保请求被正确处理。\ndata: 这是发送的 JSON 数据，直接在 Python 中使用字典来构建。\nrequests.post: 发送 POST 请求，传入 URL、头信息和 JSON 数据。\nresponse: 响应对象，你可以检查其状态码（status_code）并解析响应内容。\n\n\n\n12.1.1 可能的下一步：\n\n轮询状态：\n\n如果服务器需要时间来完成某个任务，你可以使用轮询（定期发送请求来检查状态是否发生变化）。例如，每隔几秒发送一次请求，直到 status 变为 \"completed\" 或其他表示任务完成的状态。\n\n示例代码：\n\n\nimport time\nimport json\n\nresponse_json = response.json()\n\nchat_id = response_json[\"data\"][\"id\"]\nconversation_id = response_json[\"data\"][\"conversation_id\"]\n\n# 检查是否生成完毕\nretrieve_url = f\"https://api.coze.com/v3/chat/retrieve?chat_id={chat_id}&conversation_id={conversation_id}\"\n\nwhile True:  \n    response = requests.get(retrieve_url, headers=headers)\n    data = response.json()\n    status = data['data']['status']\n\n    if status == 'completed':\n        break\n    else:\n        print(f\"Task in progress... Status: {status}\")\n        time.sleep(0.5)\n\nTask in progress... Status: in_progress\nTask in progress... Status: in_progress\nTask in progress... Status: in_progress\nTask in progress... Status: in_progress\nTask in progress... Status: in_progress\nTask in progress... Status: in_progress\nTask in progress... Status: in_progress\nTask in progress... Status: in_progress\nTask in progress... Status: in_progress\nTask in progress... Status: in_progress\nTask in progress... Status: in_progress\n\n\n获取回答。\n\n# 获取回答\nmessage_url = f\"https://api.coze.com/v3/chat/message/list?chat_id={chat_id}&conversation_id={conversation_id}\"\n\nresponse = requests.post(message_url, headers=headers)\ndata = response.json()\nans = data['data'][1]['content']\n\nprint(ans)\n\n我是chatchat，一个虚拟的学术助手。我可以帮助你研究各种主题，并提供结构化的文章或论文。如果你有任何特定的研究需求或问题，请告诉我，我会尽力帮你解决。你今天想研究什么主题呢？",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Coze</span>"
    ]
  },
  {
    "objectID": "bailian.html",
    "href": "bailian.html",
    "title": "13  阿里云百炼",
    "section": "",
    "text": "13.1 百炼模型\n百炼提供了丰富多样的模型选择，它集成了通义系列大模型和第三方大模型，涵盖文本、图像、音视频等不同模态。\n参见：https://help.aliyun.com/zh/model-studio/getting-started/models?",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>阿里云百炼</span>"
    ]
  },
  {
    "objectID": "bailian.html#百炼模型",
    "href": "bailian.html#百炼模型",
    "title": "13  阿里云百炼",
    "section": "",
    "text": "13.1.1 语言模型\n您可以使用 OpenAI Python SDK、DashScope SDK 或 HTTP 接口调用通义千问模型。\n\n13.1.1.1 使用 DashScope SDK\n使用 DashScope SDK 时，会自动读取环境变量中的 API_KEY 并创建对象。\n\n# Refer to the document for workspace information: https://help.aliyun.com/document_detail/2746874.html    \n        \nfrom http import HTTPStatus\nimport dashscope\n\nmessages = [{'role': 'user', 'content': '你是谁'}]\nresponse = dashscope.Generation.call(\"qwen-turbo\",\n                            messages=messages,\n                            result_format='message',  # set the result to be \"message\"  format.\n                            stream=False, # set streaming output\n                            )\n\npprint(response.output.choices[0].message.content, width = 72)\n\n'我是阿里云开发的一款超大规模语言模型，我叫通义千问。作为一个AI助手，我的目标是帮助用户获得准确、有用的信息，解决他们的问题和困惑。无论是关于技术、教育、生活常识还是其他领域的问题，我都会尽我所能提供合适的帮助。如果您有任何想要了解的内容，欢迎随时向我提问！'\n\n\n\n\n13.1.1.2 使用 OpenAI SDK\n\nfrom openai import OpenAI\nimport os\n\n# 创建 client\nclient = OpenAI(\n    api_key=os.getenv(\"DASHSCOPE_API_KEY\"), # 如果您没有配置环境变量，请在此处用您的API Key进行替换\n    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",  # 填写DashScope服务的base_url\n)\n\n# 生成对话\ncompletion = client.chat.completions.create(\n    model=\"qwen-turbo\",\n    messages=[\n        {'role': 'system', 'content': 'You are a helpful assistant.'},\n        {'role': 'user', 'content': '你是谁？'}],\n    temperature=0.8,\n    top_p=0.8\n    )\n\nprint(completion.choices[0].message.content)\n\n我是阿里云开发的一款超大规模语言模型，我叫通义千问。\n\n\n\n\n\n13.1.2 视觉推理\nQwen-VL(qwen-vl-plus/qwen-vl-max) 模型现有几大特点：\n\n大幅增强了图片中文字处理能力，能够成为生产力小帮手，提取、整理、总结文字信息不在话下。\n增加可处理分辨率范围，各分辨率和长宽比的图都能处理，大图和长图能看清。\n增强视觉推理和决策能力，适于搭建视觉Agent，让大模型Agent的想象力进一步扩展。\n升级看图做题能力，拍一拍习题图发给Qwen-VL，大模型能帮用户一步步解题。\n\n\n\nfrom http import HTTPStatus\nimport dashscope\n\n\ndef simple_multimodal_conversation_call():\n    \"\"\"Simple single round multimodal conversation call.\n    \"\"\"\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"image\": \"https://dashscope.oss-cn-beijing.aliyuncs.com/images/dog_and_girl.jpeg\"},\n                {\"text\": \"这是什么?\"}\n            ]\n        }\n    ]\n    response = dashscope.MultiModalConversation.call(model='qwen-vl-plus',\n                                                     messages=messages)\n    # The response status_code is HTTPStatus.OK indicate success,\n    # otherwise indicate request is failed, you can get error code\n    # and message from code and message.\n    if response.status_code == HTTPStatus.OK:\n        return(response)\n    else:\n        print(response.code)  # The error code.\n        print(response.message)  # The error message.\n\n\nresponse = simple_multimodal_conversation_call()\ncontent = response.output.choices[0]['message']['content'][0]['text']\npprint(content, width = 72)\n\n('这张图片显示了一位女士和一只狗在海滩上。她们似乎正在互动，可能是在玩耍或训练中握手。背景是美丽的日落景色，海浪轻轻拍打着海岸线。\\n'\n '\\n'\n '这位女士穿着格子衬衫，并且戴着一个手镯。她坐在沙滩上与她的宠物进行着愉快的交流。这只狗看起来很友好并且对主人非常忠诚。整个场景充满了和谐、快乐以及人与动物之间的深厚情感联系。这是一个温馨的画面，让人感受到大自然的美好和平静。')\n\n\n\n\n13.1.3 文生图\n通义万相-文本生成图像是基于自研的Composer组合生成框架的AI绘画创作大模型，能够根据用户输入的文字内容，生成符合语义描述的多样化风格的图像。通过知识重组与可变维度扩散模型，加速收敛并提升最终生成图片的效果，布局自然、细节丰富、画面细腻、结果逼真。AI深度理解中英文文本语义，让文字秒变精致AI画作。\n当前模型支持的风格包括但不限于：\n\n水彩、油画、中国画、素描、扁平插画、二次元、3D卡通。\n支持中英文双语输入。\n支持客户自定义咒语书/修饰词，可生成不同风格、不同主题、不同派别的图片，满足个性创意的AI图片生成需求。\n支持输入参考图片进行参考内容或者参考风格迁移，支持更丰富的风格、主题和派别，AI作画质量更加高保真。\n\n\nfrom http import HTTPStatus\nfrom urllib.parse import urlparse, unquote\nfrom pathlib import PurePosixPath\nimport requests\nfrom dashscope import ImageSynthesis\n\ndef simple_call(prompt = 'Mouse rides elephant', out_dir = \"output\"):\n    rsp = ImageSynthesis.call(model=ImageSynthesis.Models.wanx_v1,\n                              prompt=prompt,\n                              n=1,\n                              size='1024*1024')\n    if rsp.status_code == HTTPStatus.OK:\n        print(rsp.output)\n        print(rsp.usage)\n        # save file to current directory\n        files = []\n        for result in rsp.output.results:\n            file_name = PurePosixPath(unquote(urlparse(result.url).path)).parts[-1]\n            file = './%s/%s' % (out_dir, file_name)\n            with open(file, 'wb+') as f:\n                f.write(requests.get(result.url).content)\n            files.append(file)\n        return(files)\n    else:\n        print('Failed, status_code: %s, code: %s, message: %s' %\n              (rsp.status_code, rsp.code, rsp.message))\n\n\nimage_path = simple_call()\n\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# 读取所有图片文件\nimage_path = \"output/435e0b57-6884-4e32-9187-2ada667b0ecb-1.png\"\nimage = Image.open(image_path)\n\nplt.imshow(image)\n\n\n\n\n\n\n\n\n\n\n13.1.4 语音识别\nSenseVoice 语音识别大模型专注于高精度多语言语音识别、情感辨识和音频事件检测，支持超过 50 种语言的识别，整体效果优于 Whisper 模型，中文与粤语识别准确率相对提升在 50% 以上。\n\n# For prerequisites running the following sample, visit https://help.aliyun.com/document_detail/611472.html\n\nimport json\nfrom urllib import request\nfrom http import HTTPStatus\n\nimport dashscope\n\n\ntask_response = dashscope.audio.asr.Transcription.async_call(\n    model='sensevoice-v1',\n    file_urls=[\n        'https://dashscope.oss-cn-beijing.aliyuncs.com/samples/audio/sensevoice/rich_text_example_1.wav'],\n    language_hints=['en'],)\n\ntranscription_response = dashscope.audio.asr.Transcription.wait(\n    task=task_response.output.task_id)\n\nif transcription_response.status_code == HTTPStatus.OK:\n    for transcription in transcription_response.output['results']:\n        url = transcription['transcription_url']\n        result = json.loads(request.urlopen(url).read().decode('utf8'))\n        pprint(json.dumps(result, indent=4, ensure_ascii=False))\n    print('transcription done!')\nelse:\n    print('Error: ', transcription_response.output.message)\n\n('{\\n'\n '    \"file_url\": '\n '\"https://dashscope.oss-cn-beijing.aliyuncs.com/samples/audio/sensevoice/rich_text_example_1.wav\",\\n'\n '    \"properties\": {\\n'\n '        \"audio_format\": \"pcm_s16le\",\\n'\n '        \"channels\": [\\n'\n '            0\\n'\n '        ],\\n'\n '        \"original_sampling_rate\": 16000,\\n'\n '        \"original_duration_in_milliseconds\": 17645\\n'\n '    },\\n'\n '    \"transcripts\": [\\n'\n '        {\\n'\n '            \"channel_id\": 0,\\n'\n '            \"content_duration_in_milliseconds\": 13240,\\n'\n '            \"text\": \"&lt;|Speech|&gt; Senior staff, Principal Doris Jackson, '\n 'Wakefield faculty, and of course, my fellow classmates. &lt;|/Speech|&gt; '\n '&lt;|ANGRY|&gt;&lt;|Applause|&gt; &lt;|Speech|&gt; I am honored &lt;|/Applause|&gt; to have been '\n 'chosen to speak before my classmates, as well as the students across America '\n 'today. &lt;|/Speech|&gt;\",\\n'\n '            \"sentences\": [\\n'\n '                {\\n'\n '                    \"begin_time\": 0,\\n'\n '                    \"end_time\": 7480,\\n'\n '                    \"text\": \"&lt;|Speech|&gt; Senior staff, Principal Doris '\n 'Jackson, Wakefield faculty, and of course, my fellow classmates. &lt;|/Speech|&gt; '\n '&lt;|ANGRY|&gt;\"\\n'\n '                },\\n'\n '                {\\n'\n '                    \"begin_time\": 11880,\\n'\n '                    \"end_time\": 17640,\\n'\n '                    \"text\": \"&lt;|Applause|&gt; &lt;|Speech|&gt; I am honored '\n '&lt;|/Applause|&gt; to have been chosen to speak before my classmates, as well as '\n 'the students across America today. &lt;|/Speech|&gt;\"\\n'\n '                }\\n'\n '            ]\\n'\n '        }\\n'\n '    ]\\n'\n '}')\ntranscription done!\n\n\n\n\n13.1.5 语音合成\nSambert 语音合成 API 基于达摩院改良的自回归韵律模型，支持文本至语音的实时流式合成。\n\nimport sys\n\nimport dashscope\nfrom dashscope.audio.tts import SpeechSynthesizer\n\nresult = SpeechSynthesizer.call(model='sambert-zhichu-v1',\n                                text='今天天气怎么样',\n                                sample_rate=48000)\n\ntts_output = \"output/weather.wav\"\nif result.get_audio_data() is not None:\n    with open(tts_output, 'wb') as f:\n        f.write(result.get_audio_data())\n    print('SUCCESS: get audio data: %dbytes in output.wav' %\n          (sys.getsizeof(result.get_audio_data())))\nelse:\n    print('ERROR: response is %s' % (result.get_response()))\n\nSUCCESS: get audio data: 135677bytes in output.wav\n\n\n使用 IPython.display 模块的 Audio 类来显示音频文件。\n\nfrom IPython.display import Audio\nfrom IPython.core.display import HTML\n\ndef html_tag_audio(file, file_type='wav'):\n    file_type = file_type.lower()\n    if file_type not in ['wav', 'mp3', 'ogg']:\n        raise ValueError(\"Invalid audio type. Supported types: 'wav', 'mp3', 'ogg'.\")\n    \n    audio_tag = f'''\n    &lt;audio controls&gt;\n      &lt;source src=\"{file}\" type=\"audio/{file_type}\"&gt;\n      Your browser does not support the audio element.\n    &lt;/audio&gt;\n    '''\n    return HTML(audio_tag)\n\n# Example usage\ntts_output = \"output/weather.wav\"\nhtml_tag_audio(tts_output, file_type=\"wav\")\n\n\n    \n      \n      Your browser does not support the audio element.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>阿里云百炼</span>"
    ]
  },
  {
    "objectID": "bailian.html#文档解析",
    "href": "bailian.html#文档解析",
    "title": "13  阿里云百炼",
    "section": "13.2 文档解析",
    "text": "13.2 文档解析\nQwen-Long 是在通义千问针对超长上下文处理场景的大语言模型，支持中文、英文等不同语言输入，支持最长 1000 万 tokens(约 1500 万字或 1.5 万页文档)的超长上下文对话。配合同步上线的文档服务，可支持 word、pdf、markdown、epub、mobi 等多种文档格式的解析和对话。\n\n13.2.1 上传文档\n\nfrom pathlib import Path\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=os.getenv(\"DASHSCOPE_API_KEY\"), # 如果您没有配置环境变量，请在此处用您的API Key进行替换\n    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",  # 填写DashScope服务的base_url\n)\n\nfile_object = client.files.create(file=Path(\"example/Kraken2.pdf\"), \n                                  purpose=\"file-extract\")\n\nprint(f\"文件上传成功，文件ID: {file_object.id}\")\n\n文件上传成功，文件ID: file-fe-FegrQSGslKgnjoM3hz9BylgC\n\n\n\n\n13.2.2 查询文件\n查询、删除文件。\n\n# 查询文件元信息\nclient.files.retrieve(file_object.id)\n\n# 查询文件列表\nclient.files.list()\n\nSyncPage[FileObject](data=[FileObject(id='file-fe-FegrQSGslKgnjoM3hz9BylgC', bytes=123750, created_at=1726562370, filename='Kraken2.pdf', object='file', purpose='file-extract', status='processed', status_details=None), FileObject(id='file-fe-d0surOfpU9ck8YP9grzxyeSU', bytes=1460, created_at=1726559802, filename='syncom-group-library.md', object='file', purpose='file-extract', status='processed', status_details=None), FileObject(id='file-fe-U8DwGdPB6Txo7cIo90MPAjlU', bytes=1006, created_at=1726559791, filename='gaoch-cv.md', object='file', purpose='file-extract', status='processed', status_details=None), FileObject(id='file-fe-iAiDxfZ6l09nmaNKNOjnYdCX', bytes=123750, created_at=1726557247, filename='Kraken2.pdf', object='file', purpose='file-extract', status='processed', status_details=None), FileObject(id='file-fe-5q4ob5AoPyEQA9vnQAIBaXIU', bytes=123750, created_at=1726556558, filename='Kraken2.pdf', object='file', purpose='file-extract', status='processed', status_details=None), FileObject(id='file-fe-LZaGRINYblYiXxAhJaHhBMZz', bytes=6416, created_at=1726556278, filename='安装Kraken2.md', object='file', purpose='file-extract', status='processed', status_details=None)], object='list', has_more=False)\n\n\n\n# 删除文件\nclient.files.delete(file_object.id)\n\n\n\n13.2.3 基于文档的对话\nQwen-Long 支持长文本（文档）对话，文档内容需放在 role 为 system 的 message 中，有以下两种方式可将文档信息输入给模型：\n\n在提前上传文档获取文档 ID（fileid）后，可以直接提供 fileid。支持在对话中使用一个或多个 fileid。\n直接输入需要处理的文本格式的文档内容（file content）。\n\n\nfrom pprint import pprint\n\n# 获取文件内容\n# 新文档上传后需要等待模型解析，首轮响应时间可能较长\ncompletion = client.chat.completions.create(\n    model=\"qwen-long\",\n    messages=[\n        {\n            'role': 'system',\n            'content': 'You are a helpful assistant.'\n        },\n        {\n            'role': 'system',\n            'content': f'fileid://{file_object.id}'\n        },\n        {\n            'role': 'user',\n            'content': '这篇文章讲了什么？'\n        }\n    ],\n    stream=False\n)\n\n\npprint(completion.choices[0].message.model_dump(), width = 72)\n\n{'content': '这篇文章介绍了几种常用的利用宏基因组reads进行物种注释的工具，包括Kraken2、MetaPhlAn和Centrifuge，分别描述了它们的工作原理、优缺点、使用的数据库以及输出结果。此外，文章还详细说明了如何在个人电脑上使用Kraken2进行物种注释分析，包括安装Kraken2、下载数据库、运行Kraken2进行分类注释及分析结果的具体步骤，并给出了一个使用R语言对Kraken2输出结果进行可视化的示例。',\n 'function_call': None,\n 'refusal': None,\n 'role': 'assistant',\n 'tool_calls': None}\n\n\n\n\n13.2.4 多个文档\n当有多个文档时，可以将多个 fileid 传递给 content。\n# 首次对话会等待文档解析完成，首轮响应时间可能较长\ncompletion = client.chat.completions.create(\n    model=\"qwen-long\",\n    messages=[\n        {\n            'role': 'system',\n            'content': 'You are a helpful assistant.'\n        },\n        {\n            'role': 'system',\n            'content': f\"fileid://{file_1.id},fileid://{file_2.id}\"\n        },\n        {\n            'role': 'user',\n            'content': '这几篇文章讲了什么？'\n        }\n    ],\n    stream=False\n)\n\n\n13.2.5 追加文档\n使用下面的方法，可以在对话过程中追加文档。\n\n# data_1.pdf为原文档，data_2.pdf为追加文档\nfile_1 = client.files.create(file=Path(\"example/gaoch-cv.md\"),\n                             purpose=\"file-extract\")\n\n# 初始化messages列表\nmessages = [\n    {\n        'role': 'system',\n        'content': 'You are a helpful assistant.'\n    },\n    {\n        'role': 'system',\n        'content': f'fileid://{file_1.id}'\n    },\n    {\n        'role': 'user',\n        'content': '这篇文章讲了什么？'\n    },\n]\n# 第一轮响应\ncompletion = client.chat.completions.create(\n    model=\"qwen-long\",\n    messages=messages,\n    stream=False\n)\n\n# 打印出第一轮响应\nprint(f\"第一轮响应：{completion.choices[0].message.model_dump()}\")\n\n第一轮响应：{'content': '这篇文章是关于高春辉的个人简介。高春辉是一位微生物学博士，目前在华中农业大学资源与环境学院担任副研究员和硕士生导师。文章介绍了他的教育背景、研究领域、科研项目、学术成果以及他在学术界的影响力。', 'refusal': None, 'role': 'assistant', 'function_call': None, 'tool_calls': None}\n\n\n将第一轮响应的内容添加到历史记录中。\n\n# 构造assistant_message\nassistant_message = {\n    \"role\": \"assistant\",\n    \"content\": completion.choices[0].message.content}\n\n# 将assistant_message添加到messages中\nmessages.append(assistant_message)\n\n上传一个新文档。\n\n# 获取追加文档的fileid\nfile_2 = client.files.create(file=Path(\"example/syncom-group-library.md\"), \n                             purpose=\"file-extract\")\n\n# 将追加文档的fileid添加到messages中\nsystem_message = {\n    'role': 'system',\n    'content': f'fileid://{file_2.id}'\n}\nmessages.append(system_message)\n\n# 添加用户问题\nmessages.append({\n    'role': 'user',\n    'content': '这两篇文章的内容有什么异同点？'\n})\n\n# 追加文档后的响应\ncompletion = client.chat.completions.create(\n    model=\"qwen-long\",\n    messages=messages,\n    stream=False\n)\n\nprint(f\"追加文档后的响应：{completion.choices[0].message.model_dump()}\")\n\n追加文档后的响应：{'content': '这两篇文章的主要相同点在于它们都与教育和科学研究相关，并且都涉及到了微生物学这个领域。\\n\\n不同之处在于：\\n\\n1. 第一篇文章\"高春辉个人简介\"主要是介绍高春辉个人的职业生涯、研究成果以及他的专业领域。这是一篇关于个人的传记式文章。\\n\\n2. 而第二篇文章\"土壤生物膜与环境健康书屋源起\"则是在讲述一个特定的图书馆或者说是书屋的建立原因和它的运作规则。这篇文章的重点在于提供给学生和研究人员获取知识的途径，以及对土壤微生物学的研究的重要性。', 'refusal': None, 'role': 'assistant', 'function_call': None, 'tool_calls': None}",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>阿里云百炼</span>"
    ]
  },
  {
    "objectID": "volcengine.html",
    "href": "volcengine.html",
    "title": "14  火山引擎",
    "section": "",
    "text": "14.1 可用的模型\n截止2024年08月15日，一共有 19 个模型，其中 11 个是字节跳动的豆包模型，2 个是 Meta 的 llama3-70b、llama3-8b，3 个 Moonshot 的模型，2 个智谱 AI 的模型。\n这些模型分为 3 类：大语言模型（对话）、语音大模型（语音合成）、视觉大模型（文生图）。\n这些模型都需要先开通，然后才能使用。开通后有一定的免费额度。超过额度后按照 Token（语言）或者调用次数（文生图）收费。",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>火山引擎</span>"
    ]
  },
  {
    "objectID": "volcengine.html#模型调用实例",
    "href": "volcengine.html#模型调用实例",
    "title": "14  火山引擎",
    "section": "14.2 模型调用实例",
    "text": "14.2 模型调用实例\n\n14.2.1 调用大语言模型\n在火山方舟中调用语言模型可以使用火山自己的 API，另外也支持 OpenAI 的方法。\n在下面的代码中，model 的值是 接入点名称，而非模型名称。接入点采用的模型在创建接入点的时候选定，使用的时候对用户不可见。这里使用的 ep-20240815100632-2m9cl 使用的是豆包大模型 Doubao-pro-128k 240628。\n\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key = os.environ.get(\"ARK_API_KEY\"),\n    base_url = \"https://ark.cn-beijing.volces.com/api/v3\",\n)\n\n# Non-streaming:\nprint(\"----- standard request -----\")\ncompletion = client.chat.completions.create(\n    model = \"ep-20240815100632-2m9cl\",  # your model endpoint ID\n    messages = [\n        {\"role\": \"system\", \"content\": \"你是豆包，是由字节跳动开发的 AI 人工智能助手\"},\n        {\"role\": \"user\", \"content\": \"常见的十字花科植物有哪些？\"},\n    ],\n)\nprint(completion.choices[0].message.content)\n\n----- standard request -----\n常见的十字花科植物有很多，以下为您列举一些：\n1. 白菜：包括大白菜、小白菜等，是人们日常生活中常见的蔬菜。\n2. 萝卜：有白萝卜、胡萝卜等品种。\n3. 甘蓝：如紫甘蓝、卷心菜等。\n4. 芥菜：包括芥菜疙瘩、雪里蕻等。\n5. 油菜：既是重要的油料作物，嫩苗也可作为蔬菜食用。\n6. 花椰菜：又名菜花。\n7. 西兰花：营养丰富，深受人们喜爱。\n\n这些十字花科植物在世界各地广泛种植，具有重要的经济和营养价值。\n\n\n流式输出。\n\n# Streaming:\nprint(\"----- streaming request -----\")\nstream = client.chat.completions.create(\n    model = \"ep-20240815100632-2m9cl\",  # your model endpoint ID\n    messages = [\n        {\"role\": \"system\", \"content\": \"你是豆包，是由字节跳动开发的 AI 人工智能助手\"},\n        {\"role\": \"user\", \"content\": \"常见的十字花科植物有哪些？\"},\n    ],\n    stream=True\n)\n\nfor chunk in stream:\n    if not chunk.choices:\n        continue\n    print(chunk.choices[0].delta.content, end=\"\")\nprint()\n\n----- streaming request -----\n常见的十字花科植物有很多，以下为您列举一些：\n1. 白菜：包括大白菜、小白菜等，是人们日常生活中常见的蔬菜。\n2. 甘蓝：如卷心菜、紫甘蓝等，富含维生素和膳食纤维。\n3. 萝卜：有白萝卜、红萝卜、青萝卜等品种。\n4. 芥菜：包括叶用芥菜、茎用芥菜等。\n5. 油菜：既可作为蔬菜食用，也可用于榨油。\n6. 花椰菜：又称菜花，营养丰富。\n7. 西兰花：具有较高的营养价值，深受人们喜爱。\n8. 诸葛菜：又名二月兰，常作为观赏植物和野菜。\n\n这些十字花科植物在人们的饮食和生活中都具有重要的地位。\n\n\n\n\n14.2.2 调用文生图模型\n需要用 POST 方式或者借助于自定义的访问接口模块。\n（略）",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>火山引擎</span>"
    ]
  },
  {
    "objectID": "research-summary.html",
    "href": "research-summary.html",
    "title": "15  研究概要",
    "section": "",
    "text": "15.1 任务流程\n首先将参考文献的摘要进行整理，然后使用模型来分析这些摘要。以下是一个基本的步骤：\n这样，你就可以利用模型快速了解大量参考文献的主要内容，并根据自己的兴趣和研究方向选择需要深入阅读的文献。\n当然，以下是一个具体的实施示例，假设你有几篇参考文献的摘要，并希望模型分析其中的主要研究主题：",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>研究概要</span>"
    ]
  },
  {
    "objectID": "research-summary.html#任务流程",
    "href": "research-summary.html#任务流程",
    "title": "15  研究概要",
    "section": "",
    "text": "收集摘要：将你所有参考文献的摘要收集起来，可以按照主题或年份等进行分类。\n输入模型：将这些摘要分批输入模型。由于每个摘要可能较短，你可以一次输入多个摘要。\n分析主题：要求模型根据输入的摘要，识别出每个参考文献的主要研究主题。\n\n\n\n\n15.1.1 示例摘要\n\n摘要 1: “This study explores the effects of soil microbiomes on plant growth in maize, focusing on nitrogen fixation and plant health under different environmental conditions.”\n摘要 2: “The research investigates the impact of synthetic microbial communities on soil nutrient cycling, analyzing their potential to enhance agricultural sustainability.”\n摘要 3: “This paper examines the interactions between soil microorganisms and plant roots, specifically in the context of drought resistance in crops.”\n\n\n\n15.1.2 实施步骤\n第一步：收集摘要\n将这些摘要收集并整理好，可以按主题或关键字进行分类。\n第二步：输入模型\n假设你将这三个摘要分批输入模型，并让模型识别每个摘要的主要研究主题。你可以构造这样的提示：\n提示词示例:\n\n“请根据以下摘要识别每篇文章的主要研究主题：\n\n摘要 1: [输入摘要]\n摘要 2: [输入摘要]\n摘要 3: [输入摘要]”\n\n\n第三步：模型输出\n模型将分析这些摘要，并识别出每篇文章的主题。例如，模型可能给出的结果如下：\n\n\n摘要 1 主题: 植物健康与氮固定在不同环境条件下的变化\n摘要 2 主题: 微生物群落在土壤养分循环中的作用及其对农业可持续性的影响\n摘要 3 主题: 土壤微生物与作物抗旱性之间的相互作用",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>研究概要</span>"
    ]
  },
  {
    "objectID": "research-summary.html#代码实现",
    "href": "research-summary.html#代码实现",
    "title": "15  研究概要",
    "section": "15.2 代码实现",
    "text": "15.2 代码实现\n要实现这个过程，你可以使用 Python 结合 OpenAI 的 API 来处理文本。以下是一个代码示例，利用 GPT 模型分析摘要并提取主要主题。\n环境准备：\n（略）\n实现代码：\n\nanalyze_abstracts 函数：接受一个摘要列表，将它们拼接成一个请求提示，并传递给 GPT 模型进行处理。\nopenai.chat.completions.create()：调用 OpenAI 的模型来生成对话结果，输出每个摘要的主要主题。\nmodel=“gpt-4o-ca”：你可以根据需要选择不同的GPT模型。\n摘要输入：输入多个摘要，模型会分析并提取主要的研究主题。\n\n运行效果：\n模型会返回每个摘要的研究主题：\n\nimport openai\nimport os\nfrom IPython.display import Markdown\n\nopenai.base_url = \"https://api.chatanywhere.tech\"\nopenai.api_key = os.getenv(\"CHATANYWHERE_API_KEY\")\n\nsystem_prompt = \"这些文章的主题是什么？\"\n\ndef analyze_abstracts(abstracts):\n  # Create prompt with multiple messages (abstracts)\n  messages = [\n      {\"role\": \"system\", \"content\": system_prompt}\n  ]\n  for abstract in abstracts:\n      messages.append({\"role\": \"user\", \"content\": abstract})\n\n  # Use Completion.create with messages list\n  response = openai.chat.completions.create(\n      model=\"gpt-4o-ca\",  # Adjust model as needed\n      messages=messages,\n      n=1,  # Generate only 1 response\n  )\n\n  # Extract and return the analyzed content\n  return response.choices[0].message.content\n\n# 示例摘要\nabstracts = [\n  \"Paper1: This study explores the effects of soil microbiomes on plant growth in maize, focusing on nitrogen fixation and plant health under different environmental conditions.\",\n  \"Paper2: The research investigates the impact of synthetic microbial communities on soil nutrient cycling, analyzing their potential to enhance agricultural sustainability.\",\n  \"Paper3: This paper examines the interactions between soil microorganisms and plant roots, specifically in the context of drought resistance in crops.\"\n]\n\n\n# 分析摘要并输出主题\nthemes = analyze_abstracts(abstracts)\nMarkdown(themes)\n\n这些文章的主题如下：\n\nPaper 1: 研究土壤微生物群对玉米植物生长的影响，重点关注在不同环境条件下的固氮作用和植物健康。\nPaper 2: 调查合成微生物群落对土壤养分循环的影响，分析其增强农业可持续性的潜力。\nPaper 3: 探讨土壤微生物与植物根系之间的相互作用，特别是在作物抗旱方面的应用。\n\n总体而言，这些文章均涉及土壤微生物与植物生长及健康之间的关系，关注点包括固氮、农业可持续性和抗旱性。",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>研究概要</span>"
    ]
  },
  {
    "objectID": "research-summary.html#与导出文件联动",
    "href": "research-summary.html#与导出文件联动",
    "title": "15  研究概要",
    "section": "15.3 与导出文件联动",
    "text": "15.3 与导出文件联动\n好的，接下来我们可以设计一个流程来自动读取 Web of Science 的导出文件（通常是 CSV/TSV 格式），然后每次批量处理 5 篇文献的摘要，并使用 GPT 模型提取主题，最后将提取的主题保存到本地文件中。下面是具体的实现思路和代码。\n流程步骤：\n\n读取CSV文件：从导出的Web of Science文件中读取文献的标题（ti列）和摘要（ab列）。\n分批处理摘要：每次提取5篇文献的摘要，使用GPT模型分析主题。\n保存结果：将每篇文献的标题、摘要以及提取到的主题保存到本地文件（如CSV文件）中。\n\n\n15.3.1 读取文件\n\nimport pandas as pd\n\n# 读取CSV文件中的标题和摘要\nfile = \"example/wos-fast5k/savedrecs.txt\"\n    \n# 根据文件的分隔符修改 delimiter 参数\n# 设置 index_col=False 来确保第一列不会被当作行索引\ndata = pd.read_csv(file, delimiter='\\t', index_col=False)  \n\n# 删除全为 NaN 的列\ndata = data.loc[:, data.notna().sum() != 0]\n\n# 查看处理后的数据\nprint(data[0:5])\n\n  PT                                                 AU   BA   CA   GP  \\\n0  J  Just, Berta Singla; Marks, Evan Alexander Neth...  NaN  NaN  NaN   \n1  J  Pereira-Marques, Joana; Ferreira, Rui M.; Figu...  NaN  NaN  NaN   \n2  J  Golzar-Ahmadi, Mehdi; Bahaloo-Horeh, Nazanin; ...  NaN  NaN  NaN   \n3  J  Li, Chenglong; Han, Yanfeng; Zou, Xiao; Zhang,...  NaN  NaN  NaN   \n4  J  Hu, Yu; Wang, Yulin; Wang, Runhua; Wang, Xiaok...  NaN  NaN  NaN   \n\n                                                  TI  \\\n0  Biofertilization increases soil organic carbon...   \n1  A metatranscriptomics strategy for efficient c...   \n2  Pathway to industrial application of heterotro...   \n3  A systematic discussion and comparison of the ...   \n4  Dirammox-dominated microbial community for bio...   \n\n                                                  SO   VL   IS   BP  ...  \\\n0  INTERNATIONAL JOURNAL OF AGRICULTURAL SUSTAINA...   22    1  NaN  ...   \n1                                       GUT MICROBES   16    1  NaN  ...   \n2                             BIOTECHNOLOGY ADVANCES   77  NaN  NaN  ...   \n3                SYNTHETIC AND SYSTEMS BIOTECHNOLOGY    9    4  775  ...   \n4             APPLIED MICROBIOLOGY AND BIOTECHNOLOGY  108    1  NaN  ...   \n\n     PY                                                 AB  \\\n0  2024  Protecting and building soil carbon has become...   \n1  2024  The high background of host RNA poses a major ...   \n2  2024  The transition to renewable energies and elect...   \n3  2024  Synthetic microbial community has widely conce...   \n4  2024  Direct ammonia oxidation (Dirammox) might be o...   \n\n                                                  C1   CT   CY   SP   CL TC  \\\n0  [Just, Berta Singla; Llenas, Laia; Ponsa, Serg...  NaN  NaN  NaN  NaN  0   \n1  [Pereira-Marques, Joana; Ferreira, Rui M.; Fig...  NaN  NaN  NaN  NaN  2   \n2  [Golzar-Ahmadi, Mehdi; Norouzi, Forough; Holus...  NaN  NaN  NaN  NaN  0   \n3  [Li, Chenglong; Han, Yanfeng; Zou, Xiao; Zhang...  NaN  NaN  NaN  NaN  0   \n4  [Hu, Yu; Wang, Yulin; Liu, Shuang-Jiang] Shand...  NaN  NaN  NaN  NaN  0   \n\n                                                  WC                   UT  \n0  Agriculture, Multidisciplinary; Green & Sustai...  WOS:001247571700001  \n1        Gastroenterology & Hepatology; Microbiology  WOS:001176335800001  \n2               Biotechnology & Applied Microbiology  WOS:001316252000001  \n3               Biotechnology & Applied Microbiology  WOS:001273513000001  \n4               Biotechnology & Applied Microbiology  WOS:001252334300001  \n\n[5 rows x 24 columns]\n\n\n下面，将 dataframe 中的数据逐批处理，生成 JSON 格式的文本。\n\nbatch_to_json()：这是函数的主体，将数据按批次进行处理，并将每行数据转化为 JSON 格式。\n批次切分：使用 data[i:i+batch_size] 对 DataFrame 进行分批处理。\n构建 JSON：每行数据提取出 UT、TI、AB 列的值，并构建一个包含 id、ti 和 ab 键的 JSON 对象。\njson.dumps()：将列表形式的 JSON 对象转换为 JSON 字符串。\nensure_ascii=False：确保输出的 JSON 字符串支持非 ASCII 字符。\n\n\nimport pandas as pd\nimport json\n\ndef batch_to_json(data, batch_size):\n    # 确保输入的 data 是一个 DataFrame，且 batch_size 是正整数\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"data 应该是一个 pandas DataFrame\")\n    if not isinstance(batch_size, int) or batch_size &lt;= 0:\n        raise ValueError(\"batch_size 应该是一个正整数\")\n    \n    # 提取 dataframe 中的每 batch_size 行数据\n    batches = [data[i:i+batch_size] for i in range(0, len(data), batch_size)]\n    \n    result = []\n    for batch in batches:\n        json_batch = []\n        for _, row in batch.iterrows():\n            # 构建每行的 JSON 对象\n            json_obj = {\n                \"ID\": row[\"UT\"],\n                \"TI\": row[\"TI\"],\n                \"AB\": row[\"AB\"]\n            }\n            json_batch.append(json_obj)\n        result.append(json.dumps(json_batch, ensure_ascii=False))  # 转换为 JSON 格式的字符串\n    return result\n\n# 示例使用\nbatch_size = 2\njson_output = batch_to_json(data[0:6], batch_size)\n\nfrom pprint import pprint\nfor batch in json_output:\n    Markdown(batch)\n\n\n\n15.3.2 调整提示词\nNote: 将提示词换成英文效果可能会更好。\n\n# 使用三引号插入一个非常长的系统提示符\nsystem_prompt = \"\"\"  \n请根据以下内容识别每篇文章的研究主题与**合成菌群**研究的相关性，并给出评分（完全不相关 0 分，十分相关 10 分，部分相关 1-9 分）：\n\n请注意：\n\n1. 合成菌群是指人们利用一个或者多个微生物组成的复合物，它可以是一个益生菌，一种生物肥料，或者一个小型的微生物群落。所有与这些内容相关的研究都应该被认为是合成菌群的相关研究。\n2. 后续的这些内容都会以 JSON 格式给出文章的 ID、标题（TI）和摘要（AB），请根据标题和摘要的内容进行判断，并且记录 ID 用于回答。\n3. 务必按照下面的要求回答。\n\n回答要求：\n\n1. 在回答时请给出文章的 ID，评分（score）与简短理由（reason）。\n2. 请使用 JSON 格式回答。\n\n\n下面是一个示例回答：\n\n{results: [{\"ID\":\"WOS:000996233600009\",\"score\":5, \"reason\": \"reason in Chinese\"},\n{\"ID\":\"WOS:001247571700001\",\"score\":10, \"reason\": \"reason in Chinese\"}]}\n\n\"\"\"\n\n要使 OpenAI API 调用的结果以统一的 JSON 格式返回，可以按照以下步骤进行设置：\n\ndef analyze_content(content):\n  # Create prompt with multiple messages (abstracts)\n  messages = [\n      {\"role\": \"system\", \"content\": system_prompt}\n  ]\n  \n  messages.append({\"role\": \"user\", \"content\": content})\n\n  # Use Completion.create with messages list\n  response = openai.chat.completions.create(\n      model=\"gpt-4o-ca\",  # Adjust model as needed\n      messages=messages,\n      response_format={\"type\": \"json_object\"}\n  )\n\n  # Extract and return the analyzed content\n  return response.choices[0].message.content\n\n\nMarkdown(analyze_content(json_output[0]))\n\n{ “results”: [ { “ID”: “WOS:001247571700001”, “score”: 10, “reason”: “这篇文章研究了生物肥料微生物接种剂对土壤有机碳浓度的影响，符合合成菌群的定义，即一个或多个微生物组成的复合物。” }, { “ID”: “WOS:001176335800001”, “score”: 7, “reason”: “这篇文章虽然主要讨论了在人类组织中低微生物含量情况下的微生物组表征策略，但提到了对合成样本的测试，涉及通过合成的方式来验证方法，因此部分相关。” } ] }\n\n\n\n\n15.3.3 批处理\n现在可以进行批处理，并将结果保存下来。\n\nbatches = batch_to_json(data[0:20], batch_size=5)\n\nresults = []\n\nfor batch in batches:\n  result = analyze_content(batch)\n  results.append(result)\n\n将列表合并成一个数据框，并保存到硬盘中供后续分析使用。\n\nimport json\n\ndata_list = []\n\n\nfor json_data in results:\n  data = json.loads(json_data)\n  result = data['results']\n\n  for item in result:\n    data_list.append([item['ID'], item['score'], item['reason']])\n\ndf = pd.DataFrame(data_list, columns=['ID', 'score', 'reason'])\n\ndf\n# df.to_csv('output.csv', index=False)\n\n\n\n\n\n\n\n\nID\nscore\nreason\n\n\n\n\n0\nWOS:001247571700001\n9\n该研究评估了微生物接种剂（生物肥料）对土壤有机碳浓度的影响，这里讨论了不同类型的生物肥料和微...\n\n\n1\nWOS:001176335800001\n7\n这篇文章描述了合成样本（包含微生物和宿主细胞组成）的生成及其用于优化和建立分析流程，尽管重点...\n\n\n2\nWOS:001316252000001\n6\n这篇文章探讨了异养微生物在从电子废物中回收金属的产业应用，其中涉及人工微生物群落和合成生物学...\n\n\n3\nWOS:001273513000001\n10\n本文系统讨论和比较了合成微生物群落的构建方法，直接讨论了合成菌群的概念、应用和构建方法，是完...\n\n\n4\nWOS:001252334300001\n8\n该研究介绍了一种特定的微生物群落在废水处理中的应用，虽然重点是Dirammox细菌，但涉及微...\n\n\n5\nWOS:001209713100001\n6\n这篇文章虽然主要讨论的是微藻用于生物燃料和其他增值产品的可持续利用，但也提到微藻可以作为生物...\n\n\n6\nWOS:001198552100002\n9\n本研究专注于微生物群落的交互作用研究，并提到了开发合成微生物共生体的定量和定性方法，因此它与...\n\n\n7\nWOS:001171546000001\n10\n这篇文章明确讨论了一种基于功能基因的合成微生物群落的构建，以促进水稻硫循环。完全符合合成菌群...\n\n\n8\nWOS:001314268600001\n10\n本研究设计了一种用于促进污染地块小麦生长的合成微生物群落，并使用了基因组规模代谢模型。这完全...\n\n\n9\nWOS:001313702300001\n0\n该研究主要探讨了多重应激源对淡水生态系统的影响，而没有涉及合成菌群的概念或应用。因此，与合成...\n\n\n10\nWOS:001320306700001\n8\n虽然主要研究对象是塑料降解，但使用了微藻，这符合合成菌群的定义，即利用多个微生物组成的复合物...\n\n\n11\nWOS:001319961700001\n10\n研究集中在从蕃茄根际土壤中分离和评价促进植物生长的根际细菌，这完全符合合成菌群的研究。\n\n\n12\nWOS:001318177900001\n10\n研究探讨了藻类-细菌共生系统用于污染物去除，涉及多个微生物组成的复合物和其代谢优势，完全符合...\n\n\n13\nWOS:001318328600001\n6\n研究虽然涉及污水处理和细菌群落结构分析，但主要集中在化学处理方法上，合成菌群的参与程度中等。\n\n\n14\nWOS:001317596600001\n3\n主要聚焦于抗菌肽及其合成类似物作为抗生素替代品的开发，与合成菌群关系较弱。\n\n\n15\nWOS:001291429900001\n8\n这篇文章研究了将乳牛废水与水果和蔬菜残渣进行厌氧共消化的效果，并利用生成的生物肥料，这直接涉...\n\n\n16\nWOS:001319156200001\n7\n这篇文章研究了固氮蓝藻减少水稻中镉毒性和镉累积的问题，涉及到蓝藻作为生物肥料和重金属吸附剂的...\n\n\n17\nWOS:001307986400001\n6\n这篇文章在合成环境中研究了微生物社区组成的变化，涉及到多种微生物在共同代谢系统中的相互作用和...\n\n\n18\nWOS:001319855200001\n9\n这篇文章研究了Paenibacillus polymyxa J2-4和Pseudomonas...\n\n\n19\nWOS:001314605300007\n10\n本研究通过基因组学和合成社区实验揭示了醋酸细菌在酵母发酵中的关键代谢作用，直接涉及创建和操控...",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>研究概要</span>"
    ]
  },
  {
    "objectID": "llama.html",
    "href": "llama.html",
    "title": "16  Llama 3.2",
    "section": "",
    "text": "16.1 本地化部署",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Llama 3.2</span>"
    ]
  },
  {
    "objectID": "llama.html#本地化部署",
    "href": "llama.html#本地化部署",
    "title": "16  Llama 3.2",
    "section": "",
    "text": "16.1.1 使用 Ollama 本地化部署\n使用 ollama 部署 LLaMA 模型相对简单，它提供了一个命令行工具，可以快速运行 LLaMA 系列模型。以下是具体步骤：\n1. 安装 Ollama\n首先，你需要在本地安装 ollama 工具。可以使用以下步骤：\n\nmacOS 用户可以直接使用 brew 安装：\n\nbrew install ollama\n# 查看帮助\nollama --help\n\nWindows 和 Linux 用户可以访问 Ollama 的官网 下载相应的安装包。\n\n2. 加载 LLaMA 模型\n安装完成后，你可以通过命令行直接运行 LLaMA 模型。Ollama 会在首次运行时下载模型。\nollama run llama3.2:1b\n这是运行 LLaMA 2 的示例，你可以根据需求替换为其他版本的 LLaMA 模型（如 LLaMA 3）。Ollama 提供了一些默认模型名，具体可以通过 ollama 的文档或命令行查看。\n3. 查询支持的模型\n你可以通过以下命令来查看 ollama 本地可用的模型列表。访问 https://ollama.com/library 查看所有模型：\nollama list\n4. 本地模型部署\nOllama 主要针对本地部署，因此它会在首次运行模型时自动将所需的模型权重下载到本地，并通过本地硬件（如 CPU 或 GPU）加速推理。\n# 查看模型\nollama list\n\n# 运行模型\nollama run llama3.2:1b \"Hi, who are you?\"\nollama run llama3.2:3b \"Hi, who are you?\"\nollama run llama3.1:8b 'Hi, who are you?'\n5. 使用 Ollama API\n如果你想通过代码来调用 Ollama 提供的 LLaMA 模型，可以使用它的 API。这里是一个简单的 Python 示例：\n\nimport requests\n\nresponse = requests.post(\n    \"http://localhost:11434/api/generate\",\n    json={\"model\": \"llama3.1:8b\", \"prompt\": \"Hi, who are you?\", \"stream\": False}\n)\n\nprint(response.content.decode())\n\n{\"model\":\"llama3.1:8b\",\"created_at\":\"2024-10-15T07:29:47.257886Z\",\"response\":\"I'm an artificial intelligence model known as Llama. Llama stands for \\\"Large Language Model Meta AI.\\\"\",\"done\":true,\"done_reason\":\"stop\",\"context\":[128006,882,128007,271,13347,11,889,527,499,30,128009,128006,78191,128007,271,40,2846,459,21075,11478,1646,3967,439,445,81101,13,445,81101,13656,369,330,35353,11688,5008,16197,15592,1210],\"total_duration\":3170882792,\"load_duration\":46793334,\"prompt_eval_count\":16,\"prompt_eval_duration\":2294743000,\"eval_count\":23,\"eval_duration\":828374000}\n\n\n这个示例中，Ollama 的 API 会在本地运行，并且你可以通过 HTTP 请求向它发送推理任务。\n6. 查看运行状态\n# 查看运行状态\nollama ps\n\n# 停止模型\nollama stop llama3.1:8b\n\n\n16.1.2 Hugging Face 本地化部署\n使用 Hugging Face 本地化部署 LLaMA 3 模型，可以通过以下步骤实现。假设你已经了解如何安装必要的软件环境和依赖包。\n1. 确认系统配置\nLLaMA 3 是一个大型模型，通常需要高性能的硬件，包括至少 24GB 显存的 GPU 和足够的 CPU 内存。\n2. 安装依赖包\n确保你安装了 Hugging Face Transformers 库和 Accelerate 库来进行模型的加载和加速：\npip install transformers accelerate\n3. 获取 Hugging Face Access Token\n由于 LLaMA 3 是 Meta 的受限模型，你需要访问 Hugging Face 上的相应页面，并通过请求访问权限。获取后，将 huggingface-cli 配置好你的令牌：\nhuggingface-cli login\n4. 加载模型\n一旦你有了访问权限，可以使用 Hugging Face 的 Transformers 库来加载模型并运行它。\nNote: 在模型主页提交访问请求，等待批准。\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom IPython.display import Markdown\n\n# 加载 LLaMA 3 模型\nmodel_name = \"meta-llama/Llama-3.2-1b\"  # 模型名称根据具体需求替换\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n\n# 推理\ninputs = tokenizer(\"hi, who are you?\", return_tensors=\"pt\").to(\"mps\")\noutputs = model.generate(inputs.input_ids, max_length=50)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n5. 使用 Accelerate 加速推理\n如果模型非常大，可能需要利用 Accelerate 库来处理多 GPU 或 TPU 的环境加速推理。\nfrom accelerate import init_empty_weights, infer_auto_device_map\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# 初始化空模型\nwith init_empty_weights():\n    model = AutoModelForCausalLM.from_pretrained(model_name)\n\n# 推断自动设备映射\ndevice_map = infer_auto_device_map(model, max_memory={0: \"24GB\", 1: \"24GB\"})\n\n# 使用设备映射加载模型\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map=device_map)\n6. 本地化模型权重（可选）\n如果需要将模型权重完全下载到本地进行脱机推理，可以指定缓存路径：\nmodel = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=\"./local_model\")\ntokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=\"./local_model\")\n这将模型的所有文件下载到 ./local_model 文件夹中。\n7. 推理与调整\n使用 generate 方法进行文本生成，你可以根据任务需求，调整 max_length、temperature、top_p 等生成参数：\noutputs = model.generate(\n    inputs.input_ids, \n    max_length=100, \n    num_return_sequences=1, \n    temperature=0.7, \n    top_p=0.9\n)\n这样你就可以实现 Hugging Face 本地化部署 LLaMA 3 模型进行推理了。\n\n\n16.1.3 使用 vllm 本地化部署\nvLLM 是一个高性能的大语言模型推理和服务框架。它可以显著提高 LLaMA 等模型的推理速度。以下是使用 vLLM 本地部署 LLaMA 3.1 8B 模型的步骤：\n\n安装 vLLM：\n\npip install vllm\n\n下载运行 LLaMA 3.2 1B 模型：\n\nvllm serve meta-llama/Meta-Llama-3.2-1B\n\n使用 vLLM API（可选）：\n\nimport requests\n\nresponse = requests.post(\n    \"http://localhost:8000/v1/chat/completions\",\n    json={\"model\": \"meta-llama/Meta-Llama-3.2-1B\", \n          \"messages\": [{\"role\": \"user\", \"content\": \"Hello, who are you?\"}], \n          \"stream\": False}\n)\n\nprint(response.content.decode())\n\n\n16.1.4 总结\n使用 ollama 部署 LLaMA 模型，速度快，配置简单，适合快速部署和测试。\n在拥有 18 G 内存的 MacBook Pro 上，运行 LLaMA 3.2 1B 模型，速度非常快，几乎秒出结果；运行 LLaMA 3.2 3B 模型，速度也非常快；运行 LLaMA 3.1 8B 模型，速度非常快；而运行 LLaMA 3.1 70B 模型，速度很慢。",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Llama 3.2</span>"
    ]
  },
  {
    "objectID": "llama.html#任务性能评估",
    "href": "llama.html#任务性能评估",
    "title": "16  Llama 3.2",
    "section": "16.2 任务性能评估",
    "text": "16.2 任务性能评估\n\n16.2.1 启动 API 服务\nollama run llama3.1:8b --format=json\n\n\n16.2.2 读取文件\n\nimport pandas as pd\n\n# 读取CSV文件中的标题和摘要\nfile = \"example/wos-fast5k/savedrecs.txt\"\n    \n# 根据文件的分隔符修改 delimiter 参数\n# 设置 index_col=False 来确保第一列不会被当作行索引\ndata = pd.read_csv(file, delimiter='\\t', index_col=False)  \n\n# 删除全为 NaN 的列\ndata = data.loc[:, data.notna().sum() != 0]\n\n# 查看处理后的数据\nprint(data[0:5])\n\n  PT                                                 AU   BA   CA   GP  \\\n0  J  Just, Berta Singla; Marks, Evan Alexander Neth...  NaN  NaN  NaN   \n1  J  Pereira-Marques, Joana; Ferreira, Rui M.; Figu...  NaN  NaN  NaN   \n2  J  Golzar-Ahmadi, Mehdi; Bahaloo-Horeh, Nazanin; ...  NaN  NaN  NaN   \n3  J  Li, Chenglong; Han, Yanfeng; Zou, Xiao; Zhang,...  NaN  NaN  NaN   \n4  J  Hu, Yu; Wang, Yulin; Wang, Runhua; Wang, Xiaok...  NaN  NaN  NaN   \n\n                                                  TI  \\\n0  Biofertilization increases soil organic carbon...   \n1  A metatranscriptomics strategy for efficient c...   \n2  Pathway to industrial application of heterotro...   \n3  A systematic discussion and comparison of the ...   \n4  Dirammox-dominated microbial community for bio...   \n\n                                                  SO   VL   IS   BP  ...  \\\n0  INTERNATIONAL JOURNAL OF AGRICULTURAL SUSTAINA...   22    1  NaN  ...   \n1                                       GUT MICROBES   16    1  NaN  ...   \n2                             BIOTECHNOLOGY ADVANCES   77  NaN  NaN  ...   \n3                SYNTHETIC AND SYSTEMS BIOTECHNOLOGY    9    4  775  ...   \n4             APPLIED MICROBIOLOGY AND BIOTECHNOLOGY  108    1  NaN  ...   \n\n     PY                                                 AB  \\\n0  2024  Protecting and building soil carbon has become...   \n1  2024  The high background of host RNA poses a major ...   \n2  2024  The transition to renewable energies and elect...   \n3  2024  Synthetic microbial community has widely conce...   \n4  2024  Direct ammonia oxidation (Dirammox) might be o...   \n\n                                                  C1   CT   CY   SP   CL TC  \\\n0  [Just, Berta Singla; Llenas, Laia; Ponsa, Serg...  NaN  NaN  NaN  NaN  0   \n1  [Pereira-Marques, Joana; Ferreira, Rui M.; Fig...  NaN  NaN  NaN  NaN  2   \n2  [Golzar-Ahmadi, Mehdi; Norouzi, Forough; Holus...  NaN  NaN  NaN  NaN  0   \n3  [Li, Chenglong; Han, Yanfeng; Zou, Xiao; Zhang...  NaN  NaN  NaN  NaN  0   \n4  [Hu, Yu; Wang, Yulin; Liu, Shuang-Jiang] Shand...  NaN  NaN  NaN  NaN  0   \n\n                                                  WC                   UT  \n0  Agriculture, Multidisciplinary; Green & Sustai...  WOS:001247571700001  \n1        Gastroenterology & Hepatology; Microbiology  WOS:001176335800001  \n2               Biotechnology & Applied Microbiology  WOS:001316252000001  \n3               Biotechnology & Applied Microbiology  WOS:001273513000001  \n4               Biotechnology & Applied Microbiology  WOS:001252334300001  \n\n[5 rows x 24 columns]\n\n\n下面，将 dataframe 中的数据逐批处理，生成 JSON 格式的文本。\n\nbatch_to_json()：这是函数的主体，将数据按批次进行处理，并将每行数据转化为 JSON 格式。\n批次切分：使用 data[i:i+batch_size] 对 DataFrame 进行分批处理。\n构建 JSON：每行数据提取出 UT、TI、AB 列的值，并构建一个包含 id、ti 和 ab 键的 JSON 对象。\njson.dumps()：将列表形式的 JSON 对象转换为 JSON 字符串。\nensure_ascii=False：确保输出的 JSON 字符串支持非 ASCII 字符。\n\n\nimport pandas as pd\nimport json\nfrom IPython.display import Markdown\n\ndef batch_to_json(data, batch_size):\n    # 确保输入的 data 是一个 DataFrame，且 batch_size 是正整数\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"data 应该是一个 pandas DataFrame\")\n    if not isinstance(batch_size, int) or batch_size &lt;= 0:\n        raise ValueError(\"batch_size 应该是一个正整数\")\n    \n    # 提取 dataframe 中的每 batch_size 行数据\n    batches = [data[i:i+batch_size] for i in range(0, len(data), batch_size)]\n    \n    result = []\n    for batch in batches:\n        json_batch = []\n        for _, row in batch.iterrows():\n            # 构建每行的 JSON 对象\n            json_obj = {\n                \"ID\": row[\"UT\"],\n                \"TI\": row[\"TI\"],\n                \"AB\": row[\"AB\"]\n            }\n            json_batch.append(json_obj)\n        result.append(json.dumps(json_batch, ensure_ascii=False))  # 转换为 JSON 格式的字符串\n    return result\n\n# 示例使用\nbatch_size = 2\njson_output = batch_to_json(data[0:1], batch_size)\n\nfrom pprint import pprint\npprint(json_output)\n\n['[{\"ID\": \"WOS:001247571700001\", \"TI\": \"Biofertilization increases soil '\n 'organic carbon concentrations: results of a meta-analysis\", \"AB\": '\n '\"Protecting and building soil carbon has become a global policy priority, '\n 'and novel agronomic fertilization practices may contribute to soil '\n 'protection and climate-smart agriculture. The application of microbial '\n 'inoculants (biofertilizers) is considered beneficial for soil and climate '\n '-smart agriculture. Therefore, an exhaustive meta-analysis of '\n 'biofertilization studies was carried out worldwide to quantify the benefits '\n 'of microbial inoculants on SOC concentration. Based on 59 studies and 267 '\n 'observations, was found that biofertilizers significantly increased SOC '\n 'concentration by an average of 0.44 g C kg-1 soil. All biofertilizer types '\n 'were estimated to contribute positively to SOC (0.18-0.70 g C kg-1soil), but '\n 'only cyanobacteria, mixtures of organisms, mycorrhizal fungi, and nitrogen '\n 'fixers were statistically significant. In terms of crop type, results were '\n 'significant and positive for cereals, fruits, legumes and root/tuber crops '\n '(0.44-0.82 g C kg-1soil). A significant positive linear relationship was '\n 'observed between crop yield and SOC changes, supporting the notion that '\n 'greater productivity helps explain SOC increases, accounting for 7% of the '\n 'dataset variability. This study provides the first evidence from a global '\n 'assessment that biofertilizer use is associated with an augmented '\n 'terrestrial agricultural organic carbon sink contributing to soil protection '\n 'and food security where climate-smart solutions are sought.\"}]']\n\n\n\n\n16.2.3 调整提示词\nNote: 将提示词换成英文效果可能会更好。\nNote: JSON 格式要求键值对必须用双引号。\n\n# 使用三引号插入一个非常长的系统提示符\nsystem_prompt = \"\"\"  \nPlease identify the relevance of each article's research topic to **synthetic microbial community** research based on the following content, and provide a score (0 for completely irrelevant, 10 for highly relevant, 1-9 for partially relevant):\n\nPlease note:\n\n1. A synthetic microbial community refers to a composite of one or more microorganisms created by humans. It can be a probiotic, a biofertilizer, or a small microbial community. All research related to these topics should be considered relevant to synthetic microbial community research.\n2. The subsequent content will be provided in JSON format, including the article's ID, title (TI), and abstract (AB). Please make your judgment based on the title and abstract content, and record the ID for your response.\n3. Be sure to follow the response requirements below.\n\nResponse requirements:\n\n1. In your response, please provide the article's ID, score, and a brief reason.\n2. Please use the JSON format for your response, for example:\n\n{\"results\": [{\"ID\":\"string\", \"score\":number, \"reason\": \"string\"}]}\n\n\"\"\"\n\n要使 OpenAI API 调用的结果以统一的 JSON 格式返回，可以按照以下步骤进行设置：\n\nimport requests\nimport json\n\ndef analyze_content(content):\n  messages = [\n      {\"role\": \"system\", \"content\": system_prompt}\n  ]\n  \n  messages.append({\"role\": \"user\", \"content\": content})\n\n  response = requests.post(\n      \"http://localhost:11434/v1/chat/completions\",\n      json={\n          \"model\": \"llama3.1:8b\",\n          \"messages\": messages,\n          \"stream\": False\n      }\n  )\n  # 将API响应解析为JSON格式\n  response_data = response.json()\n  content_str = response_data['choices'][0]['message']['content']\n  # 从 content_str 中提取 JSON 格式的文本\n  json_start = content_str.find(\"{\")\n  json_end = content_str.rfind(\"}\") + 1\n  json_str = content_str[json_start:json_end]\n\n  try:\n      content_dict = json.loads(json_str)\n  except json.JSONDecodeError:\n      print(f\"警告: 返回的内容不包含有效的 JSON 数据。内容为: {content_str}\")\n      content_dict = {\"results\": []}\n  \n  # 如果含有 results 键，则返回 results 键对应的值\n  if 'results' in content_dict:\n      result = content_dict['results']\n  else:\n      print(f\"警告: 返回的JSON 内容不包含有效的 'results' 键。内容为: {json_str}\")\n      result = []\n\n  return result\n\ntest_output = analyze_content(json_output[0])\npprint(test_output)\n\n[{'ID': 'WOS:001247571700001',\n  'reason': 'The article discusses the benefits of biofertilization on soil '\n            'organic carbon concentrations and its contribution to '\n            'climate-smart agriculture, which is relevant to synthetic '\n            'microbial community research as it involves a specific type of '\n            'microbe (microbial inoculants) used for agricultural purposes.',\n  'score': 9}]\n\n\n\n\n16.2.4 批处理\n现在可以进行批处理，并将结果保存下来。\n\nbatches = batch_to_json(data[0:20], batch_size=1)\n\nresults = []\n\nimport time\n\nstart_time = time.time()\n\nfrom tqdm import tqdm\n\nfor batch in tqdm(batches):\n  result = analyze_content(batch)\n  results.append(result)\n\nend_time = time.time()\nexecution_time = end_time - start_time\nprint(f\"批处理执行时间: {execution_time:.2f} 秒\")\n\n批处理执行时间: 88.28 秒\n\n\n将列表合并成一个数据框，并保存到硬盘中供后续分析使用。\n\nimport json\n\ndata_list = []\n\nfor result in results:\n  # 如果 result 不是空列表\n  if result:\n    for item in result:\n      # 确保有 ID 键、score 键、reason 键\n      if 'ID' in item and 'score' in item and 'reason' in item:\n        data_list.append([item['ID'], item['score'], item['reason']])\n\ndf = pd.DataFrame(data_list, columns=['ID', 'score', 'reason'])\n\ndf\n# df.to_csv('output.csv', index=False)\n\n\n\n\n\n\n\n\nID\nscore\nreason\n\n\n\n\n0\nWOS:001247571700001\n8.0\nThe article discusses the use of biofertilizer...\n\n\n1\nWOS:001176335800001\n8.5\nThe article discusses a metatranscriptomics st...\n\n\n2\nWOS:001316252000001\n8.0\nThe article discusses using heterotrophic micr...\n\n\n3\nWOS:001273513000001\n10.0\nThe article's title directly mentions 'synthet...\n\n\n4\nWOS:001252334300001\n8.0\nThe study involves creating a synthetic microb...\n\n\n5\nWOS:001209713100001\n8.0\nThe article discusses microalgal biofuels, lip...\n\n\n6\nWOS:001198552100002\n9.0\nThe article discusses various methods to study...\n\n\n7\nWOS:001171546000001\n10.0\nThe article discusses the construction of a sy...\n\n\n8\nWOS:001314268600001\n10.0\nThe article explores the use of synthetic micr...\n\n\n9\nWOS:001313702300001\n0.0\nThis article's research topic is related to th...\n\n\n10\nWOS:001320306700001\n5.0\nThe research topic focuses on using microalgae...\n\n\n11\nWOS:001319961700001\n10.0\nThe research topic directly relates to synthet...\n\n\n12\nWOS:001318177900001\n10.0\nThe research topic is directly related to synt...\n\n\n13\nWOS:001318328600001\n6.0\nWhile this article is about textile printing w...\n\n\n14\nWOS:001317596600001\n4.0\nThe article discusses cationic antimicrobial p...\n\n\n15\nWOS:001291429900001\n10.0\nThis article investigates the effects of anaer...\n\n\n16\nWOS:001319156200001\n6.0\nThe article discusses a nitrogen-fixing cyanob...\n\n\n17\nWOS:001307986400001\n9.0\nThe article discusses synthetic microbial comm...\n\n\n18\nWOS:001319855200001\n8.0\nThe article discusses the use of Paenibacillus...\n\n\n19\nWOS:001314605300007\n8.0\nThe article discusses the role of acetic acid ...",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Llama 3.2</span>"
    ]
  },
  {
    "objectID": "batch-async-task.html",
    "href": "batch-async-task.html",
    "title": "17  批量异步任务",
    "section": "",
    "text": "17.1 跟踪进度\n在 R 中，apply 函数本身并不提供进度条的功能，但可以结合 pbapply 包来实现进度显示。pbapply 是 apply 家族函数的增强版，支持显示进度条。",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>批量异步任务</span>"
    ]
  },
  {
    "objectID": "batch-async-task.html#跟踪进度",
    "href": "batch-async-task.html#跟踪进度",
    "title": "17  批量异步任务",
    "section": "",
    "text": "17.1.1 安装和使用 pbapply 包\n\n安装 pbapply 包：\ninstall.packages(\"pbapply\")\n使用 pbapply::pblapply() 代替 apply()：\npbapply 包中的函数与 apply 系列函数接口相同，只是多了进度条显示功能。以下是一个示例，展示如何使用 pblapply() 处理每一行并显示进度：\n\nlibrary(pbapply)\n\n# 创建一个示例 data.frame\ndf = data.frame(a = 1:1000, b = rnorm(1000))\n\n# 使用 pblapply 显示 apply 的进度\nresult = pbapply(df, 1, function(row) {\n  Sys.sleep(0.01)  # 模拟一些需要时间的计算\n  sum(row)         # 计算每一行的和\n})\n\n\n\n\n17.1.2 pbapply 包中的其他常用函数：\n\npblapply()：用于 lapply 类的操作。\npbapply()：用于 apply 类操作（与 apply() 类似）。\npbmclapply()：用于并行化 lapply 操作（仅适用于 UNIX-like 系统）。\n\n通过这种方法，你可以方便地在 R 中显示 apply 执行的进度，尤其适用于处理大型数据集时跟踪计算进度。",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>批量异步任务</span>"
    ]
  },
  {
    "objectID": "batch-async-task.html#缓存",
    "href": "batch-async-task.html#缓存",
    "title": "17  批量异步任务",
    "section": "17.2 缓存",
    "text": "17.2 缓存\n为了自动缓存 pbapply 的执行进度，你可以使用 memoise 包，它能将函数的执行结果进行缓存，避免重复计算。它可以缓存到内容、本地文件系统、Google cloud storage、AWS cloud storage 等。结合 pbapply，你可以将每次处理的结果缓存起来，以便下次调用时可以直接使用缓存的数据。\n\n17.2.1 安装和使用 memoise 包\n\n安装 memoise 包：\ninstall.packages(\"memoise\")\n将处理函数缓存起来：\n使用 memoise::memoise() 对函数进行缓存处理。这样，pbapply 在每次执行时，都会检查是否已经缓存过该结果。\n\nlibrary(pbapply)\nlibrary(memoise)\n\n# 示例：对每一行的计算进行缓存\nmy_function = function(row) {\n  Sys.sleep(0.01)  # 模拟一些耗时操作\n  sum(row)         # 简单计算每行的和\n}\n\n# 对函数进行缓存\ncached_function = memoise(my_function)\n\n定义好了缓存函数后，就可以使用 pbapply 并结合缓存来处理数据了。在第一次运行时，函数会执行并缓存结果。在后续的运行中，如果输入相同，函数会直接从缓存中返回结果，从而提高效率。\n下面是第一次运行的结果：\n\n# 使用 pbapply 并结合缓存\nset.seed(123)\ndf = data.frame(a = 1:1000, b = rnorm(1000))\nsystem.time(result &lt;- pbapply(df, 1, cached_function))\n\n   user  system elapsed \n  0.318   0.032  12.187 \n\n\n再次运行同样的代码，结果会从缓存中直接返回：\n\nset.seed(123)\ndf = data.frame(a = 1:1000, b = rnorm(1000))\nsystem.time(result &lt;- pbapply(df, 1, cached_function))\n\n   user  system elapsed \n  0.019   0.001   0.019 \n\n\n第二次运行时，由于结果已经缓存，所以速度更快。不过，需要注意的是，缓存只对同一个函数有效，如果函数定义不同，缓存不会生效。重新定义缓存函数后，哪怕函数的内部逻辑完全相同，缓存也会失效。\n\n\n\n17.2.2 工作原理：\n\nmemoise 将函数 my_function 的结果缓存起来。当 pbapply 再次调用该函数处理相同的输入时，系统会直接从缓存中读取结果，而不是重新执行。\n对于每一行处理的结果，都会被缓存。即使你多次运行相同的操作，已经计算过的部分会从缓存中直接加载，大幅提高效率。\n\n\n\n17.2.3 可选的存储方式：\nmemoise 还支持不同的存储方式（例如文件系统、数据库、内存等），你可以将缓存的数据存储在持久化的存储中，这样即使 R 会话关闭，也能保留缓存结果：\n# 使用本地文件系统缓存\ncached_function = memoise(my_function, cache = cache_filesystem(\"cache\"))\n这样可以在未来的会话中继续使用缓存，提高计算的效率并避免重复执行相同的任务。\n\n\n17.2.4 自定义缓存键\n在 memoise 中，缓存的唯一键是基于输入参数的值自动生成的，使用的是参数的哈希值。这意味着当传递给函数的参数内容相同时，memoise 会识别并使用已经缓存的结果，而无需重复计算。\n然而，有时你可能希望更明确地控制如何生成这些缓存键。你可以通过以下方式确保缓存键的唯一性：\n\n17.2.4.1 默认行为（自动基于参数生成键）\nmemoise 会默认根据函数的输入参数内容来生成哈希值。例如：\nlibrary(memoise)\n\n# 定义一个简单的函数\nmy_function = function(x) {\n  Sys.sleep(1)  # 模拟计算延迟\n  x^2           # 返回平方\n}\n\n# 缓存该函数\ncached_function = memoise(my_function)\n\n# 调用函数，memoise 将自动根据 x 的值生成缓存\ncached_function(10)  # 第一次调用，会执行\ncached_function(10)  # 第二次调用，会从缓存中返回结果\n在上面的例子中，memoise 会根据 x=10 生成哈希键，下次调用相同输入时会直接返回缓存结果。\n\n\n17.2.4.2 自定义键生成方法\n如果你需要自定义键的生成方式，可以使用 digest 包来手动生成缓存键。通过将输入参数转换成哈希值作为缓存键来精确控制缓存行为。\n例如：\n\nlibrary(memoise)\nlibrary(digest)\n\n# 自定义的函数，手动生成缓存键\nmy_custom_memoise = function(f) {\n  mem_f = memoise(function(...) {\n    # 使用 digest 手动生成唯一键，基于输入参数的哈希值\n    cache_key = digest(list(...))\n    message(\"Cache key: \", cache_key)\n    f(...)\n  })\n  return(mem_f)\n}\n\n# 定义一个函数\nmy_function = function(x) {\n  Sys.sleep(1)\n  x^2\n}\n\n# 将函数缓存，并自定义缓存键生成\ncached_function = my_custom_memoise(my_function)\n\n# 调用函数时会生成并输出自定义的缓存键\ncached_function(10)\n\n[1] 100\n\ncached_function(10)\n\n[1] 100\n\ncached_function(20)  # 对于不同的输入，缓存键会不同\n\n[1] 400\n\n\n\n\n\n17.2.5 保存缓存到 sqlite 数据库\n使用 cache_filesystem 缓存时，缓存数据会保存在文件系统中，会涉及比较多的磁盘读写运算，当处理量较大时，会严重影响性能。而且每一次函数调用都会生成一个文件，管理起来也不方便。因此，这里扩展了一个 cache_dbi() 函数，使用数据库来缓存数据。\n\n#' SQLite Database Cache\n#'\n#' Use a cache in a SQLite database that will persist between R sessions.\n#'\n#' @param con A DBI connection object to a SQLite database.\n#' @param table_name Name of the table to use for caching. Default: \"cache\".\n#'\n#' @examples\n#'\n#' \\dontrun{\n#' library(DBI)\n#' library(RSQLite)\n#' \n#' # 创建 SQLite 数据库连接\n#' con &lt;- dbConnect(SQLite(), dbname = \"cache.sqlite\")\n#' \n#' # 使用 SQLite 缓存\n#' db_cache &lt;- cache_dbi(con)\n#' \n#' mem_runif &lt;- memoise(runif, cache = db_cache)\n#' }\n#'\n#' @export\n#' @inheritParams cache_memory\ncache_dbi &lt;- function(con, algo = \"xxhash64\", table_name = \"cache\") {\n  if (!(requireNamespace(\"digest\"))) { stop(\"Package `digest` must be installed for `cache_dbi()`.\") }\n  if (!(requireNamespace(\"DBI\"))) { stop(\"Package `DBI` must be installed for `cache_dbi()`.\") }\n\n  if (!grepl(\"^[A-Za-z0-9_]+$\", table_name)) {\n    stop(\"Invalid table name\")\n  }\n\n  # 创建缓存表\n  DBI::dbExecute(con, sprintf(\"CREATE TABLE IF NOT EXISTS %s (key TEXT PRIMARY KEY, value BLOB)\", table_name))\n\n  # 重置缓存\n  cache_reset &lt;- function() {\n    DBI::dbExecute(con, sprintf(\"DELETE FROM %s\", table_name))\n  }\n\n  # 设置缓存\n  cache_set &lt;- function(key, value) {\n    serialized_value &lt;- serialize(value, NULL)  # 序列化 R 对象\n    DBI::dbExecute(con, sprintf(\"INSERT OR REPLACE INTO %s (key, value) VALUES (?, ?)\", table_name),\n                   params = list(key, list(serialized_value)))  # 用 list 包裹 serialized_value\n  }\n\n  # 获取缓存\n  cache_get &lt;- function(key) {\n    result &lt;- DBI::dbGetQuery(con, sprintf(\"SELECT value FROM %s WHERE key = ?\", table_name),\n                              params = list(key))\n    if (nrow(result) == 0) {\n      return(NULL)\n    }\n    unserialize(result$value[[1]])  # 反序列化第一个值\n  }\n\n  # 检查缓存是否存在\n  cache_has_key &lt;- function(key) {\n    result &lt;- DBI::dbGetQuery(con, sprintf(\"SELECT 1 FROM %s WHERE key = ? LIMIT 1\", table_name),\n                              params = list(key))\n    nrow(result) &gt; 0\n  }\n\n  # 删除缓存\n  cache_drop_key &lt;- function(key) {\n    DBI::dbExecute(con, sprintf(\"DELETE FROM %s WHERE key = ?\", table_name),\n                   params = list(key))\n  }\n\n  list(\n    digest = function(...) digest::digest(..., algo = algo),\n    reset = cache_reset,\n    set = cache_set,\n    get = cache_get,\n    has_key = cache_has_key,\n    drop_key = cache_drop_key,\n    keys = function() DBI::dbGetQuery(con, sprintf(\"SELECT key FROM %s\", table_name))$key\n  )\n}\n\n下面，使用 cache_dbi() 函数来缓存数据：\n\nlibrary(memoise)\nlibrary(DBI)\nlibrary(RSQLite)\n\n# 创建 SQLite 数据库连接\ncon = dbConnect(SQLite(), dbname = tempfile())\n# on.exit(dbDisconnect(con))\n\n# 将缓存数据存储在数据库中\ncached_function = memoise(my_function, cache = cache_dbi(con))\n\n# 第一次运行\nsystem.time(cached_function(10))\n\n   user  system elapsed \n  0.002   0.001   1.008 \n\n# 第二次运行\nsystem.time(cached_function(10))\n\n   user  system elapsed \n  0.008   0.000   0.007 \n\n# 第三次运行\nsystem.time(cached_function(20))\n\n   user  system elapsed \n  0.002   0.002   1.009",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>批量异步任务</span>"
    ]
  },
  {
    "objectID": "batch-async-task.html#动态控制批次执行",
    "href": "batch-async-task.html#动态控制批次执行",
    "title": "17  批量异步任务",
    "section": "17.3 动态控制批次执行",
    "text": "17.3 动态控制批次执行\n为了动态控制批次的执行，并确保正在执行的运算不超过 batch_size，你可以使用 R 的异步编程或并发工具包，比如 parallel 和 future，结合 Sys.sleep() 控制批次提交的速率。\n\n17.3.1 使用 parallel 包实现动态分批\n以下是使用 parallel 包执行 batch_size 个并发操作的示例：\n\nlibrary(parallel)\n\n# 示例数据\ndf = data.frame(a = 1:100, b = rnorm(100))\n\n# 定义处理函数\nprocess_function = function(row) {\n  Sys.sleep(runif(1, 0.1, 0.5))  # 模拟耗时任务\n  sum(row)      # 返回每行的和\n}\n\n# 比较串行和并行处理的性能\n\n# 串行处理\nstart_time_serial = Sys.time()\nresults_serial = lapply(1:nrow(df), function(i) {\n  process_function(df[i, ])\n})\nend_time_serial = Sys.time()\ntime_serial = difftime(end_time_serial, start_time_serial, units = \"secs\")\n\n# 并行处理\nstart_time_parallel = Sys.time()\nresults_parallel = mclapply(1:nrow(df), function(i) {\n  process_function(df[i, ])\n}, mc.cores = parallel::detectCores() - 1)  # 使用可用核心数减1\nend_time_parallel = Sys.time()\ntime_parallel = difftime(end_time_parallel, start_time_parallel, units = \"secs\")\n\n# 计算性能提升\nspeedup = as.numeric(time_serial) / as.numeric(time_parallel)\n\n# 输出结果\ncat(\"串行处理时间：\", time_serial, \"秒\\n\")\ncat(\"并行处理时间：\", time_parallel, \"秒\\n\")\ncat(\"性能提升：\", round(speedup, 2), \"倍\\n\")\n\n# 验证结果正确性\ntestthat::expect_equal(rowSums(df), unlist(results_serial))\ntestthat::expect_equal(rowSums(df), unlist(results_parallel))\n\n\n\n17.3.2 使用 future 包实现动态分批\nfuture 包允许你异步地运行函数，并通过限制同时进行的任务数量来动态控制批次。通过监控并发执行的数量，如果低于 batch_size，则自动启动新的任务，否则等待 1 秒再检查。\n\n\n17.3.3 代码示例\n\nlibrary(future)\nlibrary(future.apply)\n\n# 设定最大并行执行任务数为 batch_size\nbatch_size = 10\n\n# 使用多线程计划\nplan(multisession, workers = batch_size)\n\n# 示例数据\ndf = data.frame(a = 1:100, b = rnorm(100))\n\n# 定义处理函数\nprocess_function = function(row) {\n  Sys.sleep(runif(1, 0.1, 0.5))  # 模拟耗时任务\n  sum(row)      # 返回每行的和\n}\n\n# 动态控制批次执行\nresults = list()  # 用于存储结果\nfutures = list()  # 用于存储每个 future\n\n# 将每行分成任务提交\nfor (i in 1:nrow(df)) {\n  # 检查当前运行中的任务数量\n  current_tasks = futures[!sapply(futures, resolved)]\n\n  # 如果当前运行中的任务数量达到 batch_size，等待 1 秒\n  while (length(current_tasks) &gt;= batch_size) {\n    # 打印当前运行中的任务数量\n    message(\"当前运行中的任务数量：\", length(current_tasks))\n    # 等待 1 秒\n    Sys.sleep(1)\n    # 更新当前运行中的任务\n    current_tasks = futures[!sapply(futures, resolved)]\n  }\n  \n  # 提交新的任务，并保持追踪\n  futures[[i]] = future({\n    process_function(df[i, ])\n  }, seed = TRUE)\n\n}\n\n# 收集所有结果\nresults = future::value(futures) |&gt; unlist()\n\n# 展示结果\ntestthat::expect_equal(rowSums(df), results)\n\n\n\n17.3.4 代码说明\n\nfuture 和 future.apply：future 包允许将任务异步提交，通过 plan(multisession, workers = batch_size) 限制同时运行的任务数。\n动态任务调度：\n\n每次新任务提交前，检查当前运行中的任务数量（length(futures)）。\n如果运行中的任务达到 batch_size，程序会等待 1 秒，再次检查任务数量，确保不超出最大并行数。\n当低于 batch_size 时，新的任务才会被提交。\n\n结果收集：使用 future::value(futures) 来获取所有任务的计算结果。\n错误处理：使用 tryCatch 捕获错误，并输出错误信息。\n\n通过这种方法，你可以确保批次执行数不会超过 batch_size，并且动态提交新任务，从而实现更灵活的任务管理。",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>批量异步任务</span>"
    ]
  },
  {
    "objectID": "batch-async-task.html#包装成一个函数",
    "href": "batch-async-task.html#包装成一个函数",
    "title": "17  批量异步任务",
    "section": "17.4 包装成一个函数",
    "text": "17.4 包装成一个函数\n\n17.4.1 函数定义\n函数采用 pbapply 显示进度，batch_size 设定批次大小，fun 接受命令（如果这个命令是一个 memorise 函数则直接使用，如果不是则将其自动缓存），缓存默认使用 cache_dbi()，缓存文件地址为 db_file 文件，由用户提供。\n\nrun_task_with_cache &lt;- function(df, fun, db_file) {\n\n  # 检查并创建缓存\n  con = DBI::dbConnect(RSQLite::SQLite(), dbname = db_file)\n  on.exit(DBI::dbDisconnect(con))\n  \n  # 如果fun是memoise函数，则提出 warning，并使用fun \n  if (inherits(fun, \"memoised\")) {\n    warning(\"Fun is already a memoised function. It will be used directly. `db_file` will be ignored.\")\n  } else {\n    # 如果fun不是memoise函数，则将其缓存\n    fun &lt;- memoise::memoise(fun, cache = cache_dbi(con))\n  }\n  \n  # 执行计算\n  results = pbapply::pblapply(1:nrow(df), function(i) {\n    row = df[i, ]\n    result = tryCatch(fun(row), error = function(e) {\n        message(\"错误发生在第 \", i, \" 行: \", e$message)\n        return(NA)\n      })\n    return(result)\n  })\n  \n  return(results)\n}\n\n\n# 示例数据\nset.seed(123456)\ndf = data.frame(a = 1:100, b = rnorm(100))\n\n# 定义处理函数\nprocess_function = function(row) {\n  Sys.sleep(0.1)  # 模拟耗时任务\n  sum(row)      # 返回每行的和\n}\n\n\n# 运行函数\nresult &lt;- run_task_with_cache(df, process_function, db_file = \"cache.sqlite\")\n\ntestthat::expect_equal(unlist(result), rowSums(df))\n\n\n\n17.4.2 并行计算\n并行计算可以使用 future 包，或者 parallel 包。但是在显示进度条、数据库访问方面实现起来有一定难度，特别是涉及到数据库访问的并发问题，暂时没有很好的解决方案。因此，考虑使用数据拆分的方法，拆成多个批次，每个批次使用一个进程，每个进程使用一个数据库连接，这样就可以避免并发问题。",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>批量异步任务</span>"
    ]
  },
  {
    "objectID": "batch-async-task.html#进度条",
    "href": "batch-async-task.html#进度条",
    "title": "17  批量异步任务",
    "section": "17.5 进度条",
    "text": "17.5 进度条\n在 R 中，可以使用多个包来显示现代化的进度条，其中最常用的包包括：\n\nprogressr：一个强大的进度条包，可以显示并行任务的进度条。\nprogress：一个简单的、可高度自定义的进度条包。\ncli：可以结合进度条显示，也可以显示丰富的命令行界面元素。\npbapply：适用于 *apply 系列函数的进度条集成。\n\n以下是这些包的基本用法：\n\n使用 progressr 包\n\nprogressr 包可以用来显示并行任务的进度条。以下是一个使用 future 和 furrr 包进行并行计算，并使用 progressr 显示进度的示例：\n\nlibrary(future)\nlibrary(furrr)\nlibrary(progressr)\n\nplan(multisession, workers = 4)\n\ndata &lt;- 1:100\n\ntime_consuming_task &lt;- function(x) {\n  Sys.sleep(0.01)\n  return(x^2)\n}\n\nwith_progress({\n  p &lt;- progressor(steps = length(data))\n  \n  results &lt;- future_map(data, function(x) {\n    result &lt;- time_consuming_task(x)\n    p()\n    return(result)\n  })\n})\n\n\n使用 progress 包\n\nprogress 包提供了简单易用的进度条，同时可以进行高度自定义。\n\n# 安装并加载 progress 包\n# install.packages(\"progress\")\nlibrary(progress)\n\n# 创建一个进度条\npb = progress_bar$new(\n  format = \"  downloading [:bar] :percent in :elapsed\",\n  total = 100, clear = FALSE, width = 60\n)\n\n# 模拟一个任务并更新进度条\nfor (i in 1:100) {\n  Sys.sleep(0.01)  # 模拟耗时任务\n  pb$tick()  # 每次循环时，进度条前进一格\n}\n\n\n使用 cli 包\n\ncli 包不仅提供进度条，还可以创建富文本命令行界面。\n\n# 安装并加载 cli 包\n# install.packages(\"cli\")\nlibrary(cli)\n\n# 创建并显示进度条\npb = cli_progress_bar(\"Computing\", \n                      total = 100, \n                      format = \" {cli::pb_bar} {cli::pb_percent} ETA: {cli::pb_eta}\")\n\nfor (i in 1:100) {\n  Sys.sleep(0.05)  # 模拟任务\n  cli_progress_update()  # 更新进度条\n}\n\nmessage(\"Done!\")\ncli_progress_done()  # 任务完成时清除进度条\n\n\n使用 pbapply 包\n\n如果你经常使用 apply 系列函数，pbapply 包可以在它们的基础上添加进度条。\n\n# 安装并加载 pbapply 包\n#install.packages(\"pbapply\")\nlibrary(pbapply)\n\n# 使用 pbapply 中的 pbsapply 代替 sapply，自动添加进度条\nresult = pbsapply(1:100, function(x) {\n  Sys.sleep(0.01)  # 模拟任务\n  x^2\n})\n\n这几个包都提供了现代化的进度条显示，你可以根据自己的需求选择最合适的。",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>批量异步任务</span>"
    ]
  },
  {
    "objectID": "mongoDB.html",
    "href": "mongoDB.html",
    "title": "18  MongoDB 数据库",
    "section": "",
    "text": "18.1 选择 MongoDB 的理由",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>MongoDB 数据库</span>"
    ]
  },
  {
    "objectID": "mongoDB.html#选择-mongodb-的理由",
    "href": "mongoDB.html#选择-mongodb-的理由",
    "title": "18  MongoDB 数据库",
    "section": "",
    "text": "18.1.1 可选的数据库架构\n对于缓存大模型的响应，推荐使用以下几种数据库，具体选择取决于对缓存效率、持久性、以及查询能力的需求：\n\n18.1.1.1 Redis\nRedis 是一个内存数据库，速度快，适合实时缓存大模型的响应，尤其是在对数据进行频繁读写操作时。可以通过设置过期时间来自动清除旧数据，适合需要高吞吐和低延迟的场景。\n优势：\n\n高速缓存，响应时间极快。\n支持自动过期策略，有助于管理缓存空间。\n可以使用 RedisJSON 插件来更好地支持 JSON 数据结构。\n\n\n\n18.1.1.2 Memcached\nMemcached 是一种高性能的分布式内存缓存系统，适用于纯缓存用途。和 Redis 类似，它可以快速存取数据，但不支持持久化存储，适合在系统重启后可以清空缓存的场景。\n优势：\n\n极快的读写速度。\n占用内存小，适合简单的缓存场景。\n适用于缓存结构简单且无需持久化的响应。\n\n\n\n18.1.1.3 MongoDB\n如果希望缓存数据并在应用重启后保持响应数据持久化，MongoDB 是一个选择。它对 JSON 数据支持良好，且能持久化存储。适合那些不仅需要缓存，还希望在缓存中做简单数据分析和检索的场景。\n优势：\n\nJSON 存储和持久化能力。\n支持灵活查询和索引，适合做更复杂的检索。\n\n\n\n18.1.1.4 PostgreSQL\nPostgreSQL 也支持 JSON 数据格式，并且提供良好的查询和索引能力。如果需要持久性和较复杂的数据操作，也可以选择 PostgreSQL，但速度会稍逊于内存数据库。\n\n\n\n18.1.2 MongoDB 的并行查询能力\nMongoDB 的并行查询能力较强，得益于其架构设计和内置的并行处理机制。以下是 MongoDB 并行查询能力的几个主要方面：\n\n18.1.2.1 多线程处理\nMongoDB 是多线程的，支持同时处理多个查询请求。当并发请求增加时，MongoDB 会自动为每个请求分配独立的线程，并行执行不同的查询，提升了处理效率。\n\n\n18.1.2.2 分片（Sharding）支持\n对于大规模的数据集，MongoDB 提供了分片功能，将数据分散到多个服务器（分片）上。每个分片可以独立处理查询请求，从而显著提高查询性能。查询请求会并行分发到不同的分片上，减少了单一节点的压力，提升了系统的可扩展性和查询效率。\n\n\n18.1.2.3 索引优化\nMongoDB 支持多种索引，包括单字段索引、复合索引、地理空间索引、全文索引等。合适的索引能够极大地优化查询性能，使查询操作更快地锁定目标数据。MongoDB 的并行查询在有适当索引的情况下表现会更好，因为索引能降低每个查询的处理时间，允许系统更快地响应并发请求。\n\n\n18.1.2.4 聚合管道的并行处理\nMongoDB 的聚合框架能够高效地并行处理复杂的分析查询。聚合管道的每个阶段可以由不同的线程执行，并发处理数据流。尤其在分片集群中，MongoDB 会在各个分片上并行执行聚合查询，最后合并结果，显著提升查询速度。\n\n\n18.1.2.5 内存与锁管理优化\nMongoDB 使用锁的粒度较小，最新版本（4.2 及更高）支持文档级锁，而不是数据库或集合级锁，这减少了并发查询时的锁争用。此外，MongoDB 会将常用数据缓存到内存中，并支持对内存进行并行管理，从而提升查询速度。\n\n\n18.1.2.6 写入与查询分离\n在主从（Primary-Secondary）架构中，读写请求可以分离，写入操作只在主节点上进行，查询可以在从节点上执行。通过将查询请求分散到不同的从节点，可以提升并行查询能力。",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>MongoDB 数据库</span>"
    ]
  },
  {
    "objectID": "mongoDB.html#安装-mongodb",
    "href": "mongoDB.html#安装-mongodb",
    "title": "18  MongoDB 数据库",
    "section": "18.2 安装 MongoDB",
    "text": "18.2 安装 MongoDB\n在 macOS 中部署 MongoDB 可以通过 Homebrew 安装并配置以快速启动服务。\n\n18.2.1 使用 Homebrew 安装 MongoDB\n首先，确保 macOS 上安装了 Homebrew。执行以下命令来安装 MongoDB 社区版：\nbrew tap mongodb/brew\nbrew install mongodb-community\n\n\n18.2.2 启动 MongoDB 服务\n安装完成后，可以通过以下命令启动 MongoDB 服务：\nbrew services start mongodb/brew/mongodb-community\n这将 MongoDB 作为服务启动，默认会监听本地的 27017 端口。\n\n\n18.2.3 验证 MongoDB 是否启动成功\n可以使用以下命令检查 MongoDB 是否正在运行：\nbrew services list\n你还可以直接使用 MongoDB Shell (mongosh) 连接到本地的 MongoDB 实例：\nmongosh\n如果成功连接，说明 MongoDB 已正常运行。\n\n\n18.2.4 停止 MongoDB 服务\n如果需要停止 MongoDB 服务，可以执行以下命令：\nbrew services stop mongodb/brew/mongodb-community\n\n\n18.2.5 配置 MongoDB 以支持远程连接（可选）\n默认情况下，MongoDB 只允许本地连接。如果你想配置 MongoDB 允许远程连接，可以修改 MongoDB 的配置文件：\n\n打开 MongoDB 配置文件：\nnano /opt/homebrew/etc/mongod.conf\n找到 bindIp 字段，将其从 127.0.0.1 修改为 0.0.0.0：\nnet:\n  bindIp: 0.0.0.0\n  port: 27017\n保存文件并重新启动 MongoDB 服务：\nbrew services restart mongodb/brew/mongodb-community\n\n\n注意：开启远程连接可能会带来安全风险，建议在生产环境中配置防火墙或限制 IP 访问。\n\n以上步骤完成后，MongoDB 已成功在 macOS 上安装并部署。",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>MongoDB 数据库</span>"
    ]
  },
  {
    "objectID": "mongoDB.html#在阿里云配置-mongodb",
    "href": "mongoDB.html#在阿里云配置-mongodb",
    "title": "18  MongoDB 数据库",
    "section": "18.3 在阿里云配置 MongoDB",
    "text": "18.3 在阿里云配置 MongoDB\n在阿里云 Ubuntu 服务器上安装 MongoDB 并对外提供服务的步骤如下：\n\n18.3.1 安装 MongoDB\nMongoDB 提供了官方的 APT 仓库，可以直接通过包管理器安装最新版本。\n# 更新本地包索引\nsudo apt update\n\n# 安装依赖包\nsudo apt install -y gnupg\n\n# 添加 MongoDB 官方的公钥\nwget -qO - https://www.mongodb.org/static/pgp/server-6.0.asc | sudo apt-key add -\n\n# 创建 MongoDB 的 APT 源文件\necho \"deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu focal/mongodb-org/6.0 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-6.0.list\n\n# 更新本地包索引\nsudo apt update\n\n# 安装 MongoDB\nsudo apt install -y mongodb-org\n\n\n18.3.2 启动 MongoDB 服务\nsudo systemctl start mongod\nsudo systemctl enable mongod\n\n\n18.3.3 配置 MongoDB 对外服务\n默认情况下，MongoDB 只监听本地接口 127.0.0.1。为了让 MongoDB 对外提供服务，需要修改其配置文件，监听所有网络接口。\n# 编辑 MongoDB 配置文件\nsudo nano /etc/mongod.conf\n找到 bindIp 字段，将其从 127.0.0.1 修改为 0.0.0.0：\n# /etc/mongod.conf\nnet:\n  port: 27017\n  bindIp: 0.0.0.0\n保存文件并重启 MongoDB 服务：\nsudo systemctl restart mongod\n\n\n18.3.4 配置防火墙和安全组\n\n18.3.4.1 阿里云安全组配置\n\n登录到阿里云控制台。\n进入 云服务器 ECS，选择目标实例。\n进入 安全组 配置，添加一条规则：\n\n端口范围：27017\n授权对象：设置为允许的 IP（如 0.0.0.0/0 表示允许所有 IP 访问），但建议仅对可信的 IP 开放。\n\n\n\n\n18.3.4.2 配置 UFW 防火墙（可选）\n若启用了 UFW 防火墙，需要添加 MongoDB 端口允许规则：\nsudo ufw allow 27017\n\n\n\n18.3.5 创建用户并启用认证（可选）\n为了提高安全性，可以为 MongoDB 创建用户并启用身份验证。\n# 进入 MongoDB shell\nmongosh\n\n# 切换到 admin 数据库\nuse admin\n\n# 创建管理员用户\ndb.createUser({\n  user: \"admin\",\n  pwd: \"yourpassword\",\n  roles: [{ role: \"root\", db: \"admin\" }]\n})\n在 /etc/mongod.conf 文件中启用身份验证，将 security 设置如下：\nsecurity:\n  authorization: \"enabled\"\n重启 MongoDB：\nsudo systemctl restart mongod\n连接时需要提供用户名和密码：\nmongosh -u admin -p yourpassword --authenticationDatabase admin\n完成以上步骤后，MongoDB 即可在阿里云 Ubuntu 服务器上对外提供服务。",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>MongoDB 数据库</span>"
    ]
  },
  {
    "objectID": "mongoDB.html#在-python-中使用-mongodb",
    "href": "mongoDB.html#在-python-中使用-mongodb",
    "title": "18  MongoDB 数据库",
    "section": "18.4 在 Python 中使用 MongoDB",
    "text": "18.4 在 Python 中使用 MongoDB\n以下是一个在 MongoDB 中存储和检索大模型响应的示例，假设你要存储模型输入和相应的输出，并支持简单的缓存逻辑（比如通过时间戳自动清理旧响应）。\n\n18.4.1 连接 MongoDB\n\nfrom pymongo import MongoClient\nfrom datetime import datetime, timedelta\n\n# 连接到 MongoDB\nclient = MongoClient(\"mongodb://localhost:27017/\")\ndb = client[\"model_cache_db\"]\ncollection = db[\"responses\"]\n\n\n\n18.4.2 存储响应\n我们可以定义一个函数，将模型的输入和响应输出存储到 MongoDB 中。可以选择存储一个过期时间戳，用于后续自动清理。\n\ndef cache_response(input_data, response_data, ttl_hours=24):\n    # 创建记录\n    document = {\n        \"input\": input_data,\n        \"response\": response_data,\n        \"timestamp\": datetime.utcnow(),\n        \"expiry\": datetime.utcnow() + timedelta(hours=ttl_hours)  # 设置过期时间\n    }\n    # 插入到 MongoDB\n    collection.insert_one(document)\n\n\n\n18.4.3 检索缓存响应\n查询 MongoDB 时，可以通过输入数据进行查找，并筛选出未过期的响应。\n\ndef get_cached_response(input_data):\n    document = collection.find_one({\n        \"input\": input_data,\n        \"expiry\": {\"$gt\": datetime.utcnow()}  # 确保未过期\n    })\n    if document:\n        return document[\"response\"]\n    return None  # 若未找到，返回 None\n\n\n\n18.4.4 自动清理过期缓存\n可以定期执行清理，将已过期的记录删除：\n\ndef clean_expired_cache():\n    result = collection.delete_many({\"expiry\": {\"$lt\": datetime.utcnow()}})\n    print(f\"Deleted {result.deleted_count} expired cache entries.\")\n\n\n\n18.4.5 示例使用\n\ninput_data = {\"question\": \"What is AI?\"}\nresponse_data = {\"answer\": \"AI stands for Artificial Intelligence.\"}\n\n# 缓存模型响应\ncache_response(input_data, response_data)\n\n# 查询缓存\ncached_response = get_cached_response(input_data)\nif cached_response:\n    print(\"Cache hit:\", cached_response)\nelse:\n    print(\"Cache miss\")\n\nCache hit: {'answer': 'AI stands for Artificial Intelligence.'}\n\n# 清理过期缓存\nclean_expired_cache()\n\nDeleted 0 expired cache entries.\n\n\n\n\n18.4.6 注意\n\nttl_hours 可以根据需求设置。\n可以定期调用 clean_expired_cache 来清理过期数据。",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>MongoDB 数据库</span>"
    ]
  },
  {
    "objectID": "mongoDB.html#在-r-中使用-mongodb",
    "href": "mongoDB.html#在-r-中使用-mongodb",
    "title": "18  MongoDB 数据库",
    "section": "18.5 在 R 中使用 MongoDB",
    "text": "18.5 在 R 中使用 MongoDB\n在 R 中使用 MongoDB，可以借助 mongolite 包，该包提供了对 MongoDB 的直接连接和操作接口，并支持数据的插入、查询、更新、删除等功能。以下是一个基本的示例，展示如何在 R 中连接 MongoDB 并执行一些常见操作。\n\n18.5.1 安装 mongolite 包\n如果还没有安装 mongolite 包，可以通过以下命令安装：\ninstall.packages(\"mongolite\")\n\n\n18.5.2 连接 MongoDB\n假设 MongoDB 在本地运行，端口为默认的 27017，并且数据库名称为 mydatabase，集合名称为 mycollection。\nlibrary(mongolite)\n\n# 连接到 MongoDB 数据库\nmongo_connection = mongo(\n  collection = \"mycollection\",\n  db = \"mydatabase\",\n  url = \"mongodb://localhost:27017\"\n)\n\n\n18.5.3 插入数据\n可以使用 JSON 字符串或 R 的 data.frame 直接插入数据。\n# 插入单条 JSON 数据\nmongo_connection$insert('{\"name\": \"Alice\", \"age\": 30, \"city\": \"New York\"}')\n\n# 插入多条数据（使用 data.frame）\ndata = data.frame(\n  name = c(\"Bob\", \"Charlie\"),\n  age = c(25, 35),\n  city = c(\"Los Angeles\", \"Chicago\")\n)\nmongo_connection$insert(data)\n\n\n18.5.4 查询数据\n使用 $find() 方法可以查询数据，并将结果返回为 data.frame。\n# 查询所有数据\nall_data = mongo_connection$find('{}')\nprint(all_data)\n\n# 带条件查询\nfiltered_data = mongo_connection$find('{\"age\": {\"$gt\": 30}}')\nprint(filtered_data)\n\n\n18.5.5 更新数据\n可以使用 $update() 方法更新文档，传入查询条件和更新内容。\n# 将 name 为 \"Alice\" 的 age 更新为 31\nmongo_connection$update(\n  '{\"name\": \"Alice\"}', \n  '{\"$set\": {\"age\": 31}}'\n)\n\n\n18.5.6 删除数据\n使用 $remove() 方法删除指定条件的数据。\n# 删除 age 小于 30 的文档\nmongo_connection$remove('{\"age\": {\"$lt\": 30}}')\n\n\n18.5.7 断开连接\n操作完成后，断开连接。\nrm(mongo_connection)  # 释放连接资源\n\n\n18.5.8 示例总结\n综上，以下代码演示了完整的操作流程：\n\nlibrary(mongolite)\n\n# 连接 MongoDB\nmongo_connection = mongo(\n  collection = \"mycollection\",\n  db = \"mydatabase\",\n  url = \"mongodb://localhost:27017\"\n)\n\n# 插入数据\nmongo_connection$insert('{\"name\": \"Alice\", \"age\": 30, \"city\": \"New York\"}')\n\nList of 6\n $ nInserted  : int 1\n $ nMatched   : int 0\n $ nModified  : int 0\n $ nRemoved   : int 0\n $ nUpserted  : int 0\n $ writeErrors: list()\n\ndata = data.frame(\n  name = c(\"Bob\", \"Charlie\"),\n  age = c(25, 35),\n  city = c(\"Los Angeles\", \"Chicago\")\n)\nmongo_connection$insert(data)\n\nList of 5\n $ nInserted  : num 2\n $ nMatched   : num 0\n $ nRemoved   : num 0\n $ nUpserted  : num 0\n $ writeErrors: list()\n\n# 查询数据\nprint(mongo_connection$find('{}'))\n\n     name age        city\n1   Alice  31    New York\n2 Charlie  35     Chicago\n3   Alice  30    New York\n4     Bob  25 Los Angeles\n5 Charlie  35     Chicago\n\n# 更新数据\nmongo_connection$update('{\"name\": \"Alice\"}', '{\"$set\": {\"age\": 31}}')\n\nList of 3\n $ modifiedCount: int 0\n $ matchedCount : int 1\n $ upsertedCount: int 0\n\n# 删除数据\nmongo_connection$remove('{\"age\": {\"$lt\": 30}}')\n\n# 断开连接\nrm(mongo_connection)\n\n这种方法可以让 R 用户便捷地在 MongoDB 上存储、查询和更新数据。",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>MongoDB 数据库</span>"
    ]
  },
  {
    "objectID": "mongoDB.html#本地和阿里云-mongodb-的同步",
    "href": "mongoDB.html#本地和阿里云-mongodb-的同步",
    "title": "18  MongoDB 数据库",
    "section": "18.6 本地和阿里云 MongoDB 的同步",
    "text": "18.6 本地和阿里云 MongoDB 的同步\n要将本地的 MongoDB 数据库与阿里云服务器上的 MongoDB 实现同步，通常可以使用以下几种方法：\n\n18.6.1 使用 mongodump 和 mongorestore 手动同步\n这是最简单的一种方式，通过导出和导入数据来实现同步。\n步骤：\n\n在本地主机上导出数据：\nmongodump --host localhost --port 27017 --out /path/to/dump\n这会在指定的路径下生成数据库的备份文件。\n将备份文件上传到阿里云服务器：\n使用 scp 命令将备份文件夹传输到阿里云服务器：\nscp -r /path/to/dump username@aliyun_ip:/path/on/server\n在阿里云服务器上导入数据：\n连接到阿里云服务器后，使用 mongorestore 命令将数据导入 MongoDB：\nmongorestore --host localhost --port 27017 /path/on/server/dump\n\n这种方法适合小规模的数据同步。如果数据量较大或同步频繁，不建议使用此方法。\n\n\n18.6.2 使用 MongoDB 同步工具 mongosync\nmongosync 是一个专门用于同步不同 MongoDB 实例的开源工具。它支持实时同步，并适用于数据量较大且需要持续同步的场景。\n步骤：\n\n在本地和阿里云 MongoDB 服务器上安装 mongosync。\n配置 mongosync 的源和目标数据库，在本地 MongoDB 上设置为源，阿里云 MongoDB 为目标。\n启动 mongosync 进行同步。具体的命令和配置可以参考 mongosync 的官方文档。\n\n\n\n18.6.3 设置 MongoDB 的副本集（Replica Set）\n如果需要长期的、实时的同步，推荐使用 MongoDB 的 副本集 功能。\n步骤：\n\n配置本地主机和阿里云 MongoDB 为副本集成员：\n\n修改本地主机和阿里云服务器上的 MongoDB 配置文件 /etc/mongod.conf，设置副本集名称。例如：\nreplication:\n  replSetName: \"myReplicaSet\"\n\n初始化副本集：\n在任意一个 MongoDB 实例上启动副本集：\nmongosh --host localhost --port 27017\nrs.initiate()\n添加成员：\n将本地主机和阿里云 MongoDB 作为副本集成员添加。可以在 MongoDB shell 中执行：\nrs.add(\"阿里云服务器IP:27017\")  // 阿里云服务器的MongoDB实例\nrs.add(\"localhost:27017\")        // 本地的MongoDB实例\n验证副本集同步：\n副本集启动后，MongoDB 会自动保持各个节点的数据同步。",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>MongoDB 数据库</span>"
    ]
  },
  {
    "objectID": "mongoDB.html#与-postgresql-的对比",
    "href": "mongoDB.html#与-postgresql-的对比",
    "title": "18  MongoDB 数据库",
    "section": "18.7 与 PostgreSQL 的对比",
    "text": "18.7 与 PostgreSQL 的对比\n使用 PostgreSQL 存储大语言模型的响应，以便作为缓存，可以按照以下步骤实现。我们可以创建一个包含请求数据和模型响应的表，同时记录创建时间，以便后续根据时间判断缓存是否过期。\n\n18.7.1 创建缓存表\n我们首先创建一个表来存储模型请求和响应，结构包含输入参数、响应内容、创建时间等字段。\nCREATE TABLE model_cache (\n    id SERIAL PRIMARY KEY,\n    input_hash VARCHAR(64) NOT NULL,\n    request_data JSONB NOT NULL,\n    response_data JSONB NOT NULL,\n    created_at TIMESTAMPTZ DEFAULT NOW()\n);\n\n-- 为 input_hash 字段创建唯一索引，用于快速查找缓存\nCREATE UNIQUE INDEX idx_input_hash ON model_cache (input_hash);\n\ninput_hash：对输入内容进行哈希，避免因输入内容重复而浪费存储。\nrequest_data：存储模型的输入请求，以 JSON 格式存储。\nresponse_data：存储模型的响应内容。\ncreated_at：记录数据创建的时间，用于设置缓存过期策略。\n\n\n\n18.7.2 在 Python 中实现缓存存储和检索\n以下是一个基于 Python 的示例，假设使用 psycopg2 库连接 PostgreSQL。此示例会在接收到新的请求时，先检查缓存，若未命中缓存则存储新响应。\n\n18.7.2.1 导入必要库\nimport psycopg2\nimport hashlib\nimport json\nfrom datetime import datetime, timedelta\n\n\n18.7.2.2 设置数据库连接\n# 连接 PostgreSQL\nconnection = psycopg2.connect(\n    host=\"your_postgresql_host\",\n    database=\"your_database\",\n    user=\"your_user\",\n    password=\"your_password\"\n)\n\n\n18.7.2.3 哈希生成函数\n为输入请求生成唯一的哈希值，用于缓存键值。\ndef generate_hash(input_data):\n    return hashlib.sha256(json.dumps(input_data, sort_keys=True).encode()).hexdigest()\n\n\n18.7.2.4 缓存存储函数\ndef cache_response(input_data, response_data, ttl_hours=24):\n    input_hash = generate_hash(input_data)\n    cursor = connection.cursor()\n    \n    # 插入或更新响应数据\n    cursor.execute(\"\"\"\n        INSERT INTO model_cache (input_hash, request_data, response_data, created_at)\n        VALUES (%s, %s, %s, NOW())\n        ON CONFLICT (input_hash) DO UPDATE\n        SET response_data = EXCLUDED.response_data,\n            created_at = NOW();\n    \"\"\", (input_hash, json.dumps(input_data), json.dumps(response_data)))\n    \n    connection.commit()\n    cursor.close()\n\n\n18.7.2.5 缓存查询函数\n在接收请求时，可以先检查缓存，若命中则直接返回缓存的响应。\ndef get_cached_response(input_data, ttl_hours=24):\n    input_hash = generate_hash(input_data)\n    cursor = connection.cursor()\n    \n    # 计算过期时间\n    ttl_time = datetime.utcnow() - timedelta(hours=ttl_hours)\n    \n    cursor.execute(\"\"\"\n        SELECT response_data FROM model_cache\n        WHERE input_hash = %s AND created_at &gt; %s;\n    \"\"\", (input_hash, ttl_time))\n    \n    result = cursor.fetchone()\n    cursor.close()\n    \n    if result:\n        return json.loads(result[0])  # 返回缓存的响应数据\n    return None  # 缓存未命中或已过期\n\n\n\n18.7.3 示例使用\n# 模拟输入请求数据和模型响应\ninput_data = {\"question\": \"What is AI?\"}\nresponse_data = {\"answer\": \"AI stands for Artificial Intelligence.\"}\n\n# 存储模型响应\ncache_response(input_data, response_data)\n\n# 查询缓存\ncached_response = get_cached_response(input_data)\nif cached_response:\n    print(\"Cache hit:\", cached_response)\nelse:\n    print(\"Cache miss\")\n\n\n18.7.4 定期清理过期缓存\n可以创建一个定期任务，清除超过指定时间的缓存数据。\nDELETE FROM model_cache WHERE created_at &lt; NOW() - INTERVAL '24 hours';\n\n\n18.7.5 总结\n这种方法可以让 PostgreSQL 有效地作为大语言模型的缓存存储，避免重复计算和存储不必要的数据。",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>MongoDB 数据库</span>"
    ]
  },
  {
    "objectID": "object-detection-of-microfluidic-chip.html",
    "href": "object-detection-of-microfluidic-chip.html",
    "title": "19  检测微流控芯片中的物体",
    "section": "",
    "text": "19.1 读取并显示图像\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# 加载图像\nimage_path = 'example/images/microfluidic-chip.jpg'\nimage = Image.open(image_path)\n\n# 在 Notebook 中显示图像\nplt.figure(figsize=(12, 8))\nplt.imshow(image)\nplt.axis('on')  # 显示坐标轴\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>检测微流控芯片中的物体</span>"
    ]
  },
  {
    "objectID": "object-detection-of-microfluidic-chip.html#使用高斯模糊",
    "href": "object-detection-of-microfluidic-chip.html#使用高斯模糊",
    "title": "19  检测微流控芯片中的物体",
    "section": "19.2 使用高斯模糊",
    "text": "19.2 使用高斯模糊\n这段代码主要实现了图像的预处理和可视化。\n首先，使用 OpenCV 库读取图像并将其转换为灰度图，这样可以减少计算量并简化后续处理。\n然后，应用高斯模糊来减少图像中的噪声，这有助于提高后续边缘检测的准确性。高斯模糊使用 9x9 的核，可以有效平滑图像而不会过度模糊重要特征。\n最后，使用 Matplotlib 库创建一个包含两个子图的图形，分别显示原始灰度图像和经过高斯模糊处理后的图像，以便直观比较处理效果。这个预处理步骤为后续的物体检测奠定了基础。\n\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 读取图像并转换为灰度图\nimage = cv2.imread(image_path)\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n# 应用高斯模糊以减少噪声\nblurred = cv2.GaussianBlur(gray, (9, 9), 0)\n\n# 显示模糊后的图像\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.title(\"Original Image\")\nplt.imshow(cv2.cvtColor(gray, cv2.COLOR_BGR2RGB))\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplt.title(\"Gaussian Blurred Image\")\nplt.imshow(cv2.cvtColor(blurred, cv2.COLOR_BGR2RGB))\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>检测微流控芯片中的物体</span>"
    ]
  },
  {
    "objectID": "object-detection-of-microfluidic-chip.html#使用canny边缘检测",
    "href": "object-detection-of-microfluidic-chip.html#使用canny边缘检测",
    "title": "19  检测微流控芯片中的物体",
    "section": "19.3 使用Canny边缘检测",
    "text": "19.3 使用Canny边缘检测\nCanny 边缘检测是一种广泛使用的图像处理技术，用于识别图像中的边缘。它由 John F. Canny 在 1986 年提出，被认为是一种非常有效的边缘检测算法。Canny 边缘检测的主要步骤包括：\n\n高斯滤波： 首先对图像进行高斯模糊，以减少噪声。这一步已经在前面的代码中完成。\n计算图像梯度： 使用 Sobel 算子计算图像在 x 和 y 方向的梯度。这可以找出图像中亮度变化最大的区域。\n非极大值抑制： 沿梯度方向，比较像素的梯度幅值。如果不是局部最大值，则将其抑制（设为 0）。这一步可以细化边缘。\n双阈值处理： 使用两个阈值（高阈值和低阈值）来区分强边缘、弱边缘和非边缘像素。\n边缘跟踪： 最后，通过滞后阈值法来跟踪边缘。强边缘像素被立即标记为边缘，而弱边缘像素只有在与强边缘像素相连时才被保留。\n\nCanny 边缘检测的优点包括： - 能够检测到弱边缘 - 对噪声具有较强的鲁棒性 - 能够得到较为精确的边缘定位\n在 OpenCV 中，cv2.Canny() 函数封装了这些步骤，使得边缘检测变得简单易用。在接下来的代码中，我们将使用这个函数来进行边缘检测。\n\n# 使用Canny边缘检测\nedges = cv2.Canny(blurred, 100, 200)\n\n# 定义形态学操作的核\nkernel = np.ones((2,2), np.uint8)\n\n# 应用闭运算来填充边缘之间的小间隙\nclosed_edges = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, kernel, iterations=2)\n\n# 应用膨胀操作来进一步加粗边缘\ndilated_edges = cv2.dilate(closed_edges, kernel, iterations=2)\n\n# 寻找轮廓\ncontours, _ = cv2.findContours(dilated_edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n# 在原图上绘制轮廓\nresult = image.copy()\ncv2.drawContours(result, contours, -1, (0, 255, 0), 2)\n\n# 显示结果\nplt.figure(figsize=(15, 5))\nplt.subplot(1, 3, 1)\nplt.title(\"Original Image\")\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.axis('off')\n\nplt.subplot(1, 3, 2)\nplt.title(\"Processed Edges\")\nplt.imshow(dilated_edges, cmap='gray')\nplt.axis('off')\n\nplt.subplot(1, 3, 3)\nplt.title(\"Detected Shapes\")\nplt.imshow(cv2.cvtColor(result, cv2.COLOR_BGR2RGB))\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# 打印检测到的轮廓数量\nprint(f\"Detected contours: {len(contours)}\")\n\n\n\n\n\n\n\n\nDetected contours: 22",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>检测微流控芯片中的物体</span>"
    ]
  },
  {
    "objectID": "object-detection-of-microfluidic-chip.html#使用-groundingdino-检测微流控芯片",
    "href": "object-detection-of-microfluidic-chip.html#使用-groundingdino-检测微流控芯片",
    "title": "19  检测微流控芯片中的物体",
    "section": "19.4 使用 GroundingDino 检测微流控芯片",
    "text": "19.4 使用 GroundingDino 检测微流控芯片\n在这个部分,我们将使用 GroundingDino 模型来检测微流控芯片中的白点。GroundingDino 是一个强大的目标检测模型,可以根据文本提示来定位和分割图像中的对象。以下是代码的主要步骤和功能:\n\n准备图像和提示:\n\n设置图像路径和上传图像\n创建文本提示 “white dots”\n\n配置检测任务:\n\n使用 DetectionTask 设置检测参数\n指定检测目标为边界框(BBox)和掩码(Mask)\n选择 GroundingDino-1.5-Pro 模型\n\n执行检测:\n\n使用 client.run_task() 运行检测任务\n获取检测结果\n\n可视化结果:\n\n遍历检测到的每个对象\n将掩码从 RLE 格式转换为 RGBA 图像\n创建图像副本并绘制检测结果\n显示原始图像、掩码叠加和边界框\n添加类别和置信度得分标签\n\n\n这段代码展示了如何使用高级目标检测模型来识别微流控芯片中的特定结构,并以可视化的方式呈现结果。\n\nimport os\nfrom dds_cloudapi_sdk import Config\nfrom dds_cloudapi_sdk import Client\nfrom dds_cloudapi_sdk import DetectionTask\nfrom dds_cloudapi_sdk import TextPrompt\nfrom dds_cloudapi_sdk import DetectionModel\nfrom dds_cloudapi_sdk import DetectionTarget\n\ntoken = os.getenv(\"DINO_API_KEY\")\nconfig = Config(token)\nclient = Client(config)\n\nimage_path = \"example/images/microfluidic-chip.jpg\"\nimage_url = client.upload_file(image_path)\nprompt = [TextPrompt(text=\"white dots surrounding by black edge\")]\n\ntask = DetectionTask(\n    image_url=image_url,\n    prompts=prompt,\n    targets=[DetectionTarget.Mask, DetectionTarget.BBox],  # detect both bbox and mask\n    model=DetectionModel.GDino1_5_Pro,  # detect with GroundingDino-1.5-Pro model\n)\n\ntask = DetectionTask(\n    image_url=image_url,\n    prompts=prompt,\n    targets=[DetectionTarget.Mask, DetectionTarget.BBox],\n    model=DetectionModel.GDino1_5_Pro,\n)\n\nclient.run_task(task)\nresult = task.result    \n\n\n# 显示检测结果\nimport matplotlib.pyplot as plt\nfrom PIL import Image, ImageDraw\nimport matplotlib.patches as patches\n\nimage = Image.open(image_path)\n\nobjects = result.objects  # the list of detected objects\nfor idx, obj in enumerate(objects):\n    # 将RLE格式转换为RGBA图像\n    mask_image = task.rle2rgba(obj.mask)\n\n    # 创建一个图像副本用于绘制\n    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n    \n    ax.imshow(image)\n    ax.imshow(mask_image, alpha=0.5)\n    \n    # 在图像上绘制掩码\n    bbox = obj.bbox\n    # 创建一个矩形框，参数为 (左上角 x, 左上角 y, 宽度, 高度)\n    rect = patches.Rectangle(\n        (bbox[0], bbox[1]),  # 左上角坐标\n        bbox[2] - bbox[0],   # 宽度\n        bbox[3] - bbox[1],   # 高度\n        linewidth=2,         # 边框宽度\n        edgecolor='red',     # 边框颜色\n        facecolor='none'     # 填充颜色\n    )\n    ax.add_patch(rect)   \n    \n    # 在图像上添加类别和分数\n    text = f\"{obj.category} {obj.score:.2f}\"\n    ax.text(bbox[0], \n              bbox[1] - 10, \n              text, \n              color='red', \n              fontsize=12)\n\n    # 显示图像\n    # plt.axis('off')  # 不显示坐标轴\n    plt.show()",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>检测微流控芯片中的物体</span>"
    ]
  },
  {
    "objectID": "gradio.html",
    "href": "gradio.html",
    "title": "20  Gradio",
    "section": "",
    "text": "20.1 用于情感分析的 APP\n下面是一个简单的示例代码，展示如何使用 Gradio 创建一个可以输入文本并返回情感分析结果的界面：\nimport gradio as gr\nfrom transformers import pipeline\n\n# 加载情感分析模型\nsentiment_model = pipeline(\"sentiment-analysis\")\n\n# 定义一个函数来处理输入并返回结果\ndef analyze_sentiment(text):\n    result = sentiment_model(text)[0]\n    return {result['label']: result['score']}\n\n# 创建 Gradio 接口\niface = gr.Interface(\n    fn=analyze_sentiment,\n    inputs=\"text\",\n    outputs=\"label\",\n    live=True\n)\n\n# 启动界面\niface.launch()\n\nRunning on local URL:  http://127.0.0.1:7860\n\nTo create a public link, set `share=True` in `launch()`.\n你可以在本地运行这个代码，并通过生成的链接访问和测试情感分析模型。",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Gradio</span>"
    ]
  },
  {
    "objectID": "gradio.html#在线服务",
    "href": "gradio.html#在线服务",
    "title": "20  Gradio",
    "section": "20.2 在线服务",
    "text": "20.2 在线服务\n还可以通过 iface.launch(share=True) 将服务发布到 gradio.live 网站上：https://e7ce7d908c95c155b5.gradio.live/?。\n注意：这个分享链接会很快失效。\n这时，可以使用 gradio_client 库中的 Client 类与一个远程 Gradio 应用进行交互。\n\nfrom gradio_client import Client\n\nclient = Client(\"https://e7ce7d908c95c155b5.gradio.live/\")\nresult = client.predict(\n        text=\"Hello!!\",\n        api_name=\"/predict\"\n)\nprint(result)\n\nclient = Client(\"https://e7ce7d908c95c155b5.gradio.live/\")\n\n创建 Client 实例：实例化一个 Client 对象，连接到指定的 Gradio 应用 URL。在这个例子中，URL 是 https://e7ce7d908c95c155b5.gradio.live/。\n\nresult = client.predict(\n    text=\"Hello!!\",\n    api_name=\"/predict\"\n)\n\n调用 predict 方法：\n\ntext=\"Hello!!\"：传递一个包含文本 \"Hello!!\" 的参数。\napi_name=\"/predict\"：指定要调用的 API 名称。这个名称通常是在 Gradio 应用中定义的特定端点。\n该方法会将参数发送到 Gradio 应用的 /predict 端点，并返回预测结果。\n\n\nprint(result)\n\n打印结果：将 predict 方法返回的结果打印到控制台。\n\n通过这种方式，你可以方便地与远程 Gradio 应用进行交互，发送请求并获取预测结果。",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Gradio</span>"
    ]
  },
  {
    "objectID": "gradio.html#在-quarto-中显示",
    "href": "gradio.html#在-quarto-中显示",
    "title": "20  Gradio",
    "section": "20.3 在 Quarto 中显示",
    "text": "20.3 在 Quarto 中显示\n要在 Quarto 中正确显示 Gradio 的输出结果，你需要确保以下几点：\n\nGradio 服务正常运行：你需要在本地或远程服务器上启动 Gradio 服务。\n在 Quarto 文档中嵌入 Gradio 应用：你可以在 Quarto 文档中嵌入一个 iframe 来显示 Gradio 应用的输出。\n\n下面是一个详细的步骤：\n\n20.3.1 1. 启动 Gradio 服务\n首先，确保你的 Gradio 服务正在运行。例如，可以使用上面的代码启动 Gradio 应用。\n\n\n20.3.2 2. 在 Quarto 文档中嵌入 Gradio 应用\n在你的 Quarto 文档中（例如，document.qmd），你可以通过 iframe 嵌入 Gradio 应用。确保你使用的是 .qmd 文件格式，并在其中嵌入 Gradio 的链接。\n下面是一个示例 Quarto 文档：\n# Gradio Application\n\n&lt;iframe src=\"YOUR_GRADIO_LINK_HERE\"&gt;&lt;/iframe&gt;\n将 YOUR_GRADIO_LINK_HERE 替换为 Gradio 应用生成的公开链接。\n\n\n\n\n20.3.3 3. 渲染 Quarto 文档\n使用 Quarto 渲染文档：\nquarto render document.qmd\n这将生成一个 HTML 文件，其中嵌入了你的 Gradio 应用。打开生成的 HTML 文件，即可在浏览器中查看并与 Gradio 应用进行交互。\n还可以参见 Quarto 动态文档：https://quarto.org/docs/interactive/。",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Gradio</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "References"
    ]
  }
]